<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.83">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MAP566 - Stats in Action - Linear Regression: Quick Recap</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../docs/regression/map566-lab-nonlinear-regression.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo_X.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">MAP566 - Stats in Action</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../getting-started.html" rel="" target="">
 <span class="menu-text">Getting Started</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jchiquet/MAP566" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-hypothesis-testing" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Hypothesis Testing</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-hypothesis-testing">    
        <li class="dropdown-header">Lectures</li>
        <li>
    <a class="dropdown-item" href="../../docs/tests/map566-lecture-single.html" rel="" target="">
 <span class="dropdown-text">Statistical Tests</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/tests/map566-lecture-multiple.html" rel="" target="">
 <span class="dropdown-text">Multiple Testing</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Exercices</li>
        <li>
    <a class="dropdown-item" href="../../docs/tests/map566-lab-tests.html" rel="" target="">
 <span class="dropdown-text">Statistical tests: exercices</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-regression-models" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Regression models</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-regression-models">    
        <li class="dropdown-header">Lectures</li>
        <li>
    <a class="dropdown-item" href="../../docs/regression/map566-lecture-polynomial-regression.html" rel="" target="">
 <span class="dropdown-text">Polynomial Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/regression/map566-lecture-nonlinear-regression.html" rel="" target="">
 <span class="dropdown-text">Nonlinear Regression</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Exercices</li>
        <li>
    <a class="dropdown-item" href="../../docs/regression/map566-lab-polynomial-regression.html" rel="" target="">
 <span class="dropdown-text">Polynomial Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/regression/map566-lab-nonlinear-regression.html" rel="" target="">
 <span class="dropdown-text">Nonlinear Regression</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Supplementary material</li>
        <li>
    <a class="dropdown-item" href="../../docs/regression/map566-lecture-regression-background.html" rel="" target="">
 <span class="dropdown-text">Linear Regression: Quick Recap</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-mixed-models" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Mixed models</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-mixed-models">    
        <li class="dropdown-header">Lectures</li>
        <li>
    <a class="dropdown-item" href="../../docs/mixed-models/map566-lecture-linear-mixed-model.html" rel="" target="">
 <span class="dropdown-text">Linear Mixed Effects Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/mixed-models/map566-lecture-nonlinear-mixed-model.html" rel="" target="">
 <span class="dropdown-text">Nonlinear Mixed Effects Models</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Exercices</li>
        <li>
    <a class="dropdown-item" href="../../docs/mixed-models/map566-lab-linear-mixed-model.html" rel="" target="">
 <span class="dropdown-text">Linear Mixed Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/mixed-models/map566-lab-nonlinear-mixed-model.html" rel="" target="">
 <span class="dropdown-text">Nonlinear Mixed Effects Models</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Supplementary material</li>
        <li>
    <a class="dropdown-item" href="../../docs/mixed-models/map566-lecture-EM-linear-mixed-model.html" rel="" target="">
 <span class="dropdown-text">An EM Algorithm for Linear Mixed Effects Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-mixture-models" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Mixture models</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-mixture-models">    
        <li class="dropdown-header">Lectures</li>
        <li>
    <a class="dropdown-item" href="../../docs/mixture-models/map566-lecture-mixture-models.html" rel="" target="">
 <span class="dropdown-text">Mixture Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/mixture-models/map566-lecture-graph-clustering-part1.html" rel="" target="">
 <span class="dropdown-text">Hierarchical and Spectral methods for Graph clustering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/mixture-models/map566-lecture-graph-clustering-part2.html" rel="" target="">
 <span class="dropdown-text">Stochastic block Models</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Exercices</li>
        <li>
    <a class="dropdown-item" href="../../docs/mixture-models/map566-lab-mixture-models.html" rel="" target="">
 <span class="dropdown-text">Mixture models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/mixture-models/map566-lab-graph-partionning.html" rel="" target="">
 <span class="dropdown-text">Graph Clustering: Spectral and hierarchical methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../docs/mixture-models/map566-lab-stochastic-blockmodels.html" rel="" target="">
 <span class="dropdown-text">Graph Clustering: Stochastic Blockmodels</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Supplementary material</li>
        <li>
    <a class="dropdown-item" href="../../docs/mixture-models/map566-doc-mixture-models-example.html" rel="" target="">
 <span class="dropdown-text">Clustering and classification with model based approaches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/jchiquet/MAP573/raw/master/slides/GraphClustering/GraphClustering.pdf" rel="" target="">
 <span class="dropdown-text">Slides on Graph Clustering</span></a>
  </li>  
    </ul>
  </li>
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Linear Regression: Quick Recap</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Lectures</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/regression/map566-lecture-polynomial-regression.html" class="sidebar-item-text sidebar-link">Polynomial Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/regression/map566-lecture-nonlinear-regression.html" class="sidebar-item-text sidebar-link">Nonlinear Regression</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Exercices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/regression/map566-lab-polynomial-regression.html" class="sidebar-item-text sidebar-link">Polynomial Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/regression/map566-lab-nonlinear-regression.html" class="sidebar-item-text sidebar-link">Nonlinear Regression</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Supplementary material</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/regression/map566-lecture-regression-background.html" class="sidebar-item-text sidebar-link active">Linear Regression: Quick Recap</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#regression-models" id="toc-regression-models" class="nav-link active" data-scroll-target="#regression-models">Regression models</a></li>
  <li><a href="#ordinary-least-squares" id="toc-ordinary-least-squares" class="nav-link" data-scroll-target="#ordinary-least-squares"><span class="toc-section-number">1</span>  Ordinary least squares</a>
  <ul class="collapse">
  <li><a href="#least-squares-estimator" id="toc-least-squares-estimator" class="nav-link" data-scroll-target="#least-squares-estimator"><span class="toc-section-number">1.1</span>  Least squares estimator</a></li>
  <li><a href="#estimation-of-the-residual-error-variance" id="toc-estimation-of-the-residual-error-variance" class="nav-link" data-scroll-target="#estimation-of-the-residual-error-variance"><span class="toc-section-number">1.2</span>  Estimation of the residual error variance</a></li>
  <li><a href="#the-standard-errors-of-the-estimates" id="toc-the-standard-errors-of-the-estimates" class="nav-link" data-scroll-target="#the-standard-errors-of-the-estimates"><span class="toc-section-number">1.3</span>  The standard errors of the estimates</a></li>
  </ul></li>
  <li><a href="#statistical-inference-and-diagnostics" id="toc-statistical-inference-and-diagnostics" class="nav-link" data-scroll-target="#statistical-inference-and-diagnostics"><span class="toc-section-number">2</span>  Statistical inference and diagnostics</a>
  <ul class="collapse">
  <li><a href="#statistical-tests-for-the-model-parameters" id="toc-statistical-tests-for-the-model-parameters" class="nav-link" data-scroll-target="#statistical-tests-for-the-model-parameters"><span class="toc-section-number">2.1</span>  Statistical tests for the model parameters</a></li>
  <li><a href="#confidence-interval-for-the-model-parameters" id="toc-confidence-interval-for-the-model-parameters" class="nav-link" data-scroll-target="#confidence-interval-for-the-model-parameters"><span class="toc-section-number">2.2</span>  Confidence interval for the model parameters</a></li>
  <li><a href="#coefficient-of-determination" id="toc-coefficient-of-determination" class="nav-link" data-scroll-target="#coefficient-of-determination"><span class="toc-section-number">2.3</span>  Coefficient of determination</a></li>
  <li><a href="#f-test-of-the-overall-significance" id="toc-f-test-of-the-overall-significance" class="nav-link" data-scroll-target="#f-test-of-the-overall-significance"><span class="toc-section-number">2.4</span>  F-test of the overall significance</a></li>
  </ul></li>
  <li><a href="#model-comparison" id="toc-model-comparison" class="nav-link" data-scroll-target="#model-comparison"><span class="toc-section-number">3</span>  Model comparison</a>
  <ul class="collapse">
  <li><a href="#anova" id="toc-anova" class="nav-link" data-scroll-target="#anova"><span class="toc-section-number">3.1</span>  ANOVA</a></li>
  <li><a href="#likelihood-ratio-test" id="toc-likelihood-ratio-test" class="nav-link" data-scroll-target="#likelihood-ratio-test"><span class="toc-section-number">3.2</span>  Likelihood ratio test</a></li>
  <li><a href="#deviance" id="toc-deviance" class="nav-link" data-scroll-target="#deviance"><span class="toc-section-number">3.3</span>  Deviance</a></li>
  <li><a href="#information-criteria" id="toc-information-criteria" class="nav-link" data-scroll-target="#information-criteria"><span class="toc-section-number">3.4</span>  Information criteria</a></li>
  <li><a href="#confidence-and-prediction-intervals" id="toc-confidence-and-prediction-intervals" class="nav-link" data-scroll-target="#confidence-and-prediction-intervals"><span class="toc-section-number">3.5</span>  Confidence and prediction intervals</a></li>
  </ul></li>
  <li><a href="#appendix---maximum-likelihood-approach" id="toc-appendix---maximum-likelihood-approach" class="nav-link" data-scroll-target="#appendix---maximum-likelihood-approach">Appendix - Maximum likelihood approach</a>
  <ul class="collapse">
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood"><span class="toc-section-number">3.6</span>  Likelihood</a></li>
  <li><a href="#maximum-likelihood-estimator" id="toc-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#maximum-likelihood-estimator"><span class="toc-section-number">3.7</span>  Maximum likelihood estimator</a></li>
  <li><a href="#the-fisher-information-matrix" id="toc-the-fisher-information-matrix" class="nav-link" data-scroll-target="#the-fisher-information-matrix"><span class="toc-section-number">3.8</span>  The Fisher Information matrix</a></li>
  <li><a href="#some-general-definitions" id="toc-some-general-definitions" class="nav-link" data-scroll-target="#some-general-definitions"><span class="toc-section-number">3.9</span>  Some general definitions</a></li>
  <li><a href="#the-central-limit-theorem" id="toc-the-central-limit-theorem" class="nav-link" data-scroll-target="#the-central-limit-theorem"><span class="toc-section-number">3.10</span>  The central limit theorem</a></li>
  <li><a href="#the-fim-for-a-regression-model" id="toc-the-fim-for-a-regression-model" class="nav-link" data-scroll-target="#the-fim-for-a-regression-model"><span class="toc-section-number">3.11</span>  The FIM for a regression model</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jchiquet/MAP566/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Linear Regression: Quick Recap</h1>
<p class="subtitle lead">Lecture Notes</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="regression-models" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="regression-models">Regression models</h2>
<p>A <em>regression model</em> relates a response variable <span class="math inline">y</span> to a set of explanatory variables <span class="math inline">x</span>. Assuming that we have access to <span class="math inline">n</span> set of values <span class="math inline">(x_j, y_j)</span>, <span class="math inline">1 \leq j \leq n)</span>, of these variable, the regression model is assumed to take the form <span class="math display">y_j = f(x_j,\beta) + \varepsilon_j \quad ; \quad 1\leq j \leq n</span></p>
<p>where <span class="math inline">f</span> is a <em>structural model</em> which depends on a <span class="math inline">p</span>-vector of parameters <span class="math inline">\beta</span>. We will assume that the residuals <span class="math inline">(\varepsilon_j)</span> are independent random variables with mean 0 and variance <span class="math inline">\sigma^2</span>: <span class="math display">
\mathbb{E}(\varepsilon_j) = 0 \quad ; \quad  \mathbb{E}(\varepsilon^2_j) = \sigma^2 \quad ; \quad \mathbb{E}(\varepsilon_j \varepsilon_k) = 0  \ (j \neq k)
</span></p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this webpage, we will reproduce manually the output from simple 1-degree polynomial from the main course page:</p>
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/cars-example_f9bfb87a96b549e679513eab77881330">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(cars)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>lm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(dist <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> speed, <span class="at">data =</span> cars)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = dist ~ 1 + speed, data = cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-29.069  -9.525  -2.272   9.215  43.201 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15.38 on 48 degrees of freedom
Multiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="ordinary-least-squares" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="ordinary-least-squares"><span class="header-section-number">1</span> Ordinary least squares</h2>
<section id="least-squares-estimator" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="least-squares-estimator"><span class="header-section-number">1.1</span> Least squares estimator</h3>
<p>A method for choosing automatically the “best parameters” <span class="math inline">\beta</span> consists in minimizing the sum of squared errors of prediction, i.e.&nbsp;the residual sum of squares (RSS) :</p>
<p><span class="math display">
RSS(\beta) = \sum_{j=1}^n (y_j - f(x_j))^2 = \|y - X\beta \|^2
</span></p>
<p>Then, <span class="math display">
\hat{\beta} = \arg\min_{\beta} \|y - X\beta \|^2 = (X^\prime X)^{-1}X^\prime y
</span></p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/car-example_beta_hat_9af16a7a6f584b5a6f40321d386930d0">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(cars)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> cars<span class="sc">$</span>dist</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, cars<span class="sc">$</span>speed)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y); d <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)       speed 
 -17.579095    3.932409 </code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="estimation-of-the-residual-error-variance" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="estimation-of-the-residual-error-variance"><span class="header-section-number">1.2</span> Estimation of the residual error variance</h3>
<p>An unbiased estimate of <span class="math inline">\sigma^2</span> is <span class="math display">
\hat{\sigma}^2 = \frac{1}{n-p} \|y - X\hat{\beta} \|^2
</span> Indeed,</p>
<p><span class="math display">
\begin{aligned}
\|y - X\hat{\beta} \|^2 &amp;= \| y - X(X^\prime X)^{-1}X^\prime y\|^2   \\
&amp;= \| \left({\rm I}_n - X(X^\prime X)^{-1}X^\prime \right) \varepsilon\|^2 \\
&amp;= \varepsilon^\prime \left({\rm I}_n - X(X^\prime X)^{-1}X^\prime \right)^\prime \left({\rm I}_n - X(X^\prime X)^{-1}X^\prime \right) \varepsilon \\
&amp;= \varepsilon^\prime \left({\rm I}_n - X(X^\prime X)^{-1}X^\prime \right) \varepsilon \\
&amp;= {\rm trace} \left\{ \varepsilon^\prime \left({\rm I}_n - X(X^\prime X)^{-1}X^\prime \right) \varepsilon \right\} \\
&amp;= {\rm trace} \left\{  \left({\rm I}_n - X(X^\prime X)^{-1}X^\prime \right) \varepsilon \varepsilon^\prime \right\}
\end{aligned}
</span> Then,</p>
<p><span class="math display">
\begin{aligned}
\mathbb{E}{\|y - X\hat{\beta} \|^2} &amp;= {\rm trace} \left(  \left({\rm I}_n - X(X^\prime X)^{-1}X^\prime \right) \mathbb{E}{\varepsilon \varepsilon^\prime} \right) \\
&amp;= \sigma^2 {\rm trace} \left(  {\rm I}_n - X(X^\prime X)^{-1}X^\prime \right) \\
&amp;= \sigma^2 \left( {\rm trace} \left( {\rm I}_n \right)  - {\rm trace} \left(X(X^\prime X)^{-1}X^\prime \right) \right) \\
&amp;= \sigma^2 \left( n  - {\rm trace} \left((X^\prime X)^{-1}X^\prime X \right) \right) \\
&amp;= \sigma^2 \left( n  - {\rm trace} \left({\rm I}_d \right) \right) \\
&amp;= \sigma^2 ( n - p)
\end{aligned}
</span></p>
<p>The standard deviation of the residual errors, called <em>residual standard error</em> in <code>R</code>, is the square root of this estimated variance</p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/unnamed-chunk-1_7ddcec8410f6d2f4c3109fb8a1993631">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_hat</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>residuals <span class="ot">&lt;-</span> y <span class="sc">-</span> y_hat</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>sigma2_hat <span class="ot">&lt;-</span> <span class="fu">sum</span>(residuals<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (n <span class="sc">-</span> d)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(sigma2_hat) <span class="co"># residual standard error in R</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 15.37959</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="the-standard-errors-of-the-estimates" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="the-standard-errors-of-the-estimates"><span class="header-section-number">1.3</span> The standard errors of the estimates</h3>
<p>We can remark that <span class="math display">
\begin{aligned}
\hat{\beta} &amp;=  (X^\prime X)^{-1}X^\prime y \\
&amp;= \beta + (X^\prime X)^{-1}X^\prime \varepsilon
\end{aligned}
</span></p>
<p>Then, since <span class="math inline">\mathbb{E}{e}=0</span>, <span class="math display"> \mathbb{E}(\hat{\beta}) = \beta</span></p>
<p>and <span class="math display">
\begin{aligned}
\mathbb{V}{\hat\beta} &amp;=  \mathbb{V}{(X^\prime X)^{-1}X^\prime \varepsilon} \\
&amp;= (X^\prime X)^{-1}X^\prime \mathbb{V}(\varepsilon) X (X^\prime X)^{-1} \\
&amp;= \sigma^2 (X^\prime X)^{-1}
\end{aligned}
</span></p>
<p>We can therefore use this formula to compute the variance covariance matrix of <span class="math inline">\hat\beta</span>.</p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/unnamed-chunk-2_d7308801eda74a64a2cf904e049e517a">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>vcov_beta <span class="ot">&lt;-</span> sigma2_hat <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>vcov_beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]       [,2]
[1,] 45.676514 -2.6588234
[2,] -2.658823  0.1726509</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(lm1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            (Intercept)      speed
(Intercept)   45.676514 -2.6588234
speed         -2.658823  0.1726509</code></pre>
</div>
</div>
</div>
</div>
<p>Then, the standard error of each component of <span class="math inline">\hat\beta</span> is defined as the square root of the diagonal elements of the variance-covariance matrix <span class="math inline">V=\mathbb{V}{\hat\beta}</span>:</p>
<p><span class="math display">{\rm se}(\hat\beta_k) = \sqrt{V_{k k}} </span></p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/unnamed-chunk-3_8323d314ba1fb251c2be6cd86da69f7f">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>se_beta <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(vcov_beta))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>se_beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 6.7584402 0.4155128</code></pre>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="statistical-inference-and-diagnostics" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="statistical-inference-and-diagnostics"><span class="header-section-number">2</span> Statistical inference and diagnostics</h2>
<p>Suppose that residuals <span class="math inline">(\varepsilon_j)</span> are independent and normally distributed with mean 0 and variance <span class="math inline">\sigma^2</span>: <span class="math display">
\varepsilon_j \sim^{\mathrm{iid}} \mathcal{N}(0 \ , \ \sigma^2).
</span></p>
<section id="statistical-tests-for-the-model-parameters" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="statistical-tests-for-the-model-parameters"><span class="header-section-number">2.1</span> Statistical tests for the model parameters</h3>
<p>In this case, <span class="math inline">\hat{\beta}</span> is also normally distributed:</p>
<p><span class="math display">
\hat{\beta} \sim \mathcal{N}(\beta \ , \ \sigma^2 (X^\prime X)^{-1})
</span> and, for <span class="math inline">k=1, 2, \ldots , p</span>, <span class="math display">t_k = \frac{\hat{\beta}_k - \beta_k}{{\rm se}(\hat{\beta}_k)}</span> follows a <span class="math inline">t</span>-distribution with <span class="math inline">n-d</span> degrees of freedom.</p>
<p>For each component <span class="math inline">\beta_k</span> of <span class="math inline">\beta</span>, we can then perform a <span class="math inline">t</span>-test (known as the <em>Wald test</em>) to test <span class="math display">
H_{k,0} : ``\beta_k = 0"  \quad \text{versus} \quad H_{k,1}: ``\beta_k \neq 0" </span></p>
<p>Indeed, under the null hypothesis <span class="math inline">H_{k,0}</span>, <span class="math inline">t_{{\rm stat}, k} = {\hat{\beta}_k}/{{\rm se}(\hat{\beta}_k)}</span> follows a <span class="math inline">t</span>-distribution with <span class="math inline">n-d</span> degrees of freedom.</p>
<p>The <span class="math inline">p</span>-value for this test is therefore</p>
<p><span class="math display">
p_k= \mathbb{P}{|t_{n-d}| \geq |t_{{\rm stat}, k}^{\rm obs}| } = 2(1 - \mathbb{P}{t_{n-d} \leq |t_{{\rm stat}, k}^{\rm obs}| } )
</span></p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/unnamed-chunk-4_be3a24c167548686cf4cbdc091fde69d">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>t_stat <span class="ot">&lt;-</span> beta_hat <span class="sc">/</span> se_beta</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">beta_hat =</span> beta_hat, </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">se_beta =</span> se_beta, <span class="at">t_stat =</span> t_stat, </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_value =</span> <span class="dv">2</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">pt</span>(<span class="fu">abs</span>(t_stat), n <span class="sc">-</span> d))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    beta_hat   se_beta    t_stat      p_value
1 -17.579095 6.7584402 -2.601058 1.231882e-02
2   3.932409 0.4155128  9.463990 1.489919e-12</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="confidence-interval-for-the-model-parameters" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="confidence-interval-for-the-model-parameters"><span class="header-section-number">2.2</span> Confidence interval for the model parameters</h3>
<p>Using the fact that <span class="math inline">t_k</span> follows a <span class="math inline">t</span>-distribution with <span class="math inline">n-p</span> degrees of freedom, we can build a confidence interval for <span class="math inline">\beta_k</span> of level <span class="math inline">1-\alpha</span>:</p>
<p><span class="math display">{\rm CI}_{1-\alpha}(\beta_k) = [\hat{\beta}_k + qt_{\alpha/2, n-d}\ {\rm se}(\hat{\beta}_k) \ , \ \hat{\beta}_k + qt_{1-\alpha/2, n-d}\ {\rm se}(\hat{\beta}_k)]</span> where <span class="math inline">qt_{\alpha/2, n-d}</span> and <span class="math inline">qt_{1-\alpha/2, n-d}</span> are the quantiles of order <span class="math inline">\alpha/2</span> and <span class="math inline">1-\alpha/2</span> for a <span class="math inline">t</span>-distribution with <span class="math inline">n-p</span> df.</p>
<p>Indeed, we can easily check that <span class="math inline">\mathbb{P}{{\rm CI}_{1-\alpha}(\beta_k) \ni \beta_k} = 1-\alpha</span>.</p>
<p>The function <code>confint</code> computes such confidence intervals for <span class="math inline">\beta</span> (default level = 95%))</p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/confint-ex_9f2f9ce62c1202fe08be527ddce3ed25">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lm1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 2.5 %    97.5 %
(Intercept) -31.167850 -3.990340
speed         3.096964  4.767853</code></pre>
</div>
</div>
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/confint-reproduced_7c91a21b5fd023b5c19385ce6263a8d2">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(beta_hat <span class="sc">+</span> se_beta <span class="sc">*</span> <span class="fu">qt</span>(alpha<span class="sc">/</span><span class="dv">2</span>    , n <span class="sc">-</span> d) ,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>      beta_hat <span class="sc">+</span> se_beta <span class="sc">*</span> <span class="fu">qt</span>(<span class="dv">1</span> <span class="sc">-</span> alpha<span class="sc">/</span><span class="dv">2</span>, n <span class="sc">-</span> d))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]      [,2]
[1,] -31.167850 -3.990340
[2,]   3.096964  4.767853</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="coefficient-of-determination" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="coefficient-of-determination"><span class="header-section-number">2.3</span> Coefficient of determination</h3>
<p>The multiple R-squared <span class="math inline">R^2</span> is the proportion of variation in the response variable that has been explained by the model. Using the fact that <span class="math display">
\|y - \bar{y}\|^2 = \|X\hat{\beta} - \bar{y}\|^2 + \|y - X\hat{\beta} \|^2
</span></p>
<p><span class="math display">
R^2 =  \frac{\|X\hat{\beta} - \bar{y}\|^2}{\|y - \bar{y}\|^2} = 1 - \frac{\|y - X\hat{\beta} \|^2}{\|y - \bar{y}\|^2}
</span></p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/unnamed-chunk-5_9ce777bf966ab4663548088eb851eed8">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>y_bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(y)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>R2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>( (y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span> ) <span class="sc">/</span> <span class="fu">sum</span>( (y <span class="sc">-</span> y_bar)<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>R2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6510794</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Equivalently,</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>( (y_hat <span class="sc">-</span> y_bar)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">sum</span>((y <span class="sc">-</span> y_bar)<span class="sc">^</span><span class="dv">2</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6510794</code></pre>
</div>
</div>
</div>
</div>
<p>By construction, adding more predictors to the model, i.e.&nbsp;increasing the degree of the polynome, is always going to increase the R-squared value. Adjusted R-squared penalizes this effect by normalizing each term by the associated degree of freedom.</p>
<p><span class="math display">
\begin{aligned}
R^2_{\rm adj} &amp;=  1 - \frac{\|y - X\hat{\beta} \|^2/(n-p)}{\|y - \bar{y}\|^2/(n-1)}
\end{aligned}
</span></p>
<p>The R-squared is a purely descriptive statistics. The adjusted R-squared should be preferably used to compare the explanatory power of models built from the same dataset.</p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/unnamed-chunk-6_bd53bc56b678eec6ed62e8e4bf0c2e02">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>((y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (n<span class="sc">-</span>d) <span class="sc">/</span> ( <span class="fu">sum</span>((y<span class="sc">-</span>y_bar)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (n<span class="dv">-1</span>) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6438102</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="f-test-of-the-overall-significance" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="f-test-of-the-overall-significance"><span class="header-section-number">2.4</span> F-test of the overall significance</h3>
<p>A F-test is also performed to test if at least one of the coefficients <span class="math inline">\beta_1, \ldots , \beta_{p}</span> is non zero:</p>
<p><span class="math display">
\begin{aligned}
H_0 &amp;:  \quad (\beta_1, \beta_2, \cdots ,\beta_p) = (0, 0, \cdots, 0) \\
H_1 &amp;:  \quad (\beta_1, \beta_2, \cdots ,\beta_p) \neq (0, 0, \cdots, 0)
\end{aligned}
</span></p>
<p>The test statistic for testing <span class="math inline">H_0</span> against <span class="math inline">H_1</span> is <span class="math display">
\begin{aligned}
F_{\rm stat} &amp;=  \frac{\|X\hat{\beta} - \bar{y}\|^2/(p-1)}{\|y - X\hat{\beta} \|^2/(n-p)}
\end{aligned}
</span></p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Reproducing <code>lm</code> output
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/unnamed-chunk-7_496678c5c891f1d41702b0fc6890114a">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>F_stat <span class="ot">&lt;-</span> (<span class="fu">sum</span>((y_hat <span class="sc">-</span> y_bar)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (d<span class="dv">-1</span>))<span class="sc">/</span>(<span class="fu">sum</span>((y<span class="sc">-</span>y_hat)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(n<span class="sc">-</span>d))</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>F_stat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 89.56711</code></pre>
</div>
</div>
</div>
</div>
<p>Let us show that, under the null hypothesis, the test statistic <span class="math inline">F_{\rm stat}</span> has a F distribution with <span class="math inline">(p-1,n-p)</span> degrees of freedom:</p>
<p>By construction, <span class="math inline">\|y - X\hat{\beta} \|^2/\sigma^2</span> has a <span class="math inline">\chi^2</span> distribution with <span class="math inline">n-d</span> df. On the other hand, under <span class="math inline">H_0</span>, <span class="math inline">y_j=\varepsilon_j \sim^{\mathrm{iid}} \mathcal{N}(0,\sigma^2)</span> and <span class="math inline">\|y - \bar{y}\|^2/\sigma^2</span> has a <span class="math inline">\chi^2</span> distribution with <span class="math inline">n-1</span> df.</p>
<p>Using the fact that <span class="math display">
\|y - \bar{y}\|^2 = \|X\hat{\beta} - \bar{y}\|^2 + \|y - X\hat{\beta} \|^2
</span> We deduce that, under <span class="math inline">H_0</span>, <span class="math inline">\|X\hat{\beta} - \bar{y}\|^2/\sigma^2</span> has a <span class="math inline">\chi^2</span> distribution with <span class="math inline">(n-1) - (n-p) = p-1</span> df which leads to the conclusion since <span class="math inline">(\chi^2(\nu_1)/\nu1)/(\chi^2(\nu_2)/\nu2) = F(\nu_1,\nu_2)</span>.</p>
<p>The <span class="math inline">p</span>-value of the F-test is therefore <span class="math display">\text{p-value(F-test)} = \mathbb{P}(F_{p-1,n-p} &gt; F_{\rm stat})=1- \mathbb{P}(F_{p-1,n-p} \leq F_{\rm stat})</span> ::: {.callout-note} ## Reproducing <code>lm</code> output</p>
<div class="cell" data-hash="map566-lecture-regression-background_cache/html/unnamed-chunk-8_876539a22b3814653bb1b174eaae2583">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pf</span>(F_stat, d <span class="sc">-</span> <span class="dv">1</span>, n <span class="sc">-</span> d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.489919e-12</code></pre>
</div>
</div>
<p>:::</p>
<p><strong>Remark:</strong> t-test and F-test are equivalent for linear models with only one predictor. In the case of polynomial regression of degree <span class="math inline">d = 1</span>, both tests can be used equally for testing if <span class="math inline">\beta_1=0</span>. Indeed, <span class="math display">
\begin{aligned}
F_{\rm stat} &amp;=  \frac{\|\hat{\beta}_0 + \hat{\beta}_1 x - \bar{y}\|^2}{\|y - \hat{\beta}_0 - \hat{\beta}_1 x\|^2/(n-2)} \\[1.5ex]
&amp;=  \frac{\hat{\beta}_1^2 \|x - \bar{x}\|^2}{\hat{\sigma}^2} = \frac{\hat{\beta}_1^2}{se^2(\hat{\beta}_1)} = t_{\rm stat}^2
\end{aligned}
</span></p>
<p>Furthermore, if <span class="math inline">t_{\rm stat}</span> has a <span class="math inline">t</span> distribution with <span class="math inline">n-2</span> df, then <span class="math inline">t_{\rm stat}^2</span> has a <span class="math inline">F</span> distribution with <span class="math inline">(1,n-2)</span> df. Both p-values are therefore equal.</p>
</section>
</section>
<section id="model-comparison" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="model-comparison"><span class="header-section-number">3</span> Model comparison</h2>
<p>Again, we assume in this part that the residual errors are independent and identically distributed (i.i.d.), with a normal distribution, mean 0 and variance <span class="math inline">\sigma^2</span>.</p>
<section id="anova" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="anova"><span class="header-section-number">3.1</span> ANOVA</h3>
<p>Consider two <strong>nested</strong> linear models with, respectively, <span class="math inline">p_1</span> and <span class="math inline">p_2</span> coefficients. Let <span class="math inline">\hat{y}_1</span> and <span class="math inline">\hat{y}_2</span> be the respective predicted values under. Cochran Theorem states that</p>
<p><span class="math display"> \|y - \hat{y}_1 \|^2 = \|\hat{y}_2 - \hat{y}_1\|^2 + \|y - \hat{y}_2 \|^2 </span></p>
<p>Then, the statistics used for the test is</p>
<p><span class="math display">
F_{\rm stat} =  \frac{\text{explained variance}}{\text{unexplained variance}} = \frac{\|\hat{y}_2 - \hat{y}_1\|^2/(p_2 - p_1)}{\|y - \hat{y}_2 \|^2/(n-p_2)}
</span></p>
<p>Under the null, the test statistics <span class="math inline">F_{\rm stat}</span> follows a <span class="math inline">F</span> distribution with <span class="math inline">(p_2-p_1 , n-p_2)</span> degrees of freedom.</p>
<p>This ANOVA test can be performed by means of the <code>anova</code> function.</p>
</section>
<section id="likelihood-ratio-test" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="likelihood-ratio-test"><span class="header-section-number">3.2</span> Likelihood ratio test</h3>
<p>When two models are nested, we can compare them by performing a likelihood ratio test (LRT).</p>
<p>Let <span class="math inline">\log\ell_1</span> and <span class="math inline">\log\ell_2</span> be the log-likelihood functions of models <span class="math inline">\mathcal{M}_1</span> and <span class="math inline">\mathcal{M}_2</span>. Then, for large <span class="math inline">n</span>, the distribution of the test statistics <span class="math display">
\begin{aligned}
LRT_{\rm stat} &amp;= 2(\log\ell_2(\hat\theta_2) - \log\ell_1(\hat\theta_1) \\
&amp;= n \log \left( \frac{\sum_{j=1}^n(y_j - f_1(x_j,\hat{\beta_1}))^2}{\sum_{j=1}^n(y_j - f_2(x_j,\hat{\beta_2}))^2} \right)
\end{aligned}
</span> can be approximated by a <span class="math inline">\chi^2</span> distribution with <span class="math inline">p_2-p_1=d_2-d_1</span> df.</p>
</section>
<section id="deviance" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="deviance"><span class="header-section-number">3.3</span> Deviance</h3>
<p>The deviance for a given <strong>regression model</strong> and a given set of observations <span class="math inline">y</span>, is a measure of goodness of fit defined, in <code>R</code>, as:</p>
<p><span class="math display">
D = \sum_{j=1}^n(y_j - f(x_j,\hat{\beta}))^2
</span></p>
</section>
<section id="information-criteria" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="information-criteria"><span class="header-section-number">3.4</span> Information criteria</h3>
<p>Functions <code>AIC</code> and <code>BIC</code> compute the Akaike information criterion and Bayesian information criterion. AIC and BIC are <em>penalized</em> versions of the log-likelihood defined by:</p>
<p><span class="math display">
\begin{aligned}
AIC &amp;= -2\log\ell(\hat{\theta}) + 2P \\
BIC &amp;= -2\log\ell(\hat{\theta}) + \log(n)P
\end{aligned}
</span> where <span class="math inline">P</span> is the number of parameters of the model, i.e.&nbsp;the length of <span class="math inline">\theta</span>.</p>
<p>On one hand, <span class="math inline">-2\log\ell(\hat{\theta})</span> decreases when <span class="math inline">P</span> increases. On the other hand, the penalization term (<span class="math inline">2P</span> or <span class="math inline">\log(n)P</span>) increases with <span class="math inline">P</span>. The objective of these criteria is to propose a model with an optimal compromise between the goodness of fit (measured by the log-likelihood) and the complexity of the model (measured by the number of parameters <span class="math inline">P</span>).</p>
<p><a name="poly4"></a></p>
</section>
<section id="confidence-and-prediction-intervals" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="confidence-and-prediction-intervals"><span class="header-section-number">3.5</span> Confidence and prediction intervals</h3>
<p>For given values <span class="math inline">x^{\mathrm{new}}</span> of the explanatory variable, we can use the fitted model for estimating the predicted response <span class="math inline">f^{\mathrm{new}}=f(x^{\mathrm{new}})</span>. This estimation is defined as</p>
<p><span class="math display">
\hat{f^{\mathrm{new}}} = f(x^{\mathrm{new}} ,\hat{\beta})
</span></p>
<p><span class="math inline">\hat{f^{\mathrm{new}}}</span> is a random variable since it is a function of the observed <span class="math inline">y</span>. We can compute a confidence interval for <span class="math inline">f^{\mathrm{new}}</span> with function <code>predict(interval = "confidence")</code>, since <span class="math inline">\hat{f^{\mathrm{new}}}= x^{\mathrm{new}} \hat{\beta}</span>, <span class="math display">
\hat{f^{\mathrm{new}}} \sim \mathcal{N} (f^{\mathrm{new}} , {\rm Var}(\hat{f^{\mathrm{new}}} ) )
</span></p>
<p>where <span class="math display">\begin{aligned}
{\rm Var}(\hat{f^{\mathrm{new}}} ) &amp;=  {x^{\mathrm{new}}} {\rm Var}(\hat{\beta}) {x^{\mathrm{new}}}^\prime \\
&amp;= \sigma^2 x^{\mathrm{new}}(X^\prime X)^{-1}{x^{\mathrm{new}}}^\prime
\end{aligned}</span></p>
<p><span class="math inline">\widehat{{\rm Var}(\hat{f^{\mathrm{new}}})}</span>, an estimate of <span class="math inline">{\rm Var}(\hat{f^{\mathrm{new}}} )</span> is obtained using <span class="math inline">\hat\sigma^2</span> instead of <span class="math inline">\sigma^2</span>.</p>
<p>Then, <span class="math display">
\begin{aligned}
\left({{\rm Var}(\hat{f^{\mathrm{new}}})} \right)^{-1/2}(\hat{f^{\mathrm{new}}} - f^{\mathrm{new}}) &amp;\sim \mathcal{N}(0,  {\rm Id}_{n^{\mathrm{new}}} )  \\
\left(\widehat{{\rm Var}(\hat{f^{\mathrm{new}}})} \right)^{-1/2}(\hat{f^{\mathrm{new}}} - f^{\mathrm{new}}) &amp;\sim t_{n^{\mathrm{new}},n-p}
\end{aligned}
</span> where <span class="math inline">t_{n^{\mathrm{new}},n-p}</span> is the multivariate <span class="math inline">t</span> distribution with <span class="math inline">n-p</span> degrees of freedom (the components of this <span class="math inline">n^{\mathrm{new}}</span>-vector are independent and follow a <span class="math inline">t</span> distribution with <span class="math inline">n-p</span> df).</p>
<p>Consider now a vector of new measured values <span class="math inline">y^{\mathrm{new}}</span>. We can again use the <code>predict(interval = "prediction")</code> function for computing a prediction interval for <span class="math inline">y^{\mathrm{new}}</span>. By definition of the model, <span class="math display">
y^{\mathrm{new}} \sim \mathcal{N} (f^{\mathrm{new}} , \sigma^2 \, {\rm Id}_{n^{\mathrm{new}}} )
</span> Then, if we want to compute a prediction interval for <span class="math inline">y^{\mathrm{new}}</span>, we must take into account the variability of <span class="math inline">y^{\mathrm{new}}</span> around <span class="math inline">f^{\mathrm{new}}</span>, but also the uncertainty on <span class="math inline">f^{\mathrm{new}}</span> since it is unknown:</p>
<p><span class="math display">y^{\mathrm{new}} = \hat{f^{\mathrm{new}}} + (f^{\mathrm{new}}-\hat{f^{\mathrm{new}}}) + \varepsilon^{\mathrm{new}} </span> Thus, <span class="math display">
y^{\mathrm{new}} - \hat{f^{\mathrm{new}}} \sim \mathcal{N}(0, {\rm Var}(\hat{f^{\mathrm{new}}}) + \sigma^2 \, {\rm Id}_{n^{\mathrm{new}}} )
</span> Then, <span class="math display">
\begin{aligned}
\left(x^{\mathrm{new}} {\rm Var}(\hat{\beta}) x^{\mathrm{new}} + {\sigma}^2 {\rm Id}_{n^{\mathrm{new}}} \right)^{-1/2}(y^{\mathrm{new}} - \hat{f^{\mathrm{new}}}) &amp; \sim \mathcal{N}(0,  {\rm Id}_{n^{\mathrm{new}}} ) \\
\left(x^{\mathrm{new}} \widehat{{\rm Var}(\hat{\beta})} x^{\mathrm{new}} + \hat{\sigma}^2 {\rm Id}_{n^{\mathrm{new}}} \right)^{-1/2}(y^{\mathrm{new}} - \hat{f^{\mathrm{new}}}) &amp; \sim t_{n^{\mathrm{new}},n-p}
\end{aligned}
</span></p>
</section>
</section>
<section id="appendix---maximum-likelihood-approach" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="appendix---maximum-likelihood-approach">Appendix - Maximum likelihood approach</h2>
<p>If we assume that <span class="math inline">(\varepsilon_j, 1 \leq j \leq n)</span> is a sequence of independent and normally distributed random variables with mean 0 and variance <span class="math inline">1</span>: <span class="math display">
\varepsilon_j \sim^{\mathrm{iid}} \mathcal{N}(0, 1),
</span> then the <span class="math inline">y_j</span> are also independent and normally distributed:</p>
<p><span class="math display"> y_j \sim \mathcal{N}(f(x_j, \beta),\sigma^2).</span> The vector <span class="math inline">y=(y_1,y_2,\ldots,y_n)</span> is therefore a Gaussian vector which probability density function (pdf) depends on a vector of parameters <span class="math inline">\theta=(\beta,\sigma^2)</span>:</p>
<p><span class="math display">\begin{aligned}
\mathbb{P}(y ; \theta) = \prod_{j=1}^n \mathbb{P}(y_j; \theta) &amp; = \prod_{j=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \text{exp}\left(-\frac{1}{2\sigma^2}(y_j - f(x_j, \beta))^2 \right) \\
&amp; =  (2\pi \sigma^2)^{-\frac{n}{2}} \text{exp}\left(-\frac{1}{2\sigma^2}\sum_{j=1}^n(y_j - f(x_j, \beta))^2\right).
\end{aligned}</span></p>
<section id="likelihood" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="likelihood"><span class="header-section-number">3.6</span> Likelihood</h3>
<p>For a given vector of observations <span class="math inline">y</span>, the likelihood <span class="math inline">\ell</span> is the function of the parameter <span class="math inline">\theta=(\beta,\sigma^2)</span> defined as:</p>
<p><span class="math display">
\ell(\theta) = \mathbb{P}(y ; \theta)
</span> The log-likelihood is therefore <span class="math display">
\log\ell(\theta) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{j=1}^n(y_j - f(x_j,\beta))^2
</span></p>
</section>
<section id="maximum-likelihood-estimator" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="maximum-likelihood-estimator"><span class="header-section-number">3.7</span> Maximum likelihood estimator</h3>
<p>Assume that <span class="math inline">\theta</span> takes its values in a subset <span class="math inline">\Theta</span> of <span class="math inline">\mathbb{R}^p</span>. Then, the <em>Maximum Likelihood</em> (ML) estimator of <span class="math inline">\theta</span> is a function of <span class="math inline">y</span> that maximizes the likelihood function:</p>
<p><span class="math display">
\hat{\theta} = \arg\max_{\theta \in \Theta}\ell(\theta) = \arg\max_{\theta \in \Theta}\log\ell(\theta)
</span></p>
<p>Maximization of the log-likelihood can be performed in two steps:</p>
<ul>
<li><span class="math inline">\beta</span>, the parameter of the structural model is estimated by minimizing the residual sum of squares:</li>
</ul>
<p><span class="math display">\begin{aligned}
\hat{\beta} &amp;= \arg\min_{\beta} \left\{
n\log(2\pi) + n\log(\sigma^2) + \frac{1}{\sigma^2}\sum_{j=1}^n(y_j - f(x_j,\beta))^2
\right\} \\
&amp;= \arg\min_{\beta}\sum_{j=1}^n(y_j - f(x_j,\beta))^2
\end{aligned}</span></p>
<p>We see that, for this model, the Maximum Likelihood estimator <span class="math inline">\hat{\beta}</span> is also the Least Squares estimator of <span class="math inline">\beta</span>.</p>
<ul>
<li><span class="math inline">\sigma^2</span>, the variance of the residual errors <span class="math inline">\varepsilon_j</span> is estimated in a second step:</li>
</ul>
<p><span class="math display">\begin{aligned}
\hat{\sigma}^2 &amp;= \arg\min_{\sigma^2 \in \mathbb{R}^+} \left\{
n\log(2\pi) + n\log(\sigma^2) + \frac{1}{\sigma^2}\sum_{j=1}^n(y_j - f(x_j,\hat{\beta}))^2
\right\} \\
&amp;= \frac{1}{n}\sum_{j=1}^n(y_j - f(x_j,\hat{\beta}))^2
\end{aligned}</span></p>
<p>Finally, the log-likelihood computed with <span class="math inline">\hat{\theta}=(\hat{\beta},\hat{\sigma}^2)</span> reduces to <span class="math display">
\log\ell(\hat{\theta}) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\left(\frac{1}{n}\sum_{j=1}^n(y_j - f(x_j,\hat{\beta}))^2\right) -\frac{n}{2}
</span></p>
</section>
<section id="the-fisher-information-matrix" class="level3" data-number="3.8">
<h3 data-number="3.8" class="anchored" data-anchor-id="the-fisher-information-matrix"><span class="header-section-number">3.8</span> The Fisher Information matrix</h3>
</section>
<section id="some-general-definitions" class="level3" data-number="3.9">
<h3 data-number="3.9" class="anchored" data-anchor-id="some-general-definitions"><span class="header-section-number">3.9</span> Some general definitions</h3>
<p>The partial derivative of the log-likelihood with respect to <span class="math inline">\theta</span> is called the <em>score</em>. Under general regularity conditions, the expected value of the score is 0. Indeed, it is easy to show that</p>
<p><span class="math display">\mathbb{E}\left(\frac{\partial}{\partial \theta} \log\mathbb{P}(y;\theta^\star)\right) = 0.</span></p>
<p>where <span class="math inline">\theta^\star</span> is the ``true’’ unknown value of <span class="math inline">\theta</span> such that the observations <span class="math inline">y</span> where generated with model <span class="math inline">\mathbb{P}(\cdot;\theta^\star)</span>.</p>
<p>The variance of the score is called the <em>Fisher information matrix</em> (FIM): <span class="math display">
I_n(\theta^\star) = \mathbb{E}{\left(\frac{\partial}{\partial \theta} \log\mathbb{P}(y;\theta^\star)\right)\left(\frac{\partial}{\partial \theta} \log\mathbb{P}(y;\theta^\star)\right)^\prime}.
</span></p>
<p>Furthermore, it can be shown that if <span class="math inline">\log\ell</span> is twice differentiable with respect to <span class="math inline">\theta</span>,</p>
<p><span class="math display">
\begin{aligned}
I_n(\theta^\star) &amp;= -  \mathbb{E}\left(\frac{\partial^2}{\partial \theta \partial \theta^\prime} \log\mathbb{P}(y;\theta^\star)\right) \\
&amp;= - \sum_{j=1}^n \mathbb{E}\left(\frac{\partial^2}{\partial \theta \partial \theta^\prime} \log\mathbb{P}(y_j;\theta^\star)\right)
\end{aligned}
</span></p>
</section>
<section id="the-central-limit-theorem" class="level3" data-number="3.10">
<h3 data-number="3.10" class="anchored" data-anchor-id="the-central-limit-theorem"><span class="header-section-number">3.10</span> The central limit theorem</h3>
<p>The following central limit theorem (CLT) holds under certain regularity conditions: <span class="math display">
I_n(\theta^\star)^{\frac{1}{2}}(\hat{\theta}-\theta^\star) \xrightarrow{n\to \infty} {\mathcal N}(0,{\rm Id}_n) .
</span> This theorem shows that under relevant hypotheses, the estimator <span class="math inline">\hat{\theta}</span> is consistent and converges to <span class="math inline">\theta^\star</span> at rate <span class="math inline">\sqrt{n}</span> since <span class="math inline">I_n=\mathcal{O}(n)</span>.</p>
<p>The normalizing term <span class="math inline">I_n(\theta^\star)^{-1}</span> is unknown since it depends on the unknown parameter <span class="math inline">\theta^\star</span>. We can use instead the <em>observed Fisher information</em>: <span class="math display">
\begin{aligned}
I_{y}(\hat{\theta}) &amp;= - \frac{\partial^2}{\partial \theta^2} \log\ell(\hat{\theta}) \\
&amp;=-\sum_{i=1}^n \frac{\partial^2}{\partial \theta^2} \log \mathbb{P}(y_i ; \hat{\theta}).
\end{aligned}
</span> We can then approximate the distribution of <span class="math inline">\hat{\theta}</span> by a normal distribution with mean <span class="math inline">\theta^\star</span> and variance-covariance matrix <span class="math inline">I_y(\hat{\theta})^{-1}</span>: <span class="math display">
\hat{\theta} \approx {\mathcal N}(\theta^\star , I_y(\hat{\theta})^{-1}) .
</span> The square roots of the diagonal elements of <span class="math inline">I_y(\hat{\theta})^{-1}</span> are called the <em>standard errors</em> (s.e.) of the elements of <span class="math inline">\hat{\theta}</span>.</p>
</section>
<section id="the-fim-for-a-regression-model" class="level3" data-number="3.11">
<h3 data-number="3.11" class="anchored" data-anchor-id="the-fim-for-a-regression-model"><span class="header-section-number">3.11</span> The FIM for a regression model</h3>
<p>We have seen that, for a regression model, <span class="math display">
\begin{aligned}
\log\ell(\theta) &amp;= \log\ell(\beta,\sigma^2) \\
&amp;= -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{j=1}^n(y_j - f(x_j,\beta))^2
\end{aligned}
</span> By definition,</p>
<p><span class="math display">
I_n(\theta) =  \left( \begin{array}{cc}
-\mathbb{E}{\frac{\partial^2}{\partial \beta \partial \beta^\prime} \log\ell(\beta,\sigma^2)} &amp;
-\mathbb{E}{\frac{\partial^2}{\partial \beta \partial \sigma^2} \log\ell(\beta,\sigma^2)} \\
-\mathbb{E}{\frac{\partial^2}{\partial \sigma^2 \partial \beta^\prime } \log\ell(\beta,\sigma^2)} &amp;  
-\mathbb{E}{\frac{\partial^2}{\partial \sigma^{2^2} } \log\ell(\beta,\sigma^2)}
\end{array} \right)
</span></p>
<p>Then, <span class="math display">
\begin{aligned}
\mathbb{E}{\frac{\partial^2}{\partial \beta \partial \sigma^2} \log\ell(\beta,\sigma^2)} &amp;=
-\frac{1}{\sigma^4} \times\frac{\partial}{\partial \beta}f(x_j,\beta) \times\mathbb{E}{y_j - f(x_j,\beta)} \\
&amp;= 0
\end{aligned}
</span></p>
<p>and the FIM reduces to</p>
<p><span class="math display">
I_n(\theta) =  \left( \begin{array}{cc}
-  \mathbb{E}{\frac{\partial^2}{\partial \beta \partial \beta^\prime} \log\ell(\beta,\sigma^2)} &amp; 0 \\
0 &amp;  -\mathbb{E}{\frac{\partial^2}{\partial \sigma^{2^2} } \log\ell(\beta,\sigma^2)}
\end{array} \right)
</span></p>
<p>Because of the bloc structure of <span class="math inline">I_n(\theta^\star)</span>, the variance-covariance of <span class="math inline">\hat{\beta}</span> can be estimated by <span class="math inline">I^{-1}_y(\hat{\beta})</span> where <span class="math display">
\begin{aligned}
I_y(\hat{\beta}) &amp;= - \frac{\partial^2}{\partial \beta \partial \beta^\prime} \log\ell(\hat{\beta},\hat{\sigma}^2) \\
&amp;= \frac{1}{2\hat\sigma^2}\frac{\partial^2}{\partial \beta \partial \beta^\prime} \left(\sum_{j=1}^n(y_j - f(x_j,\hat\beta))^2 \right) \\
&amp;= \frac{1}{\hat\sigma^2} \sum_{j=1}^n \left(
\left(\frac{\partial}{\partial \beta}f(x_j,\hat\beta)\right)\left(\frac{\partial}{\partial \beta}f(x_j,\hat\beta)\right)^\prime - \frac{\partial^2}{\partial \beta \partial \beta^\prime}f(x_j,\hat\beta)y_j
\right)
\end{aligned}
</span> <strong>Remark:</strong> In the case of a linear model <span class="math inline">y=X\beta+e</span>, we find that <span class="math inline">I_y(\hat{\beta}) = (X^\prime X)/\hat\sigma^2</span>.</p>
<p>The variance of <span class="math inline">\hat{\sigma}^2</span> is estimated by <span class="math inline">I^{-1}_y(\hat{\sigma}^2)</span> where <span class="math display">
\begin{aligned}
I_y(\hat{\sigma}^2) &amp;= -\frac{\partial^2}{\partial \sigma^{2^2} } \log\ell(\hat{\beta},\hat{\sigma}^2) \\
&amp;= -\frac{n}{2\hat\sigma^4} + \frac{1}{\hat\sigma^6}\sum_{j=1}^n(y_j - f(x_j,\hat\beta))^2 \\
&amp;= \frac{n}{2\hat\sigma^4}
\end{aligned}
</span></p>
<p>Then, <span class="math inline">{\rm se}(\hat{\sigma}^2) = \hat{\sigma}^2/\sqrt{n/2}</span>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../docs/regression/map566-lab-nonlinear-regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Nonlinear Regression</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>