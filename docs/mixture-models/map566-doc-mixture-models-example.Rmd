---
title: "Mixture Models: an example"
---

## Preliminary {.unnumbered}

Functions from `R`-base and stats (preloaded) are required plus packages from the **tidyverse** for data representation and manipulation. We also need the packages **mixtools** and **mclust**, which are commonly used to fit mixture models in `R`.

```{r tests-config, message = FALSE}
#| code-fold: false
library(tidyverse)
library(gridExtra)
library(mixtools)
theme_set(theme_bw())
```


## The Iris data

The famous (Fisher's or Anderson's) iris data set gives the measurements in centimeters of the variables <a href="http://en.wikipedia.org/wiki/Sepal" target="_blank" >sepal</a> length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. (from `?iris`).

The Iris flower data set is a nice example for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning.

```{r, fig.height=5,fig.width=9}
iris <- datasets::iris
head(iris)

species_labels <- iris[,5]
species_col <- c("#7DB0DD","#86B875","#E495A5")

pairs(iris[,-5], col = species_col[species_labels],
     lower.panel = NULL, cex.labels=2, pch=19, cex = 1)

par(xpd = TRUE)
legend(x = 0.1, y = 0.4, cex = 1.2,
       legend = as.character(levels(species_labels)), fill = species_col)
```

We can see that the Setosa species are distinctly different from Versicolor and Virginica (they have lower petal length and width). But Versicolor and Virginica cannot easily be separated based on measurements of their sepal and petal width/length.

The same conclusion can be made by looking at the parallel coordinates plot of the data:

```{r}
# https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html
MASS::parcoord(iris[,-5], col = species_col[species_labels], var.label = TRUE, lwd = 2)

# Add a legend
par(xpd = TRUE)
legend(x = 1.75, y = -.25, cex = 1,
   legend = as.character(levels(species_labels)),
    fill = species_col, horiz = TRUE)
```

### Supervised classification 

#### Logistic regression for a binary variable

Let $y_i$ be a binary response that take its values in $\{0,1\}$ and let $c_{i1}, \ldots, c_{iM}$ be $M$ explanatory variables (or predictors).

Formally, the logistic regression model is that

$$\begin{aligned}
 \log\left(\frac{\mathbb{P}(y_i=1)}{\mathbb{P}(y_i=0)}\right) 
&= \log\left(\frac{\mathbb{P}(y_i=1)}{1 - \mathbb{P}(y_i=1)}\right) \\
&= \beta_0 + \sum_{m=1}^M \beta_m c_{im}
\end{aligned}$$
Then,
$$\mathbb{P}(y_i=1) = \frac{1}{1+ e^{-\beta_0 -  \sum_{m=1}^M \beta_m c_{im}}}$$

```{r, warning=FALSE}
myiris <- cbind(iris,virginica=ifelse(iris$Species=="virginica",1,0))
myiris$virginica
iris.glm <- glm(virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, family=binomial, data=myiris)
summary(iris.glm)
as.vector(round(fitted(iris.glm)))
```

```{r}
iris.glm$linear.predictors[71]
sum(cbind(1,iris[71,1:4])*iris.glm$coefficients)
```

```{r}
iris.glm$fitted.values[71]
1/(1+exp(-iris.glm$linear.predictors[71]))
```


#### Logistic regression with more than two classes

Assume now that $y_i$ takes its values in $\{1,2\ldots,L\}$.
The logistic regression model now writes


$$\begin{aligned}
 \log\left(\frac{\mathbb{P}(y_i=k)}{\mathbb{P}(y_i=L)}\right) 
&= \beta_{k0} +  \sum_{m=1}^M \beta_{k m} c_{im} \quad , \quad k=1,2,\ldots,L
\end{aligned}$$
where we set, for instance, $\beta_{L0}=\beta_{L1}=\ldots=\beta_{LM}=0$ for identifiabilty reason. Then,

$$
 \mathbb{P}(y_i=k) = \frac{e^{\beta_{k0} +  \sum_{m=1}^M \beta_{k m} c_{im}}}
 {\sum_{j=1}^K e^{\beta_{j0} +  \sum_{m=1}^M \beta_{j m} c_{im}}}
 \quad , \quad k=1,2,\ldots,L
$$

```{r, message=FALSE, warning=FALSE}
library(VGAM)
myiris <- cbind(myiris,versicolor=ifelse(iris$Species=="versicolor",1,0))
myiris <- cbind(myiris,setosa=ifelse(iris$Species=="setosa",1,0))

head(myiris)

iris.vglm <- vglm(cbind(virginica,versicolor,setosa) ~ Sepal.Length +
Sepal.Width + Petal.Length + Petal.Width, family=multinomial,
data=myiris)

summary(iris.vglm)
```

```{r}
iris.vglm@predictors[71,]
c(sum(cbind(1,iris[71,1:4])*iris.vglm@coefficients[c(1,3,5,7,9)]), 
  sum(cbind(1,iris[71,1:4])*iris.vglm@coefficients[c(2,4,6,8,10)]))
```

```{r}
iris.vglm@fitted.values[71,]
lp71 <- as.vector(c(iris.vglm@predictors[71,],0))
exp(lp71)/sum(exp(lp71))
```


```{r}
head(fitted(iris.vglm))
```

Percentage of missclassified flowers:
```{r}
# Now we use 'fitted.iris' as the model-based classifications
# of which flower is of each type.  How does this compare
# to the real types?


rclass.iris <- ifelse(apply(abs(round(fitted(iris.vglm)) - myiris[,6:8]),1,sum)==0,1,0)
(1-mean(rclass.iris))*100
```

### Non supervised classification

Ignoring the known labels (species) of theFisher Iris data, let us identify three clusters with the $k$-means method and compute the missclassification rate:
```{r, warning=FALSE}
set.seed(1234) # labels are the original ones with this seed (avoid permutation)
r.km <- kmeans(iris[,1:4], centers=3)
mean(r.km$cluster!=as.numeric(iris$Species))*100
```

Let us know fit a mixture of three multidimensional Gaussian distributions.
The model assumes the same variance covariance matrix for the three distributions (<ttt>arbvar=FALSE).
Initial centers are those given by the kmeans procedure.
```{r}
library(mixtools)
c0 <- list(r.km$centers[1,], r.km$centers[2,], r.km$centers[3,])
r.em <- mvnormalmixEM(iris[,1:4], mu=c0, arbvar=FALSE)
```
Let us display the estimated parameters of the mixture:
```{r}
r.em$lambda
r.em$mu
r.em$sigma
```

For each flower, the estimated label maximizes the posterior distribution. We can then compute the missclassification rate:
```{r}
post <- r.em$posterior
r.em$class <- sapply(seq(nrow(post)), function(i) { j <- which.max(as.vector(post[i,])) })
mean(r.em$class!=as.numeric(iris$Species))*100
```

Let us plot the original data with the three distributions estimated with EM
```{r, message=FALSE, warning=FALSE}
detach(package:mixtools)
library(ellipse)
library(gridExtra)

Species <- levels(iris$Species)
j1=1;  j2=2
df_ell <- data.frame()
for(g in (1:3)){
#  M=r.em$sigma[[g]][c(j1,j2),c(j1,j2)]
  M=r.em$sigma[c(j1,j2),c(j1,j2)]
  c=r.em$mu[[g]][c(j1,j2)]
  df_ell <- rbind(df_ell, cbind(as.data.frame(ellipse(M,centre=c, level=0.68)), group=Species[g]))
}
pl1 <- ggplot(data=iris) + geom_point(aes_string(x=iris[,j1],y=iris[,j2], colour=iris[,5])) + 
  theme(legend.position="none") +xlab(names(iris)[j1])+ylab(names(iris)[j2]) +
geom_path(data=df_ell, aes(x=x, y=y,color=group), size=1, linetype=1) 

j1=3;  j2=4
df_ell <- data.frame()
for(g in (1:3)){
#  M=r.em$sigma[[g]][c(j1,j2),c(j1,j2)]
  M=r.em$sigma[c(j1,j2),c(j1,j2)]
  c=r.em$mu[[g]][c(j1,j2)]
  df_ell <- rbind(df_ell, cbind(as.data.frame(ellipse(M,centre=c, level=0.68)), group=Species[g]))
}
pl2 <- ggplot(data=iris) + geom_point(aes_string(x=iris[,j1],y=iris[,j2], colour=iris[,5])) + 
  theme(legend.position="none") +xlab(names(iris)[j1])+ylab(names(iris)[j2]) +
geom_path(data=df_ell, aes(x=x, y=y,color=group), size=1, linetype=1) 

grid.arrange(pl1, pl2)
```


This Shiny app allows one to select the two variables to plot.

<iframe src="http://shiny.webpopix.org/sia/mm3/" style="border: none; width: 800px; height: 550px"></iframe>