---
title: "Clustering and classification with model based approaches" 
subtitle: "An short example study on the penguins dataset"
---

## Preliminary {.unnumbered}

Functions from `R`-base and stats (preloaded) are required plus packages from the **tidyverse** for data representation and manipulation. We also need the package **mclust**, which are commonly used to fit mixture models in `R`, as weel as **palmerpenguins** for the illustrative data set. **aricode** is used for clustering comparison, **VGAM** to fit multinomial models.

```{r tests-config, message = FALSE}
#| code-fold: false
library(tidyverse)
library(gridExtra)
library(GGally)
library(mclust)
library(aricode)
library(VGAM)
library(palmerpenguins)
theme_set(theme_bw())
```

## The Palmer penguins data set

The  `palmerpenguins` data [^1] (@penguins) contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. 

These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the [Palmer Station Long Term Ecological Research Program](https://pal.lternet.edu/), part of the [US Long Term Ecological Research Network](https://lternet.edu/). The data were imported directly from the [Environmental Data Initiative](https://environmentaldatainitiative.org/) (EDI) Data Portal, and are available for use by CC0 license ("No Rights Reserved") in accordance with the [Palmer Station Data Policy](https://pal.lternet.edu/data/policies). 

This data et is an alternative to Anderson's **Iris** data (see `datasets::iris`). There are both a nice example for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning.

[^1]: [more to read here](https://allisonhorst.github.io/palmerpenguins/)

```{r}
data("penguins", package = "palmerpenguins")
penguins %>% rmarkdown::paged_table()
```

We remove row with `NA` values and columns `year`, `island` and `sex` to only keep the four continuous attributes and the species of each individual:

```{r}
penguins <- penguins %>% drop_na() %>% 
  select(-year, -island, -sex)

```

The pair plot show some structure that could find by clustering or descreibe by a Gaussian mixture model: 

```{r, fig.height=7,fig.width=9}

species_col <- c("darkorange","purple","cyan4")
ggpairs(penguins, columns = c(2:5), aes(color = species)) + 
    scale_color_manual(values = species_col) +
    scale_fill_manual(values = species_col)
```

### Supervised classification 

#### Logistic regression for a binary variable

Let $y_i$ be a binary response that take its values in $\{0,1\}$ and let $c_{i1}, \ldots, c_{iM}$ be $M$ explanatory variables (or predictors).

Formally, the logistic regression model is that

$$
 \log\left(\frac{\mathbb{P}(y_i=1)}{\mathbb{P}(y_i=0)}\right) = \log\left(\frac{\mathbb{P}(y_i=1)}{1 - \mathbb{P}(y_i=1)}\right) 
 = \beta_0 + \sum_{m=1}^M \beta_m c_{im}
$$
Then,

$$\mathbb{P}(y_i=1) = \frac{1}{1+ e^{-\beta_0 -  \sum_{m=1}^M \beta_m c_{im}}}$$

We try to predict the binary indicator variable for species `Adelie` with this model:

```{r, warning=FALSE}
penguins <- penguins %>% mutate(adelie = (species == 'Adelie') * 1) 
logistic_Adelie <- glm(
  adelie ~ 
    bill_length_mm + 
    bill_depth_mm  + 
    flipper_length_mm + 
    body_mass_g, family = binomial, data = penguins)
summary(logistic_Adelie)

```

The linear predictor can be recovered as follows (e.g. for penguins #127)

```{r}
X <- as.matrix(cbind(1, penguins[, 2:5]))
beta <- coefficients(logistic_Adelie)
all.equal(
  predict(logistic_Adelie, type = "link"), 
  as.numeric(X %*% beta),
  check.attributes = FALSE)
```
The fitted value as follows:

```{r}
all.equal(
  predict(logistic_Adelie, type = "response"),
  as.numeric(1 / ( 1 + exp( - (X %*% beta)))),
 check.attributes = FALSE)
```

The ARI is close to 1 in that case

```{r}
aricode::ARI(penguins$adelie, round(predict(logistic_Adelie, type = "response")))
```

#### Logistic regression with more than two classes

Assume now that $y_i$ takes its values in $\{1,2\ldots,L\}$.
The logistic regression model now writes


$$ \log\left(\frac{\mathbb{P}(y_i=k)}{\mathbb{P}(y_i=L)}\right) 
= \beta_{k0} +  \sum_{m=1}^M \beta_{k m} c_{im} \quad , \quad k=1,2,\ldots,L
$$
where we set, for instance, $\beta_{L0}=\beta_{L1}=\ldots=\beta_{LM}=0$ for identifiabilty reason. Then,

$$
 \mathbb{P}(y_i=k) = \frac{e^{\beta_{k0} +  \sum_{m=1}^M \beta_{k m} c_{im}}}
 {\sum_{j=1}^K e^{\beta_{j0} +  \sum_{m=1}^M \beta_{j m} c_{im}}}
 \quad , \quad k=1,2,\ldots,L
$$
Let us first code all species modalities as dummy variables:

```{r, message=FALSE, warning=FALSE}
penguins <- penguins %>% 
  mutate(gentoo    = (species == 'Gentoo') * 1) %>% 
  mutate(chinstrap = (species == 'Chinstrap') * 1) 
penguins %>% rmarkdown::paged_table()
```

We now fit a multinomial model to this 3-class problem:

```{r, warning=FALSE}
multinomial_penguins <- 
  vglm(cbind(gentoo, adelie, chinstrap) ~ 
    bill_length_mm + 
    bill_depth_mm  + 
    flipper_length_mm + 
    body_mass_g, family = multinomial, data = penguins)
summary(multinomial_penguins)
```

```{r}
posterior_prob <- predict(multinomial_penguins, type = "response")
matplot(posterior_prob)
clustering_map <- apply(posterior_prob, 1, which.max)
```

we get a perfect clustering of the penguins with this model (on the training set!)

```{r}
clustering_map <- apply(posterior_prob, 1, which.max)
aricode::ARI(clustering_map, penguins$species)
```


### Non supervised classification

Ignoring the known labels (species) of the penguin data, let us identify three clusters with the $k$-means method and compute the missclassification rate:

```{r, warning=FALSE}
kclust <- kmeans(penguins[, 2:5], centers = 3, nstart = 10)
aricode::ARI(kclust$cl,  penguins$species)
```

Let us know fit a mixture of three multidimensional Gaussian distributions.

Function `Mclust` fits many multivariate Gaussian mixture model, with various parametric form for the covariances. Let us force the model to have spherical variance with equal volume (thus close to the k-means setting):

```{r}
GMM_EII <- Mclust(penguins[, 2:4], G = 3, modelNames = 'EII')
```

The fit is poor

```{r}
plot(GMM_EII, "classification")
```

```{r}
aricode::ARI(penguins$species, map(GMM_EII$z))
```

We can let Mclust chose the "best" model, relying on BIC: we found "VVE", an ellipsoidal model with equal orientation 

```{r}
GMM_best <- Mclust(penguins[, 2:4], G = 3)
plot(GMM_best, "classification")
summary(GMM_best, parameters = TRUE)
```


We get an almost perfect clustering with the MAP: 

```{r}
aricode::ARI(penguins$species, map(GMM_best$z))
```

## References {-}

