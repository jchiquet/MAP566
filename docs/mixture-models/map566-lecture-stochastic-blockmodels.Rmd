---
title: "Graph clustering"
subtitle: "Lecture Notes"
---

## Preliminary {.unnumbered}

Functions from `R`-base and stats (preloaded) are required plus packages from the **tidyverse** for data representation and manipulation. The package **igraph** is a great library for network data manipulation (interface exists in `Python`)

We will also use the package **mixtools**, which implements EM for simple mixture models to check our own implementation.

:::{.hidden}
$$
\newcommand{\Rset}{\mathbb{R}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Nset}{\mathbb{N}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\var}{\mathbb{V}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\cor}{\mathrm{cor}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Beta}{B}
\newcommand{\Nbb}{\mathbb{N}}

\newcommand{\projorth}[2]{\text{proj}^{\bot}_{#2}(#1)}
\newcommand{\proj}[2]{\text{proj}_{#2}(#1)}
\newcommand{\argmax}{\mathop{\mathrm{arg\ max}}}
\newcommand{\argmin}{\mathop{\mathrm{arg\ min}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\newcommand{\maximize}{\mathop{\mathrm{maximize}}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\distance}{\text{dist}}
% definitions related to 
\newcommand{\norm}[2][]{\left|\left|#2\right|\right|_{#1}}
\newcommand{\group}[1][k]{{\mathcal G}_{#1}}
\newcommand{\positive}{{\mathcal P}}
\newcommand{\negative}{{\mathcal N}}
\newcommand{\zero}{{\mathcal Z}}
%\renewcommand{\active}[1][k]{{\mathcal A}_{#1}}
\newcommand{\1}{\mathbf{1}}

\newcommand{\tr}{\mathrm{tr}}
\newcommand{\trace}[1]{\mathrm{trace}{\left(#1\right)}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\err}{\mathrm{err}}
\newcommand{\weights}{\mathbf{w}}
\newcommand{\supp}{\mathcal{A}}
\newcommand{\prob}{\mathbb{P}}
\renewcommand{\P}{\mathbb{P}}

%definitions related to convergences
\newcommand{\inprob}{\overset{P}{\longrightarrow}}
\newcommand{\inlaw}{\overset{D}{\longrightarrow}}

\newcommand{\C}{\texttt}
\newcommand{\R}{\C{R}}
\newcommand{\easy}{\mbox{\large\fontencoding{U}\fontfamily{wasy}\selectfont\char44}}
\newcommand{\medium}{\mbox{\large\fontencoding{U}\fontfamily{wasy}\selectfont\char47}}
\newcommand{\hard}{\raisebox{2pt}{\footnotesize\fontencoding{U}\fontfamily{futs}\selectfont\char77}}

\newcommand{\Easy}{\mbox{\Huge\fontencoding{U}\fontfamily{wasy}\selectfont\char44}}
\newcommand{\Medium}{\mbox{\Huge\fontencoding{U}\fontfamily{wasy}\selectfont\char47}}
\newcommand{\Hard}{\raisebox{3pt}{\huge\fontencoding{U}\fontfamily{futs}\selectfont\char77}}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green!50!black}{#1}}

\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

%-------------------------------------------------------------------------%
% Definitions
%-------------------------------------------------------------------------%
\def\Argmin{\mathop{\mbox{\rm argmin}}}
\def\Argmax{\mathop{\mbox{\rm argmax}}}

\newcommand{\hatbeta}{\hat{\beta}}
\newcommand{\hatbbeta}{\,\hat{\!\bbeta}}
\newcommand{\hatbetalasso}{\,\hat{\!\beta}^{\mathrm{lasso}}}
\newcommand{\hatbetacoop}{\,\hat{\!\beta}^{\mathrm{coop}}}
\newcommand{\hatbetagroup}{\,\hat{\!\beta}^{\mathrm{group}}}
\newcommand{\hatbetaspgroup}{\,\hat{\!\beta}^{\mathrm{spgroup}}}
\newcommand{\hatbetaridge}{\,\hat{\!\beta}^{\mathrm{ridge}}}
\newcommand{\hatbetaols}{\,\hat{\!\beta}^{\mathrm{ols}}}
\newcommand{\hatbbetacoop}{\,\hat{\!\bbeta}^{\mathrm{coop}}}
\newcommand{\hatbbetagroup}{\,\hat{\!\bbeta}^{\mathrm{group}}}
\newcommand{\hatbbetaridge}{\,\hat{\!\bbeta}^{\mathrm{ridge}}}
\newcommand{\hatbbetalasso}{\,\hat{\!\bbeta}^{\mathrm{lasso}}}
\newcommand{\hatbbetaols}{\,\hat{\!\bbeta}^{\mathrm{ols}}}
\newcommand{\hatbbetamv}{\,\hat{\!\bbeta}^{\mathrm{mv}}}
\newcommand{\tildebbeta}{\,\tilde{\!\bbeta}}
\newcommand{\bbetaridge}{\bbeta^{\mathrm{ridge}}}
\newcommand{\bbetacoop}{\bbeta^{\mathrm{coop}}}
\newcommand{\bbetagroup}{\bbeta^{\mathrm{group}}}
\newcommand{\bbetaols}{\bbeta^{\mathrm{ols}}}

\newcommand{\hattheta}{\hat{\theta}}
\newcommand{\hatbtheta}{\,\hat{\!\btheta}}
\newcommand{\hatthetalasso}{\,\hat{\!\theta}^{\mathrm{lasso}}}
\newcommand{\hatthetacoop}{\,\hat{\!\theta}^{\mathrm{coop}}}
\newcommand{\hatthetagroup}{\,\hat{\!\theta}^{\mathrm{group}}}
\newcommand{\hatthetaspgroup}{\,\hat{\!\theta}^{\mathrm{spgroup}}}
\newcommand{\hatthetaridge}{\,\hat{\!\theta}^{\mathrm{ridge}}}
\newcommand{\hatthetaols}{\,\hat{\!\theta}^{\mathrm{ols}}}
\newcommand{\hatbthetacoop}{\,\hat{\!\btheta}^{\mathrm{coop}}}
\newcommand{\hatbthetagroup}{\,\hat{\!\btheta}^{\mathrm{group}}}
\newcommand{\hatbthetaridge}{\,\hat{\!\btheta}^{\mathrm{ridge}}}
\newcommand{\hatbthetalasso}{\,\hat{\!\btheta}^{\mathrm{lasso}}}
\newcommand{\hatbthetaols}{\,\hat{\!\btheta}^{\mathrm{ols}}}
\newcommand{\hatbthetamv}{\,\hat{\!\btheta}^{\mathrm{mv}}}
\newcommand{\tildebtheta}{\,\tilde{\!\btheta}}
\newcommand{\bthetaridge}{\btheta^{\mathrm{ridge}}}
\newcommand{\bthetacoop}{\btheta^{\mathrm{coop}}}
\newcommand{\bthetagroup}{\btheta^{\mathrm{group}}}
\newcommand{\bthetaols}{\btheta^{\mathrm{ols}}}

\newcommand{\bTheta}{\boldsymbol\Theta}

\newcommand{\transpose}[1]{\matr{#1}^\trans}
\newcommand{\crossprod}[2]{\transpose{#1} \matr{#2}}
\newcommand{\tcrossprod}[2]{\matr{#1} \transpose{#2}}

\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\bPsi}{\boldsymbol\Psi}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bL}{\mathbf{L}}

\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}
\newcommand{\invcov}{\bOmega}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\bomega}{\boldsymbol{\omega}}

\newcommand{\tP}{\tilde{p}}
\newcommand{\diag}{\text{diag}}

\newcommand{\I}{\mathbf{1}}
\newcommand{\clG}{\mathcal{G}}
\newcommand{\clV}{\mathcal{V}}
\newcommand{\clE}{\mathcal{E}}
\newcommand{\clJ}{\mathcal{J}}
\newcommand{\clN}{\mathcal{N}}
\newcommand{\clP}{\mathcal{P}}
\newcommand{\clA}{\mathcal{A}}
\newcommand{\clD}{\mathcal{D}}
\newcommand{\clH}{\mathcal{H}}
\newcommand{\clO}{\mathcal{O}}
\newcommand{\clS}{\mathcal{S}}
\newcommand{W}{\mathbf{W}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{W}{\mathbf{y}}
\newcommand{W}{\mathbf{Y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bzr}{\mathbf{0}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bzero}{\boldsymbol 0}
\newcommand{\balpha}{\boldsymbol\alpha}
\newcommand{\bkappa}{\boldsymbol\kappa}
\newcommand{\bvarphi}{\boldsymbol\varphi}
\newcommand{\btheta}{\boldsymbol\theta}
\newcommand{\bgamma}{{\boldsymbol\gamma}}
\newcommand{\bepsilon}{\boldsymbol\epsilon}
\newcommand{\bvarepsilon}{\boldsymbol\varepsilon}
\newcommand{\blambda}{\boldsymbol\lambda}
\newcommand{\rsa}{\emphase{\mathversion{bold}{$\rightsquigarrow$}~}}
$$
:::

```{r tests-config, message = FALSE}
#| code-fold: false
library(tidyverse)
library(igraph)
library(aricode)
library(missSBM)
library(sbm)
theme_set(theme_bw())
```

```{r pdf-setting}
options(tinytex.engine = 'xelatex')
```

## Introduction

### Network data and binary graphs: minimal notation

A **network** is a collection of interacting entities. A **graph** is the mathematical representation of a network.

In what follow, a graph $\clG=(\clV,\clE)$ is a mathematical structure consisting of

  - a set $\clV=\set{1,\dots,n}$ of vertices or **nodes**
  - a set $\clE=\set{e_1,\dots,e_p:e_k=(i_k,j_k)\in (\clV\times\clV)}$ of **edges**
  - the number of vertices $|\clV|$ is called the **order**
  - the number of edges $|\clE|$ is called the **size**
  

The connectivity of a binary undirected (symmetric) graph $\clG = (\clV,\clE)$ is captured by the $|\clV|\times |\clV|$ matrix $Y$, called the adjacency matrix
$$
  (Y)_{ij} = \begin{cases}
  1  & \text{ if } i \sim j,\\
  0  & \text{otherwise}.
\end{cases}
$$
For a valued of weighted graph, a similar definition would be

$$
  (Y)_{ij} = \begin{cases}
  w_{ij}  & \text{ if } i \sim j,\\
  0  & \text{otherwise}.
\end{cases}
$$
where $w_{ij}$ is the weight associated with edge $i\sim j$.

### The French political Blogosphere

The `frenchblog2007` data is a network dataset which consists of a single day snapshot of over 200 political blogs automatically extracted the 14 October 2006 and manually classified by the "Observatoire PrÃ©sidentielle" project. It is part of the **missSBM** package. It is provided as an **igraph** object with 196 nodes. The vertex attribute "party" provides a classification of the nodes.

```{r mixture-faitful-load}
data("frenchblog2007")
summary(frenchblog2007)
igraph::V(frenchblog2007)$party %>% table() %>% as_tibble() %>% rmarkdown::paged_table()
```

A visual representation of the network data with nodes colored according to the political party each blog belongs to is achieved as follows:

```{r frenchblog-igraph-plot}
#| code-fold: TRUE
plot.igraph(frenchblog2007,
  vertex.color = factor(V(frenchblog2007)$party),
  vertex.label = NA
 )
```

Another commonly used representation is via a matrix view, where the adjacency matrix is re-ordered column-wise and row-wise according to a predefined classification. In the `frenchblog2007` data, nodes are originally reordered according to their party:

```{r frenchblog-matrix-plot}
#| code-fold: TRUE
frenchblog2007 %>% as_adj(sparse = FALSE) %>% plotMyMatrix()
```

:::{.callout-warning}
In this example, one can see that the pattern of connections between the nodes is highly related to the blog classification (the political party). However, just like with any kind of clustering, this is note always the case: the data may support a natural grouping of the node which is not necessarily related a predefined classification.
:::

:::{.callout-note}
For convenience, in the following,

  - we remove the isolated nodes or node with degree equal to one[^1]
  - we denote by $Y$ the adjacency matrix encoding the network
  - we extract the political party of the nodes as a categorical variable 

** Our objective is now to automatically find a partitioning of the node, i.e. a clustering, that groups together nodes with similar connectivity pattern. This is known as graph clustering.**

:::

[^1]: A "nice" side-effect is that the 'far-right' blogs have removed from the study. Amazing for data collected only a decade ago...

```{r export data}
blog <- frenchblog2007 %>%  delete_vertices(which(degree(frenchblog2007) <= 1))
party <- V(blog)$party %>% as_factor()
Y     <- blog %>% as_adjacency_matrix()
n_nodes <- gorder(blog)
n_edges <- gsize(blog)
party %>% table() %>% as_tibble() %>% rmarkdown::paged_table()
```

## Spectral Clustering

We start by a popular algorithm which can be seen as the equivalent of k-means algorithm for clustering network data: **the spectral clustering** (see @von2007tutorial). This algorithm is based on the spectral properties of graph, and in particular of the Laplacian matrix, which we briefly recap here. A detail introduction is made by [@chung1997spectral].

Here, we motivate the introduction of the Laplacian matrix via the graph-cut problem:

### Graph-cut

First, we need to measure the importance or quantity of information associated to a node or a subset of nodes in the graph. The degree is a natural candidate: we define

$$
\begin{aligned}
\mathrm{degree}_i & = d_i = \sum_{j} Y_{ij}, \\
\mathrm{Vol}(\mathcal{S}) & = \sum_{i\in\mathcal{S}} d_i , \\
\end{aligned}
$$
where the volume of a subset $\mathcal{S}$  of nodes is the cumulated degrees[^2].

[^2]: Note that this definition works for weighted graphs.

For instance, in the French blog data set, the volume associated to each party would be

```{r volume_party}
degree(blog) %>% split(party) %>% map_dbl(sum) %>% 
  as_tibble() %>% rmarkdown::paged_table()
```


Second, let us define the cut between two set of nodes that form a partition in the graph: 

$$
\mathrm{cut}(\mathcal{V}_A, \mathcal{V}_B) = \sum_{i\in\clV_A, j\in\clV_B} Y_{ij}, \qquad \clV_A \cup \clV_B = \clV
$$
that is, the cut is the sum of the weights of the edge set that connect the two components $clV_A$ and $\clV_B$. For instance, in this simple binary graph, the graph cut between $\clV_A= \{1,2,3,4,10\}$ and $\clV_B= \{5,6,7,8,9\}$ is 2.

```{r graph-cut-plot, echo = FALSE}
g <- graph.formula(1-2, 1-3, 1-4, 1-5, 2-3, 2-4, 3-4, 3-5, 5-2, 5-4, 10-6, 10-7, 10-8, 10-9, 6-7, 6-8, 6-9, 7-8, 7-9, 8-9, 2-6, 3-7)
plot(g)
```

We can easily define a function to compute the cut

```{r cut-function}
cut <- function(graph, A, B) {
  res <- sum(as_adj(graph, type = "upper")[A , B]) 
  res
}
V <- V(g)$name
A <- V[1:5]
B <- setdiff(V, A)
cut(g, A, B)
```



:::{.callout-note}
## Idea

A natural criterion to cluster a graph into two homogeneous groups of node is to find the two sets (the partition) that minimizes the cut. 

Based on this principle, the normalized cut  consider the connectivity between group relative to the volume of each groups:

$$
\begin{aligned}
\argmin_{\{\clV_A, \clV_B\}} \mathrm{cut}^{N}(\clV_A, \clV_B),  
\quad \mathrm{cut}^{N}(\clV_A, \clV_B) & = \frac{\mathrm{cut}(\clV_A, \clV_B)}{\mathrm{Vol}(\clV_A)} + \frac{\mathrm{cut}(\clV_A, \clV_B)}{\mathrm{Vol}(\clV_B)} \\
 & =  \mathrm{cut}(\clV_A, \clV_B)\frac{\mathrm{Vol}(\clV_A) + \mathrm{Vol}(\clV_B)}{\mathrm{Vol}(\clV_A)\mathrm{Vol}(\clV_B)} \\
\end{aligned}
$$

:::

Our function is easily amende to compute the normalized version of the graph-cut:

```{r n-cut-function}
cut <- function(graph, A, B, normalized  = TRUE) {

  Y <- as_adj(graph, type="upper")
  res <- sum(Y[A , B]) 

  if (normalized) {
    volA <- sum(Y[A, A])
    volB <- sum(Y[B, B])
    res <- res * (volA + volB) / (volA * volB)
  }

  res
}
A <- sample(1:gorder(blog), 100)
B <- setdiff(1:gorder(blog), A)
cut(blog, A, B)
```


The above problem can be formalized as follows: a partition into two clusters of the graph can be defined by a vector of $\{-1, 1\}^n$. Indeed, 

$$
x = (x_i)_{i=1,\dots,n} = 
\begin{cases}
-1 & \mathrm{if} \quad  i\in \clV_A, \\
 1 & \mathrm{if} \quad  i\in \clV_B. \\
\end{cases}
$$
Then, letting $D$ the diagonal matrix of degrees, is not difficult to show that[^3]

[^3]: let as an exercise

$$
x^\top (D - Y) x = x^\top D x - ( x^\top D x - 2 \mathrm{cut} (\clV_A, \clV_B)),
$$
so that 

$$
\mathrm{cut} (\clV_A, \clV_B) = \frac12 x^\top (D - Y) x.
$$
From this, we can show that minimizing the normalized graph-cut is equivalent to solving an integer programming problem:

$$\begin{aligned}
& \argmin_{\{\clV_A, \clV_B\}}  \mathrm{cut}^{N}(\clV_A, \clV_B) \\[1.5ex]
\Leftrightarrow \quad & \argmin_{x\in\{-1, 1\}^n} \frac{x^\top (D - Y) x}{x^\top D x}, \quad \text{s.c.} \quad x^\top D \mathbf{1}_n = 0, 
\end{aligned}
$$
where the constraint imposes only discrete values in $x$. 

This problem is combinatorial (and NP-hard). However, if we relax to $x\in[-1,1]^n$, it turns to a simple eigenvalue problem

$$
\argmin_{x\in[-1, 1]^n} x^\top (D - Y) x, \quad \text{s.c.} \quad x^\top D x = 1 \Leftrightarrow (D - Y) x = \lambda D x .
$$
where $\bL = D - Y$ is called the **Laplacian matrix** of the graph $\mathcal{G}$.

::: {.callout-tip}
## Proposition: Spectrum of $\bL$
  
  The $n\times n$ matrix $\bL$ has the following properties:
$$
  \bx^\top \bL \bx = \frac{1}{2} \sum_{i,j} Y_{ij} (x_i - x_j)^2, \quad \forall \bx\in\Rset^n .
$$
    
  - $\bL$ is a symmetric, positive semi-definite matrix,
  - $\mathbf{1}_n$ is in the kernel of $L$ since $L \mathbf{1}_n = 0$,
  - The first normalized eigen vector with eigen value $\lambda> 0$ is solution to the relaxed graph cut problem

:::

The Laplacian is easily (and fastly) computed in `R` thanks to the igraph package. Let us compute this for the French blog graph:

```{r compute-laplacian}
L <- laplacian_matrix(blog)
```

:::{.callout-note}
## Heuristics for spectral clustering

Spectral clustering exploits the spectral property of $\bL$, by building heuristic based on the above properties. We review some variants in what follows.

:::

### Bi-partionning and the Fiedler vector

The Fiedler vector is the named sometimes given to the normalized eigen vector associated with the smallest **positive** eigen-value of $\bL$. It thus solves the above relaxed graph-cut problem and  can be used to compute a bi-partition of a graph.

Let us check how we can use theses quantities to partition the French blogosphere.

We first extract the Fiedler vector

```{r Fiedler-blogosphere}
spec_L <- eigen(L)
practical_zero <- 1e-12
lambda  <- min(spec_L$values[spec_L$values>practical_zero])
fiedler <- spec_L$vectors[, which(spec_L$values == lambda)]
```

Then, we plot the values of the Fiedler vector and color point according to the party to check if a part of the underlying structure of the network can indeed be found based on this quantity.

```{r Fiedler-blogosphere-plot-1}
qplot(y = fiedler, colour = party) + 
  viridis::scale_color_viridis(discrete = TRUE)
```

Also, and since the original motivation of the graph-cut is for two-way partionning, we collapse levels from the vector of party into a simplified left/right view (we keep the analysts into a third separated group)

```{r left-right-party}
left_vs_right <- 
  forcats::fct_collapse(party, 
    left = c("green", "left", "far-left", "center-left"),
    right = c("right", "liberal", "center-rigth"),
    analyst = "analyst"
  )
```

```{r Fiedler-blogosphere-plot-2}
qplot(y = fiedler, colour = left_vs_right) + 
  viridis::scale_color_viridis(discrete = TRUE)
```

We can see that there exists an optimum value (or threshold) to separate left from right: if we compute the adjusted Rand index[^4] between a bi-partionning obtained by thresholding the Fidler vector and our reference vector `left_vs_right`, we can see that there exists an optimal threshold maximizing this quantity:

[^4]: a measure of comparison between two classification

```{r ARIs}
thresholds <- seq(-.1, .1, len = 100)
ARIs <- map_dbl(thresholds, ~ARI(left_vs_right, fiedler > .))
qplot(thresholds, ARIs) + geom_vline(xintercept = thresholds[which.max(ARIs)]) + theme_bw()
```

<!-- ```{r Ncut} -->
<!-- Ncut <- map_dbl(thresholds, ~cut(blog, which(fiedler > .), which(fiedler < .))) -->
<!-- qplot(thresholds, Ncut) + geom_vline(xintercept = thresholds[which.min(Ncut)]) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- qplot(ARIs, Ncut) -->
<!-- ``` -->

### Spectral clustering algorithms

Various heuristics generalize the approach defined above to more than 2 groups. They all rely on the spectral property of the Laplacian given above, from which we can derive the following additional properties:

::: {.callout-tip} 
## Spectrum and Graph

  - The multiplicity of the first eigen value ($0$) of $\bL$ determines the number of connected components in the graph.
  - The larger the second non trivial (positive) eigenvalue, the higher the connectivity of $\clG$.
:::

Based on these two properties, spectral clustering algorithms follow the following general principles:

1. Compute spectral decompostion of $\bL$ to perform clustering in the eigen space
2. For a graph with $K$ connected components, the first $K$ eigen-vectors are $\mathbf{1}$ spanning the eigenspace associated with eigenvalue $0$
3. Applying a simple clustering algorithm to the rows of the $K$ first eigenvectors separate the components

$\rightsquigarrow$ The principle generalizes to a fully connected graph (with a single component): spectral clustering tends to separates groups of nodes which are highly connected together

::: {.callout-tip} 
## Variants in the definition of the graph Laplacian

The **normalized** Laplacian matrix $\bL$ (see @ng2002spectral) is defined by 

$$
      \bL_N = \bD^{-1/2}\bL\bD^{-1/2} = \bI - \bD^{-1/2} \bA \bD^{-1/2}.
$$

The **absolute** Laplacian matrix $\bL_{abs}$ (see @rohe2011spectral) is defined by 

$$
  \bL_{abs} = \bD^{-1/2}\bA\bD^{-1/2} = \bI - \bL_N,
$$
with eigenvalues $1-\lambda_n \leq \dots \leq 1-\lambda_2 \leq 1-\lambda_1 = 1$, where $0=\lambda_1\leq \dots \leq \lambda_n$ are the eigenvalues of $\bL_N$.

:::


::: {.callout-note}
## Pseudo code for normalized spectral clustering 

As described in @ng2002spectral

1. Compute the laplacian matrix $\mathbf{L}$\;
2. Compute the $n\times K$ matrix  $\mathbf{U}$ of eigen vectors with the $K$ smallest eigen values
3. Normalize $\mathbf{U}$ row-wise 
4.  Apply k-means to $(\tilde{\mathbf{U}}_i)_{i=1,\dots,n}$
:::


```{r spectral_clustering}
spectral_clustering <- function(graph, nb_cluster, normalized = TRUE) {
  
  ## Compute Laplcian matrix
  L <- laplacian_matrix(graph, normalized = normalized) 
  ## Generates indices of last (smallest) K vectors
  selected <- rev(1:ncol(L))[1:nb_cluster] 
  ## Extract an normalized eigen-vectors
  U <- eigen(L)$vectors[, selected, drop = FALSE]  # spectral decomposition
  U <- sweep(U, 1, sqrt(rowSums(U^2)), '/')    
  ## Perform k-means
  res <- kmeans(U, nb_cluster, nstart = 40)$cl
  
  res
}
```

Let use perform spectral clsutering on the blogosphere for various number of group:

```{r spectral-blog}
nb_cluster <- 1:20
map(nb_cluster, ~spectral_clustering(blog, .)) %>% 
  map_dbl(ARI, party) %>% 
  qplot(nb_cluster, y = .) + geom_line() + theme_bw()
```

Once reorder according to the best clustering (obtained $k=6$) groups, the orginal data matrix looks as follows

```{r spectral-blog-plot}
#| code-fold: true
plotMyMatrix(as_adj(blog, sparse = FALSE),
  clustering = list(row = spectral_clustering(blog, 6)))
```

::: {.callout-warning}
## Some limitations

Hence, as expected, spectral clustering does a great job for recovering community structure in the network. Yet,

- What if other kind of patterns (like star/hub nodes) structure the network
- What if we do not have any clue on the target number of cluster?

A model-based approach, like the one presented below, overcomes these issues

:::


## Model-based clustering for graph data

::: {.callout-note}
## Motivation

We are still looking for an underlying organization in a observed network, yet with model-based approaches, so that statistical inference would be possible.

This session essentially aims to present the **stochastic block model**, a random graph model tailored for clustering vertices. As will be seen, this model is can be interpreted as a special mixture model for graph data. Hence, the relationship between spectral clustering for network and the Stochastic block model is the same as the one between the k-means clustering  and Gaussian mixture models. 
:::

### The ErdÃ¶s-Renyi model

We start by the most simple, yet natural model for random graph, the ErdÃ¶s-RÃ©nyi model.

::: {#def-erdosRenyi}
## ErdÃ¶s-RÃ©nyi model

Let $\clV = {1,\dots,n}$ be a set of fixed vertices. The (simple) ErdÃ¶s-RÃ©nyi model $\mathcal{G}(n,\pi)$ assumes random edges between pairs of nodes with probability $\pi$. In orther word, the (random) adjacency matrix $Y$ is such that

$$
  Y_{ij} \sim \mathcal{B}(\pi)
$$

:::


A direct consequence is that the distribution of the (random) degree $D_i$ of a vertex $i$ follows a binomial distribution, i.e.,

$$D_i \sim b(n -1, \pi).$$


```{r erdos-examples}
G1 <- igraph::sample_gnp(10, 0.1)
G2 <- igraph::sample_gnp(10, 0.9)
G3 <- igraph::sample_gnp(100, .02)
par(mfrow=c(1,3))
plot(G1, vertex.label=NA) ; plot(G2, vertex.label=NA)
plot(G3, vertex.label=NA, layout=layout.circle)
```


Because of its simplicity, a lot of mathematical derivation can be done with this model, yet its utility for adjusting real-world network is very limited since

- the degree distribution is too concentrated, with no high degree nodes,
- All nodes are equivalent, 
- No modularity is observed.

For instance, for the graph `G3` sampled above, the empirical degree distribution and and basic clustering support the over homogeneous structure of the ER model.

```{r, echo = FALSE}
par(mfrow=c(1,2))
hist(degree(G3), col="lightblue"); plot(cluster_fast_greedy(G3), G3)
```


### The Stochastic Block Model (SBM)

The SBM generalizes the ErdÃ¶s-RÃ©nyi model in a mixture framework (see @Nowicki2001, @daudin2008mixture). It provides

- a statistical framework to adjust and interpret the parameters
- a flexible yet simple specification that fits many existing network data

::: {.callout-tip}
## Stochastic Block Model: definition

Let 

- $\{1, \dots, n \}$ be some *fixed* nodes,
- with some **unknown** colors picked up from $\mathcal{C}=\{\color{#fab20a}{\bullet},\color{#0000ff}{\bullet},\color{#008000}{\bullet}\}$

Denote by

- $\alpha_\bullet  =  \mathbb{P}(i  \in  \bullet)$, $\bullet\in\mathcal{C}$ the prior probability of group memberships,
- $\pi_{\color{#fab20a}{\bullet}\color{#0000ff}{\bullet}} = \mathbb{P}(i \leftrightarrow j | i\in\color{#fab20a}{\bullet},j\in\color{#0000ff}{\bullet})$, the probability of connexion between groups.

In the binary Stochastic Block Model, the adjacency matrix $Y_{ij}$ is random, with probability of connexion between a dyad $(i,j)$ being defined conditionnaly on their respective group memberships, described by a vector of random variables $(Z_i)_{i=1,\dots,n}$.

$$
\begin{aligned}
Z_i = \mathbf{1}_{\{i \in \bullet\}}  \ & \sim^{\text{iid}} \mathcal{M}(1,\alpha), \\ 
Y_{ij} \ | \ \{i\in\color{#fab20a}{\bullet},j\in\color{#0000ff}{\bullet}\}
& \sim^{\text{ind}} \mathcal{B}(\pi_{\color{#fab20a}{\bullet}\color{#0000ff}{\bullet}})\\
\end{aligned}
$$
:::

<!-- ```{tikz sbm-model, out.width='70%', fig.cap = "The binary SBM model", fig.ext = 'png', cache=TRUE, echo = FALSE} -->
<!-- \usetikzlibrary{calc,shapes,backgrounds,arrows,automata,shadows,positioning} -->
<!-- \definecolor{myorange}{HTML}{FAB20A} -->
<!-- \definecolor{myblue}{HTML}{0000FF} -->
<!-- \definecolor{mygreen}{HTML}{008000} -->

<!-- \begin{tikzpicture} -->
<!--   %% UN GRAPH -->

<!--   \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw] -->
<!--   \tikzstyle{every state}=[draw=none,text=white,scale=0.65, font=\scriptsize, transform shape] -->
<!--   \tikzstyle{every node}=[fill=myorange] -->
<!--   % premier cluster -->
<!--   \node[state] (A1) at (0,0.5) {1}; -->
<!--   \node[state] (A2) at (1,0.5) {2}; -->
<!--   \node[state] (A3) at (.5,1.5) {3}; -->

<!--   \path (A2) edge [bend left] node[fill=white,below=.1cm] -->
<!--   {$\pi_{\color{myorange}{\bullet}\color{myorange}{\bullet}}$} -->
<!--   (A1) -->
<!--   (A1) edge [bend left] (A3) -->
<!--   (A3) edge [bend left] (A2); -->

<!--   \tikzstyle{every node}=[fill=myblue] -->
<!--   \foreach \angle/\text in {234/6, 162/7, 90/8, 18/9, -54/10} { -->
<!--     \node[fill=blue,state,xshift=5cm,yshift=3.5cm]     (\text)    at -->
<!--     (\angle:1cm) {\text}; -->
<!--   } -->
<!--   \path (7) edge (10) -->
<!--   (6) edge (9); -->
<!--   \foreach \from/\to in {6/7,7/8,9/10,10/6}{ -->
<!--     \path (\from) edge [bend left] (\to); -->
<!--   } -->

<!--   \path    (8)    edge     [bend    left]    node[fill=white] -->
<!--   {$\pi_{\color{myblue}{\bullet}\color{myblue}{\bullet}}$}  (9) ; -->

<!--   \tikzstyle{every node}=[fill=mygreen] -->
<!--   % troisieme cluster -->
<!--   \node[state] (C1) at (3,-.5) {4}; -->
<!--   \node[state] (C2) at (4,0) {5}; -->

<!--   \path (C1) edge [bend right] node[fill=white,below=.25cm] -->
<!--   {$\pi_{\color{mygreen}{\bullet}\color{mygreen}{\bullet}}$} -->
<!--   (C2); -->

<!--   % inter cluster -->
<!--   \path (A3) edge [bend right]  (7) -->
<!--   (A3)    edge    [bend    left]    node[fill=white] -->
<!--   {$\pi_{\color{myorange}{\bullet}\color{myblue}{\bullet}}$} -->
<!--   (8) -->
<!--   (C2) edge [bend right] node[fill=white,right] -->
<!--   {$\pi_{\color{myblue}{\bullet}\color{mygreen}{\bullet}}$} -->
<!--   (9) -->
<!--   (A2) edge [bend right] node[fill=white] -->
<!--   {$\pi_{\color{myorange}{\bullet}\color{mygreen}{\bullet}}$} -->
<!--   (C1); -->
<!-- \end{tikzpicture} -->
<!-- ``` -->


#### A generative model

The SBM does not assume assume any particular a priori structure of the network: because it is a probabilistic, generative model, we can easy simulate SBM-based network data with various topologies Here are a few examples:

- Community network

```{r sample-sbm-community}
pi <- matrix(c(0.3,0.02,0.02,0.02,0.3,0.02,0.02,0.02,0.3),3,3)
communities <- igraph::sample_sbm(100, pi, c(25, 50, 25))
plot(communities, vertex.label=NA, vertex.color = rep(1:3,c(25, 50, 25)))
```

- Star network

```{r sample-sbm-star}
pi <- matrix(c(0.05,0.3,0.3,0),2,2)
star <- igraph::sample_sbm(100, pi, c(4, 96))
plot(star, vertex.label=NA, vertex.color = rep(1:2,c(4,96)))
```

#### Degree distribution

Because it is defined as a simple mixture of ErdÃ¶s-RÃ©nyi, the degree distribution of the binary SBM has a simple close form

::: {#def-degreeSBM}
## SBM: Degree distribution

The conditional degree distribution of a node $i\in q$ is

$$
  D_i | i \in q \sim \mathrm{b}(n-1,\bar\pi) \approx \mathcal{P}(\lambda_q), \qquad \bar\pi_q = \sum_{\ell=1}^Q \alpha_\ell \pi_{q\ell}, \quad \lambda_q = (n-1)\bar\pi_q
$$

From this, we deduce the degree distribution of a node $i$, which can be approximated by a mixture of Poisson distributions:

$$
  \mathbb{P}(D_i = k) = \sum_{q=1}^Q\alpha_q \exp{\set{-\lambda_q}} \ \frac{\lambda_q^k}{k !}
$$

:::


## Estimation: Variational Inference of the binary SBM

### SBM: a latent variable model

Recall tha we have  *fixed* nodes $\{1, \dots, n \}$ with **hidden** colors $\mathcal{C}=\{\color{#fab20a}{\bullet},\color{#0000ff}{\bullet},\color{#008000}{\bullet}\}$. We observe the following

<!-- ```{tikz sbm-inference, out.width='50%', fig.ext = 'png', cache=TRUE, echo = FALSE} -->
<!-- \usetikzlibrary{calc,shapes,backgrounds,arrows,automata,shadows,positioning} -->
<!-- \begin{tikzpicture} -->
<!--   %% UN GRAPH -->

<!--   \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw] -->
<!--   \tikzstyle{every state}=[draw=none,text=white,scale=0.65, font=\scriptsize, transform shape] -->
<!--   \tikzstyle{every node}=[fill=lightgray] -->
<!--   % premier cluster -->
<!--   \node[state] (A1) at (0,0.5) {1}; -->
<!--   \node[state] (A2) at (1,0.5) {2}; -->
<!--   \node[state] (A3) at (.5,1.5) {3}; -->

<!--   \path (A2) edge [bend left] node[fill=white,below=.1cm] -->
<!--   {} -->
<!--   (A1) -->
<!--   (A1) edge [bend left] (A3) -->
<!--   (A3) edge [bend left] (A2); -->

<!--   \tikzstyle{every node}=[fill=blue!80!black] -->
<!--   \foreach \angle/\text in {234/6, 162/7, 90/8, 18/9, -54/10} { -->
<!--     \node[fill=lightgray,state,xshift=5cm,yshift=3.5cm]     (\text)    at -->
<!--     (\angle:1cm) {\text}; -->
<!--   } -->
<!--   \path (7) edge (10) -->
<!--   (6) edge (9); -->
<!--   \foreach \from/\to in {6/7,7/8,9/10,10/6}{ -->
<!--     \path (\from) edge [bend left] (\to); -->
<!--   } -->

<!--   \path    (8)    edge     [bend    left]    node[fill=white] -->
<!--   {}  (9) ; -->

<!--   \tikzstyle{every node}=[fill=lightgray] -->
<!--   % troisieme cluster -->
<!--   \node[state] (C1) at (3,-.5) {4}; -->
<!--   \node[state] (C2) at (4,0) {5}; -->

<!--   \path (C1) edge [bend right] (C2); -->

<!--   % inter cluster -->
<!--   \path (A3) edge [bend right]  (7) -->
<!--   (A3)    edge    [bend    left]    node[fill=white] -->
<!--   {} -->
<!--   (8) -->
<!--   (C2) edge [bend right] node[fill=white,right] -->
<!--   {} -->
<!--   (9) -->
<!--   (A2) edge [bend right] node[fill=white] -->
<!--   {} -->
<!--   (C1); -->
<!-- \end{tikzpicture} -->
<!-- ```  -->

We need to estimate the model parameters and the clustering:

- $\theta = \{\boldsymbol\alpha = (\alpha_\bullet), \boldsymbol\Pi = (\pi_{\color{#fab20a}{\bullet}\color{#0000ff}{\bullet}})\}$
- Colors of $i$, i.e. the $\mathbf{Z}_i$ (the clustering)


Just like with Gaussian mixture models, maximizing the marginal log likelihood is not straightforward


$$\ell_\theta(\mathbf{Y}_i) = \log p_\theta(\mathbf{Y}_i) = \log \int_{\mathcal{Z}} \prod_{(i,j)} p_\theta(Y_{ij} | Z_i, Z_j ) \, p_\theta(\mathbf{Z}) \mathrm{d}\mathbf{Z}$$


Integration over $\mathcal{Z} = \otimes_{k=0,\dots,K}\{1,\dots, C_k\}^{n_k}$ is intractable: we have $\mathrm{card}(C)^n$ terms!


The natural solution is to maximum the likelihood via an EM algorithm, which use the following decomposition of the loglikelihood:

$$\log p_\theta(\mathbf{Y}) = \mathbb{E}_{p_\theta(\mathbf{Z}\,|\,\mathbf{Y})} [\log p_\theta(\mathbf{Y}, \mathbf{Z})] + \mathcal{H}[p_\theta(\mathbf{Z}\,|\,\mathbf{Y})], \quad \text{ with } \mathcal{H}(p) = -\mathbb{E}_p(\log(p))$$ 

::: {.callout-warning}
## Intractable EM

EM requires to evaluate (some moments of)  $p_\theta(\mathbf{Z}\,|\,\mathbf{Y})$, which is not known for the SBM (and was explicit for Gaussian mixture models).

One could use (at least)

- MCMC/Bayesian approaches for evaluating quantities depending on $p_\theta(\mathbf{Z}\,|\,\mathbf{Y})$$
- Variational approaches, which generalize EM by approximating $p_\theta(\mathbf{Z}\,|\,\mathbf{Y})$

:::

### Variational approach: general case

The idea is to find a proxy $q_\psi(\mathbf{Z}) \approx p_\theta(\mathbf{Z} | \mathbf{Y})$ picked in a convenient class of distribution $\mathcal{Q}$

$$q(\mathbf{Z})^\star  \arg\min_{q\in\mathcal{Q}} D\left(q(\mathbf{Z}), p(\mathbf{Z} | \mathbf{Y})\right).$$

KÃ¼llback-Leibler  is a popular choice .small[(error averaged wrt the approximated distribution)]

$$KL\left(q(\mathbf{Z}), p(\mathbf{Z} | \mathbf{Y})\right) = \mathbb{E}_q\left[\log \frac{q(z)}{p(z)}\right] = \int_{\mathcal{Z}} q(z) \log \frac{q(z)}{p(z)} \mathrm{d}z.$$


For mixture model, the natural class of distribution used for approximation is the multinomial

$$\mathcal{Q} = \Big\{q_\psi: \, q_\psi(\mathbf{Z}) = \prod_i q_{\psi_i}(\mathbf{Z}_i), \, q_{\psi_i}(\mathbf{Z}_i) = \mathcal{M}\left(\mathbf{Z}_i; \boldsymbol\tau_i\right), \, \psi_i = \{\boldsymbol{\tau}_i\}, \boldsymbol{\tau}_i \in  \mathbb{R}^{K} \Big\}$$

And we maximize the ELBO (Evidence Lower BOund), a lower bound of the log-likelihood:

$$J(\theta, \psi) = \log p_\theta(\mathbf{Y}) - KL[q_\psi (\mathbf{Z}) ||  p_\theta(\mathbf{Z} | \mathbf{Y})]  = \mathbb{E}_{q} [\log p_\theta(\mathbf{Y}, \mathbf{Z})] + \mathcal{H}[q_\psi(\mathbf{Z})]$$

The variational EM has the following form

  1. Initialization: get $\mathbf{T}^0 = \{\tau_{ik}^0\}$ with Absolute Spectral Clustering

  2. M step: update $\theta^h = \{ \boldsymbol\alpha^h, \boldsymbol\Pi^h\}$

  3. VE step: find the optimal $q_\psi$, by updating $\psi^h= (\psi^h_{i})_i = \mathbf{T}^{h} = \mathbb{E}_{q^{h}} (\mathbf{Z})$:

$$\psi^h = \arg \max J(\theta^h, \psi) = \arg\min_{\psi} KL[q_\psi(\mathbf{Z}) \,||\, p_{\theta^h}(\mathbf{Z}\,|\,\mathbf{Y})]$$

$$\theta^h = \arg\max J(\theta, \psi^h) = \arg\max_{\theta} \mathbb{E}_{q_{\psi^h}} [\log p_{\theta}(\mathbf{Y}, \mathbf{Z})]$$


### Variational EM for SBM: ingredients

We now derive the quantity for the special case of SBM:

#### Variational bound


$$J(\theta, \tau ; \mathbf{Y}) = \sum_{(i,j)} \sum_{(k,\ell)} \tau_{ik} \tau_{j\ell} \log b(Y_{ij},\pi_{k\ell }) + \sum_{i} \sum_{k} \tau _{ik} \log (\alpha_k/\tau_{ik})$$


#### M-step (Analytical)


$$\alpha_k = \frac{1}{n} \sum_{i} \tau_{i k} , \quad  \pi_{k\ell } = \frac{\sum_{(i,j)} \tau_{ik}\tau_{j\ell} Y_{ij}}{\tau_{ik}\tau_{j\ell}} \qquad \left({\boldsymbol\alpha} = \mathbf{1}_n^\top\mathbf{T}, \quad {\boldsymbol\Pi} =  \frac{\mathbf{T}^\top \mathbf{Y} \mathbf{T}}{\mathbf{T}^\top  \mathbf{T}} \right)$$

#### Variational E-step (fixed point)


$$\tau_{ik} \varpropto \alpha_k \prod_{(i,j)} \prod_{\ell} b(Y_{ij} ; \pi_{k\ell})^{\tau_{j\ell}}$$

#### Model Selection

$$\mathrm{vICL}(K) = \mathbb{E}_{q} [\log L(\hat{\theta)}; \mathbf{Y}, \mathbf{Z}] - \frac{1}{2} \left(\frac{K(K+1)}{2} \log \frac{n(n-1)}{2} + (K-1) \log (n) \right)$$

### SBM: the french blogosphere

There exist a variety of packages to fit SBM: we advice here using **sbm** and **misssbm** to stick to the course[^5]

[^5]: I have been involved in their development and they share some classes and have very similar interfaces

```{r fblog-simpleSbm-analysis, cache = TRUE}
blocks <- 1:18
sbm_full <- estimateMissSBM(as_adj(blog), blocks, "node")
```

#### Convergence monitoring (ELBO)

```{r fblog-simpleSbm-analysis-plot1}
plot(sbm_full, "monitoring")
```


#### Model Selection (vICL)

```{r fblog-simpleSbm-analysis-plot2}
plot(sbm_full)
```

#### Parameters

```{r fblog-simpleSbm-analysis-theta}
plot(sbm_full$bestModel, "meso")
```

#### Clustering I

```{r fblog-simpleSbm-analysis-plot3}
plot(sbm_full$bestModel, dimLabels = list(row = "blogs", col = "blogs"))
```

#### Clustering II

```{r fblog-simpleSbm-analysis-plot4}
plot(sbm_full$bestModel, "expected", dimLabels = list(row = "blogs", col = "blogs"))
```

#### Clustering III


```{r fblog-simpleSbm-analysis-ARI}
sp_clustering <- spectral_clustering(blog, sbm_full$bestModel$fittedSBM$nbBlocks)
map(sbm_full$models, "fittedSBM") %>%
  map("memberships") %>% 
  map_dbl(ARI, party) %>% 
  qplot(blocks, y = .)  + theme_bw()
```



## References {.unnumbered}
