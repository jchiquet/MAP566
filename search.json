[
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#introduction",
    "href": "docs/tests/map566-lecture-multiple.html#introduction",
    "title": "Multiple Testing",
    "section": "Introduction",
    "text": "library(tidyverse)\nlibrary(gridExtra)\nlibrary(kableExtra)\ntheme_set(theme_bw())\n\nWhen we perform a large number of statistical tests, some will have p-values less than 0.05 purely by chance, even if all the null hypotheses are really true.\nMore precisely, if we do a large number m of statistical tests, and for m_{0\\cdot} of them the null hypothesis is actually true, we would expect about 5% of the m_{0\\cdot} tests to be significant at the 0.05 level, just due to chance: these significant results are false discoveries (or false positives). On the other hand, if some alternative hypothesis are true, we can miss some of them: these non significant results are false negatives.\n \\begin{array}{c|c|c|c}\n& \\text{Null hypothesis true} & \\text{Alternative hypothesis true}  & \\\\ \\hline \n\\text{Test non significant} & m_{00} & m_{10} & m_{\\cdot 0} \\\\ \\hline\n\\text{Test significant} & m_{01} & m_{11} & m_{\\cdot 1} \\\\ \\hline\n & m_{0 \\cdot} & m_{1 \\cdot} & m\n\\end{array} \nIf important conclusions and decisions are based on these false positives, it is then important to control the family-wise error rate (FWER):\n\nthe family-wise error rate is the probability of making one or more false discoveries, or type I errors when performing multiple hypotheses tests\n\n FWER = \\mathbb{P}(m_{01}\\geq 1) \nWhen true positives are expected, it is possible to miss some of them. We then necessarily need to accept false positives if we want to limit the number of these false negatives. It is important in such situation to control the false discovery rate (FDR)\n\nThe false discovery rate (FDR) is the expected proportion of false discoveries among the discoveries\n\nFDR = \\mathbb{E}\\left(\\frac{m_{01}}{m_{01} + m_{11}}\\right) = \\mathbb{E}\\left(\\frac{m_{01}}{m_{\\cdot1} }\\right) Several procedures exist for controlling either the FWER or the FDR."
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#distribution-of-the-p-values",
    "href": "docs/tests/map566-lecture-multiple.html#distribution-of-the-p-values",
    "title": "Multiple Testing",
    "section": "1 Distribution of the p-values",
    "text": "1.1 Introduction\nThe health effects of a Roundup-tolerant genetically modified maize, cultivated with or without Roundup, and Roundup alone, were studied during a 2 years study in rats.\nFor each sex, one control group had access to plain water and standard diet from the closest isogenic non-transgenic maize control; six groups were fed with 11, 22 and 33% of GM NK603 maize either treated or not with Roundup. The final three groups were fed with the control diet and had access to water supplemented with different concentrations of Roundup.\nA sample of 200 rats including 100 males and 100 females was randomized into 20 groups of 10 rats of the same sex. Within each group, rats received the same diet. For each sex, there are therefore nine experimental groups and one control group.\nThe file ratSurvival.csv reports the lifespan (in days) for each animal. Here, the experiment stopped after 720 days. Then, the reported survival time is 720 for those animals who were still alive at the end of the experiment.\nSee the opinion of the Haut Conseil des Biotechnologies for more information about this study.\nHere is a summary of the data,\n\nsurvival <- read_csv(\"../../data/ratSurvival.csv\") %>% mutate_if(is.character, factor) \nsurvival %>% head() %>% kable() %>% kable_classic()\n\n\n \n  \n    time \n    regimen \n    gender \n  \n \n\n  \n    490 \n    control \n    male \n  \n  \n    575 \n    control \n    male \n  \n  \n    590 \n    control \n    male \n  \n  \n    605 \n    control \n    male \n  \n  \n    610 \n    control \n    male \n  \n  \n    635 \n    control \n    male \n  \n\n\n\n\nlevels(survival$regimen)\n\n [1] \"control\"     \"NK603-11%\"   \"NK603-11%+R\" \"NK603-22%\"   \"NK603-22%+R\"\n [6] \"NK603-33%\"   \"NK603-33%+R\" \"RoundUp A\"   \"RoundUp B\"   \"RoundUp C\"  \n\n\n\n\n1.2 Single comparison between 2 groups\nOne objective of this study is the comparison of the survival between the control group and the experimental groups.\nConsider for instance the control group of females\n\ntime_control <- survival %>% \n  filter(regimen == \"control\", gender == \"female\") %>% pull(time)\n\nOnly 2 rats of this group died before the end of the experiment. On the other hand, 7 females of the group fed with 22% of maize NK693 died during the experiment.\n\ntime_test <- survival %>% \n  filter(regimen == \"NK603-22%\", gender == \"female\") %>% pull(time)\n\nA negative effect of the diet on the survival means that the rats of the experimental group tend to die before those of the control group. Then, we would like to test\n\\begin{aligned}\n& H_0: \\ \"\\text{the NK603 22\\% diet has no effect on the survival of female rats }\"\\\\\n\\text{versus } & H_1: \\ \"\\text{the NK603 22\\% diet leads to decreased survival time for female rats}\"\\\\ \\end{aligned}\nIn terms of survival functions, that means that, under H_1, the probability to be alive at a given time t is lower for a rat of the experimental group than for a rat of the control group. We then would like to test\n\\begin{aligned}\n& H_0: \\ ``\\mathbb{P}(T_{\\rm test}>t) = \\mathbb{P}(T_{\\rm control}>t), \\text{ for any } t>0\" \\\\\n\\text{versus } & H_1: \\ ``\\mathbb{P}(T_{\\rm test}>t) < \\mathbb{P}(T_{\\rm control}>t), \\text{ for any } t>0\" \\end{aligned}\nBecause of the (right) censoring process, we cannot just compare the mean survival times using a t-test. On the other hand, we can use the Wilcoxon-Mann-Whitney test which precisely aims to compare the ranks of the survival times in both groups.\n\nwilcox.test(time_test, time_control, alternative=\"less\")\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  time_test and time_control\nW = 22, p-value = 0.01144\nalternative hypothesis: true location shift is less than 0\n\n\nHere, the p-value should lead us to reject the null hypothesis and conclude that 22% of the GM maize in the diet has a negative effect on the survival."
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#a-single-comparison-among-many-others",
    "href": "docs/tests/map566-lecture-multiple.html#a-single-comparison-among-many-others",
    "title": "Multiple Testing",
    "section": "2 A single comparison… among many others",
    "text": "Should we really accept this conclusion as it stands? No, because we don’t know the whole story… Remember that there are 9 experimental groups for each sex. Then, 18 comparisons with the control groups are performed.\n\ndo_all_comparisons <- function(data) {\n  map(levels(data$gender), \n    function(g) {\n    control <- filter(data, regimen == \"control\" & gender==g) %>% pull(time)\n    regimes <- setdiff(levels(data$regimen), \"control\")\n    map(regimes, function(regime) {\n      test <- filter(data, gender == g & regimen == regime) %>% pull(time)\n      wt  <- wilcox.test(test, control, alternative = \"less\")\n      data.frame(\n        gender    = g,\n        regime    = regime,\n        statistic = unname(wt$statistic),\n        p.value   = wt$p.value\n      )\n    }) %>% bind_rows()\n  }) %>% bind_rows() %>% arrange(p.value)\n}\nall_comparisons <- do_all_comparisons(survival)\nall_comparisons %>% kable() %>% kable_classic()\n\n\n \n  \n    gender \n    regime \n    statistic \n    p.value \n  \n \n\n  \n    female \n    NK603-22% \n    22.0 \n    0.0114378 \n  \n  \n    female \n    NK603-22%+R \n    25.0 \n    0.0213175 \n  \n  \n    female \n    RoundUp C \n    26.5 \n    0.0284545 \n  \n  \n    female \n    NK603-11% \n    33.0 \n    0.0716610 \n  \n  \n    female \n    RoundUp B \n    34.0 \n    0.0845916 \n  \n  \n    female \n    RoundUp A \n    34.5 \n    0.0915661 \n  \n  \n    female \n    NK603-33%+R \n    36.0 \n    0.1155685 \n  \n  \n    female \n    NK603-33% \n    39.0 \n    0.1758393 \n  \n  \n    female \n    NK603-11%+R \n    40.0 \n    0.1879777 \n  \n  \n    male \n    NK603-33% \n    40.5 \n    0.2473324 \n  \n  \n    male \n    NK603-22%+R \n    42.0 \n    0.2849394 \n  \n  \n    male \n    NK603-11%+R \n    50.0 \n    0.5150864 \n  \n  \n    male \n    NK603-33%+R \n    51.0 \n    0.5453261 \n  \n  \n    male \n    RoundUp C \n    54.5 \n    0.6476424 \n  \n  \n    male \n    RoundUp B \n    58.5 \n    0.7528312 \n  \n  \n    male \n    NK603-22% \n    64.0 \n    0.8646590 \n  \n  \n    male \n    NK603-11% \n    74.5 \n    0.9716048 \n  \n  \n    male \n    RoundUp A \n    75.5 \n    0.9768878 \n  \n\n\n\n\n\nLet us plot the ordered p-values:\n\nall_comparisons %>% \n  ggplot() + geom_point(aes(x = 1:18, color = regime, y = p.value, shape = gender), size=4) + \n  scale_y_log10() + xlab(\"regime\") + ylab(\"p-value\") + scale_color_viridis_d() + theme_bw()\n\n\n\n\nIf we then decide to only report the largest observed differences, associated to the smallest p-values, how can we conclude that these differences are statistically significant?\n\n2.1 Permutation test\nA permutation test (also called a randomization test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points. If the labels are exchangeable under the null hypothesis, then the resulting tests yield exact significance levels. Prediction intervals can also be derived.\nIn our example, imagine that the null hypothesis is true. We can then randomly exchange the labels (i.e. the regimen) and perform the 18 comparisons between the experimental groups and the control groups.\n\npermuted_data <- survival %>% \n  split(.$gender) %>% \n  map(mutate, regimen = sample(regimen)) %>% \n  bind_rows()\ndo_all_comparisons(permuted_data) %>% kable() %>% kable_classic()\n\n\n \n  \n    gender \n    regime \n    statistic \n    p.value \n  \n \n\n  \n    male \n    NK603-33% \n    24.5 \n    0.0287739 \n  \n  \n    female \n    NK603-22%+R \n    27.5 \n    0.0443266 \n  \n  \n    male \n    NK603-33%+R \n    30.5 \n    0.0737675 \n  \n  \n    male \n    RoundUp B \n    32.0 \n    0.0897674 \n  \n  \n    male \n    NK603-22% \n    32.5 \n    0.0972588 \n  \n  \n    female \n    NK603-33%+R \n    37.0 \n    0.1609261 \n  \n  \n    male \n    NK603-22%+R \n    38.0 \n    0.1887010 \n  \n  \n    female \n    RoundUp A \n    42.0 \n    0.2761239 \n  \n  \n    female \n    RoundUp B \n    44.5 \n    0.3458949 \n  \n  \n    male \n    NK603-11% \n    46.0 \n    0.3941412 \n  \n  \n    female \n    RoundUp C \n    49.5 \n    0.5000000 \n  \n  \n    female \n    NK603-11%+R \n    52.0 \n    0.5800451 \n  \n  \n    female \n    NK603-22% \n    52.0 \n    0.5800451 \n  \n  \n    male \n    NK603-11%+R \n    53.0 \n    0.6077915 \n  \n  \n    male \n    RoundUp A \n    53.5 \n    0.6244320 \n  \n  \n    female \n    NK603-11% \n    53.5 \n    0.6335553 \n  \n  \n    male \n    RoundUp C \n    54.0 \n    0.6393183 \n  \n  \n    female \n    NK603-33% \n    54.5 \n    0.6605318 \n  \n\n\n\n\n\nThe test statistics and the p-values now really behave how they are supposed to behave under the null hypothesis. If we now repeat the same experiment using many different permutations, we will be able to estimate the m distributions of the m test statistics as well as the m distributions of the m p-values under the null hypothesis.\n\nn_replicates <- 1000\nres <- parallel::mclapply(1:n_replicates, function(i) {\n  \n  permuted_data <- survival %>% \n    split(.$gender) %>% \n    map(mutate, regimen = sample(regimen)) %>% \n    bind_rows()\n\n  do_all_comparisons(permuted_data) %>% \n    select(statistic, p.value) %>% mutate(simu = i)\n}, mc.cores = parallel::detectCores()) %>% bind_rows()\n\nWe can estimate, for instance, prediction intervals of level 90% for the m=18 ordered p-values\n\nquantiles_pvalues <- \n  split(res, res$simu) %>% \n  map(pull, p.value) %>% \n  map(sort) %>% bind_rows() %>% \n  apply(1, quantile, c(0.05, 0.5, 0.95)) %>% t() %>% as_tibble() %>%\n  setNames(c(\"low\",\"median\",\"up\")) %>% mutate(rank = 1:n())\n\nand plot them, with the original p-values\n\nquantiles_pvalues %>% ggplot() +\n  geom_errorbar(aes(x = rank, ymin = low, ymax = up), width=0.2, size=1.5, colour=\"grey50\") +\n  scale_y_log10() + xlab(\"regimen\") + ylab(\"p-value\") + scale_x_continuous(breaks=NULL)  +  \n  geom_point(data = all_comparisons, aes(x = 1:18, color = regime, y = p.value, shape=gender), size=4) \n\n\n\n\nHere, all the p-values, including the smallest ones, belong to the 90% prediction intervals: all the observed p-values behave individually how they are expected to behave under the null hypothesis.\nIn particular, when 18 comparisons are performed, it’s not unlikely under the null hypothesis to obtain a smallest p-value less than or equal to the observed one (0.011).\nThe probability of such event can easily be estimated by Monte Carlo simulation. Let p_{(1),\\ell} be the smallest p-value obtained from the \\ell-th replicate of the Monte Carlo. Then,\n\n\\mathbb{P}\\left(p_{(1)} \\leq p_{(1)}^{\\mathrm{obs}}\\right) \\approx \\frac{1}{L} \\sum_{\\ell=1}^{L} \\mathbf{1}_{\\left\\{p_{(1),\\ell} \\leq p_{(1)}^{\\mathrm{obs}}\\right\\}}\n\n\nstat_rank1 <- split(res, res$simu) %>% map_dbl(function(x) sort(x$statistic)[1])\nmean(stat_rank1 < all_comparisons$statistic[1])\n\n[1] 0.147"
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#controlling-the-family-wise-error-rate",
    "href": "docs/tests/map566-lecture-multiple.html#controlling-the-family-wise-error-rate",
    "title": "Multiple Testing",
    "section": "3 Controlling the Family Wise Error Rate",
    "text": "3.1 The Bonferroni correction\nImagine that we perform m comparisons and that all the m null hypotheses are true, i.e. m=m_{0\\cdot}.. If we use the same significance level \\alpha_m for the m tests, how should we choose \\alpha_m in order to control the family-wise error rate (FWER)?\n \\begin{aligned}\n{\\rm FWER} &= \\mathbb{P}(m_{01}\\geq 1) \\\\\n&= 1 - \\mathbb{P}(m_{01}= 0)  \\\\\n&= 1 - (1-\\alpha_m)^m\n\\end{aligned} \nThen, if we set FWER=\\alpha, the significance level for each individual test should be \\begin{aligned} \n\\alpha_m &=  1 - (1-\\alpha)^{\\frac{1}{m}} \\quad \\quad (\\text{Sidak correction})\\\\\n& \\simeq  \\frac{\\alpha}{m} \\quad \\quad (\\text{Bonferroni correction})\n\\end{aligned}\nLet p_k be the p-value of the k-th test. Using the Bonferroni correction, the k-th test is significant if   p_k \\leq \\alpha_m \\quad  \\Longleftrightarrow \\quad   m \\, p_k \\leq \\alpha  We can then either compare the original p-value p_k to the corrected significance level \\alpha/m, or compare the adjusted p-value p_k^{\\rm (bonferroni)} =\\min(1, m \\, p_k) to the critical value \\alpha.\n\nm <- nrow(all_comparisons)\nall_comparisons$p.value_bonferonni <- pmin(1, all_comparisons$p.value * m)\nall_comparisons %>% kable() %>% kable_classic()\n\n\n \n  \n    gender \n    regime \n    statistic \n    p.value \n    p.value_bonferonni \n  \n \n\n  \n    female \n    NK603-22% \n    22.0 \n    0.0114378 \n    0.2058803 \n  \n  \n    female \n    NK603-22%+R \n    25.0 \n    0.0213175 \n    0.3837141 \n  \n  \n    female \n    RoundUp C \n    26.5 \n    0.0284545 \n    0.5121818 \n  \n  \n    female \n    NK603-11% \n    33.0 \n    0.0716610 \n    1.0000000 \n  \n  \n    female \n    RoundUp B \n    34.0 \n    0.0845916 \n    1.0000000 \n  \n  \n    female \n    RoundUp A \n    34.5 \n    0.0915661 \n    1.0000000 \n  \n  \n    female \n    NK603-33%+R \n    36.0 \n    0.1155685 \n    1.0000000 \n  \n  \n    female \n    NK603-33% \n    39.0 \n    0.1758393 \n    1.0000000 \n  \n  \n    female \n    NK603-11%+R \n    40.0 \n    0.1879777 \n    1.0000000 \n  \n  \n    male \n    NK603-33% \n    40.5 \n    0.2473324 \n    1.0000000 \n  \n  \n    male \n    NK603-22%+R \n    42.0 \n    0.2849394 \n    1.0000000 \n  \n  \n    male \n    NK603-11%+R \n    50.0 \n    0.5150864 \n    1.0000000 \n  \n  \n    male \n    NK603-33%+R \n    51.0 \n    0.5453261 \n    1.0000000 \n  \n  \n    male \n    RoundUp C \n    54.5 \n    0.6476424 \n    1.0000000 \n  \n  \n    male \n    RoundUp B \n    58.5 \n    0.7528312 \n    1.0000000 \n  \n  \n    male \n    NK603-22% \n    64.0 \n    0.8646590 \n    1.0000000 \n  \n  \n    male \n    NK603-11% \n    74.5 \n    0.9716048 \n    1.0000000 \n  \n  \n    male \n    RoundUp A \n    75.5 \n    0.9768878 \n    1.0000000 \n  \n\n\n\n\n\nUsing the Bonferroni correction, none of the 18 comparisons is significant.\nRemark: the function p.adjust proposes several adjustements of the p-values for multiple comparisons, including the Bonferroni adjustment:\n\np.adjust(all_comparisons$p.value, method = \"bonferroni\")\n\n [1] 0.2058803 0.3837141 0.5121818 1.0000000 1.0000000 1.0000000 1.0000000\n [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[15] 1.0000000 1.0000000 1.0000000 1.0000000\n\n\nThe Bonferroni correction is appropriate when a single false positive in a set of tests would be a problem. It is mainly useful when there are a fairly small number of multiple comparisons and very few of them might be significant. The main drawback of the Bonferroni correction is its lack of power: it may lead to a very high rate of false negatives."
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#controlling-the-false-discovery-rate",
    "href": "docs/tests/map566-lecture-multiple.html#controlling-the-false-discovery-rate",
    "title": "Multiple Testing",
    "section": "4 Controlling the False Discovery Rate",
    "text": "4.1 Detecting associations\n(Example from Handbook of Biological Statistics)\nGarcia-Arenzana et al. (2014) tested associations of 25 dietary variables with mammographic density, an important risk factor for breast cancer, in Spanish women. They found the following results\n\ndata <- read_csv(\"../../data/dietary.csv\") %>% mutate_if(is.character, factor) \n\nWe can see that five of the variables show a significant p-value (<0.05). However, because Garc?a-Arenzana et al. (2014) tested 25 dietary variables, we would expect one or two variables to show a significant result purely by chance, even if diet had no real effect on mammographic density.\nApplying the Bonferroni correction, we divide \\alpha=0.05 by the number of tests (m=25) to get the Bonferroni critical value, so a test would have to have p<0.002 to be significant. Under that criterion, only the test for total calories is significant.\nAn alternative approach is to control the false discovery rate, i.e the expected proportion of ``discoveries” (significant results) that are actually false positives. FDR control offers a way to increase power while maintaining some principled bound on error.\nImagine for instance that we compare expression levels for 20,000 genes between liver tumors and normal liver cells. We are going to do additional experiments on any genes that show a significant difference between the normal and tumor cells. Then, because we don’t want to miss genes of interest, we are willing to accept up to 25% of the genes with significant results being false positives. We’ll find out they’re false positives when we do the followup experiments. In this case, we would set the false discovery rate to 25%.\n\n\n4.2 The Benjamini-Hochberg procedure\nThe Benjamini-Hochberg (BH) procedure controls the FDR… and it is simple to use!\nIndeed, for a given \\alpha and a given sequence of ordered p-values P_{(1)}, P_{(2)}, , P_{(m)}, it consists in computing the m adjusted p-values defined as\n P_{(i)}^{\\rm BH} = \\min\\left( P_{(i)}\\frac{m}{i} \\ , \\ P_{(i+1)}^{\\rm BH} \\right)\n\ndata$p.bh <- p.adjust(data$`p-value`, method = \"BH\")\n\nThen, the discoveries, i.e. the significant tests, are those with an ajusted p-value less than \\alpha.\nIt can be shown that this procedure guarantees that for independent tests, and for any alternative hypothesis,\n\\begin{aligned}\n{\\rm FDR} &= \\mathbb{E}{\\frac{m_{01}}{m_{01} + m_{11}}} \\\\\n&\\leq \\frac{m_{0\\cdot}}{m} \\alpha \\\\\n&\\leq \\alpha \n\\end{aligned} where m_{0\\cdot} is the (unknown) total number of true null hypotheses, and where the first inequality is an equality with continuous p-value distributions.\nIn our example, the first five tests would be significant with \\alpha=0.25, which means that we expect no more than 25% of these 5 tests to be false discoveries.\nRemark 1: The BH procedure is equivalent to consider as significant the non adjusted p-values smaller than a threshold P_{\\rm BH} defined as\n P_{\\rm BH} = \\max_i \\left\\{ P_{(i)}: \\ \\ P_{(i)} \\leq \\alpha \\frac{i}{m}  \\right\\}  In other words, the largest p-value that has P_{(i)}<(i/m)\\alpha is significant, and all of the P-values smaller than it are also significant, even the ones that aren’t less than their Benjamini-Hochberg critical value \\alpha \\times i/m\n\nalpha <- 0.25\nm <- nrow(data)\ndata$critical.value <- (1:m)/m*alpha\ndata\n\n# A tibble: 25 × 4\n   dietary           `p-value`  p.bh critical.value\n   <fct>                 <dbl> <dbl>          <dbl>\n 1 Total calories        0.001 0.025           0.01\n 2 Olive oil             0.008 0.1             0.02\n 3 Whole milk            0.039 0.21            0.03\n 4 White meat            0.041 0.21            0.04\n 5 Proteins              0.042 0.21            0.05\n 6 Nuts                  0.061 0.254           0.06\n 7 Cereals and pasta     0.074 0.264           0.07\n 8 White fish            0.205 0.491           0.08\n 9 Butter                0.212 0.491           0.09\n10 Vegetables            0.216 0.491           0.1 \n# … with 15 more rows\n\n\n\npl1 <- ggplot(data) + geom_line(aes(x=1:m,y=`p-value`), colour=\"blue\") +\ngeom_line(aes(x=1:m,y=critical.value), colour=\"red\") +xlab(\"(i)\")\ngrid.arrange(pl1,pl1 + xlim(c(1,8)) + ylim(c(0,0.21)) + geom_vline(xintercept=5.5))\n\n\n\n\nThe largest p-value with P_{(i)}<(i/m)\\alpha is proteins, where the individual p-value (0.042) is less than the (i/m)\\alpha value of 0.050. Thus the first five tests would be significant.\nRemark 2: The FDR is not bounded by \\alpha, but by (m_{0\\cdot}/m) \\alpha. We could increase the global power of the tests and get a FDR equal to the desired level \\alpha, either by defining the critical values as (i/m_{0\\cdot})\\alpha, or by multiplying the adjusted p-values by m_{0\\cdot}/m.\nUnfortunately, m_{0\\cdot} is unknown… but it can be estimated, as the number of non significant tests for instance.\n\nm0.est <- sum(data$p.bh>alpha)\ndata$crit.valc <- round(data$critical.value*m/m0.est,4)\ndata$p.bhc <- round(data$p.bh*m0.est/m,4)\nhead(data,10)\n\n# A tibble: 10 × 6\n   dietary           `p-value`  p.bh critical.value crit.valc p.bhc\n   <fct>                 <dbl> <dbl>          <dbl>     <dbl> <dbl>\n 1 Total calories        0.001 0.025           0.01    0.0125 0.02 \n 2 Olive oil             0.008 0.1             0.02    0.025  0.08 \n 3 Whole milk            0.039 0.21            0.03    0.0375 0.168\n 4 White meat            0.041 0.21            0.04    0.05   0.168\n 5 Proteins              0.042 0.21            0.05    0.0625 0.168\n 6 Nuts                  0.061 0.254           0.06    0.075  0.203\n 7 Cereals and pasta     0.074 0.264           0.07    0.0875 0.211\n 8 White fish            0.205 0.491           0.08    0.1    0.393\n 9 Butter                0.212 0.491           0.09    0.112  0.393\n10 Vegetables            0.216 0.491           0.1     0.125  0.393\n\n\nWe would consider the 7 first p-values as significant using this new correction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenjamini, Y., and Y. Hochberg. 1995. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society B 57: 289-300.\n\n\nGarcia-Arenzana, N., E.M. Navarrete-Munoz, V. Lope, P. Moreo, S. Laso-Pablos, N. Ascunce, F. Casanova-Gomez, C. Sanchez-Contador, C. Santamariña, N. Aragonès, B.P. Gomez, J. Vioque, and M. Pollan. 2014. Calorie intake, olive oil consumption and mammographic density among Spanish women. International Journal of Cancer 134: 1916-1925."
  },
  {
    "objectID": "docs/tests/map566-lecture-single.html#one-sample-t-test",
    "href": "docs/tests/map566-lecture-single.html#one-sample-t-test",
    "title": "Statistical Tests",
    "section": "2.1 One sample t-test",
    "text": "Before considering the problem of comparing two groups, let us start looking at the weight of the male rats only:\n\n\nShow the code\nrat_weight %>% filter(gender == \"Male\") %>% \n  ggplot() + aes(x = regime, y = weight) +\n  geom_violin(aes(fill = regime)) + geom_jitter(alpha = 0.5) +  ylab(\"weight (g)\") +\n  scale_fill_viridis(discrete = TRUE)\n\n\n\n\n\nLet x_1, x_2, x_n the weights of the n male rats. We will assume that the x_i’s are independent and normally distributed with mean \\mu and variance \\sigma^2:\n x_i \\sim^{\\mathrm{iid}} \\mathcal{N}(\\mu \\ , \\ \\sigma^2)\n\n2.1.1 One sided test\nWe want to test\nH_0: \\ ``\\mu \\leq \\mu_0\" \\quad \\text{versus} \\quad H_1: \\ ``\\mu > \\mu_0\" \nFunction t.test can be used for performing this test:\n\nx <- rat_weight %>% filter(gender == \"Male\") %>% pull(\"weight\")\nmu0 <- 500\nt.test(x, alternative=\"greater\", mu=mu0)\n\n\n    One Sample t-test\n\ndata:  x\nt = 1.2708, df = 77, p-value = 0.1038\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 498.0706      Inf\nsample estimates:\nmean of x \n 506.2218 \n\n\nLet us see what these outputs are and how they are computed.\nLet \\bar{x} = n^{-1}\\sum_{i=1}^n x_i be the empirical mean of the data.  \\bar{x} \\sim \\mathcal{N}(\\mu \\ , \\ \\frac{\\sigma^2}{n}) Then,  \\begin{aligned}\n\\frac{\\sqrt{n}(\\bar{x} - \\mu)}{\\sigma} \\ &   \\sim \\ \\mathcal{N}(0 \\ , \\ 1) \\\\ \n\\frac{\\sqrt{n}(\\bar{x} - \\mu)}{s} \\ &   \\sim  \\ t_{n-1} \n\\end{aligned} \nwhere s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2  is the empirical variance of the x_i’s.\nThe statistic used for the test should be a function of the data whose distribution under H_0 is known, and whose expected behavior under H_1 allows one to define a rejection region (or critical region) for the null hypothesis.\nHere, the test statistic is \nT_{\\rm stat} = \\frac{(\\bar{x} - \\mu_0)}{s/\\sqrt{n}}   which follows a t-distribution with n-1 degrees of freedom when \\mu=\\mu_0.\n\\bar{x} is expected to be less than or equal to \\mu_0 under the null hypothesis, and greater than \\mu_0 under the alternative hypothesis, Hence, T_{\\rm stat} is expected to be less than or equal to 0 under H_0 and greater than 0 under H_1. We then reject the null hypothesis H_0 if T_{\\rm stat} is greater than some threshold q.\nSuch decision rule may lead to two kinds of error:\n\nThe type I error is the incorrect rejection of null hypothesis when it is true,\nThe type II error is the failure to reject the null hypothesis when it is false.\n\nThe type I error rate or significance level is therefore the probability of rejecting the null hypothesis given that it is true.\nIn our case, for a given significance level \\alpha, we will reject H_0 if T_{\\rm stat} > qt_{1-\\alpha,n-1}, where qt_{1-\\alpha,n-1} is the quantile of order 1-\\alpha for a t-distribution with n-1 degrees of freedom.\nIndeed, by definition,\n \\begin{aligned}\n\\mathbb{P}(\\text{reject } H_0 \\ | \\ H_0 \\ \\text{true}) &= \\mathbb{P}(T_{\\rm stat} > qt_{1-\\alpha,n-1} \\ | \\ \\mu \\leq \\mu_0) \\\\\n& \\leq \\mathbb{P}(T_{\\rm stat} > qt_{1-\\alpha,n-1} \\ | \\ \\mu = \\mu_0) \\\\\n& \\leq \\mathbb{P}(t_{n-1} > qt_{1-\\alpha,n-1}) \\\\\n& \\leq \\alpha\n\\end{aligned} \n\nalpha <- 0.05\nx_mean <- mean(x)\nx_sd <- sd(x)\nn <- length(x)\ndf <- n - 1\nt.stat <- sqrt(n)*(x_mean-mu0)/x_sd\nc(t.stat,qt(1-alpha, df))\n\n[1] 1.270806 1.664885\n\n\nWe therefore don’t reject H_0 in our example since T_{\\rm stat} < qt_{1-\\alpha,n-1}.\nWe can equivalently compute the significance level for which the test becomes significant. This value is called the p-value: \\begin{aligned}\np_{\\rm value} & = \\max\\mathbb{P}_{H_0}(T_{\\rm stat} > T_{\\rm stat}^{\\rm obs}) \\\\\n& = \\mathbb{P}(T_{\\rm stat} > T_{\\rm stat}^{\\rm obs}  \\ | \\ \\mu=\\mu_0) \\\\\n&= 1 - \\mathbb{P}{t_{n-1} \\leq T_{\\rm stat}^{\\rm obs}}\n\\end{aligned}\nNow, T_{\\rm stat} > qt_{1-\\alpha,n-1} under H_0 if and only if \\mathbb{P}{t_{n-1} \\leq T_{\\rm stat}^{\\rm obs}} \\geq 1-\\alpha. Then, the test is significant at the level \\alpha if and only if p_{\\rm value}\\leq \\alpha.\n\np.value <- 1 - pt(t.stat,df) \nprint(p.value)\n\n[1] 0.1038119\n\n\n\n\n\nHere, we would reject H_0 for any significance level \\alpha \\geq 0.104.\nImportant: The fact that the test is not significant at the level \\alpha does not allow us to conclude that H_0 is true, i.e. that \\mu is less than or equal to 500. We can only say that the data does not allow us to conclude that \\mu>500.\nImagine now that we want to test if \\mu \\geq 515 for instance. The alternative here is H_1: \\ ``\\mu < 515.\n\nmu0 <- 515\nt.test(x, alternative = \"less\", mu = mu0)\n\n\n    One Sample t-test\n\ndata:  x\nt = -1.793, df = 77, p-value = 0.03845\nalternative hypothesis: true mean is less than 515\n95 percent confidence interval:\n    -Inf 514.373\nsample estimates:\nmean of x \n 506.2218 \n\n\nMore generally, we may want to test H_0: \\ ``\\mu \\geq \\mu_0\" \\quad \\text{versus} \\quad H_1: \\ ``\\mu < \\mu_0\"  We still use the statistic T_{\\rm stat} = \\sqrt{n}(\\bar{x}-\\mu_0)/s for this test, but the rejection region is now the area that lies to the left of the critical value qt_{\\alpha,n-1} since\n \\begin{aligned}\n\\mathbb{P}(\\text{reject } H_0 \\ | \\ H_0 \\ \\text{true}) &= \\mathbb{P}(T_{\\rm stat} < qt_{\\alpha,n-1} \\ | \\ \\mu \\geq \\mu_0) \\\\\n& \\leq \\mathbb{P}(T_{\\rm stat} < qt_{\\alpha,n-1} \\ | \\ \\mu = \\mu_0) \\\\\n& \\leq \\alpha\n\\end{aligned} \n\nt.stat <- sqrt(n)*(x_mean-mu0)/x_sd\np.value <- pt(t.stat,df)\nc(t.stat, df, p.value)\n\n[1] -1.79295428 77.00000000  0.03845364\n\n\nHere, the p-value is less than \\alpha=0.05: we then reject the null hypothesis at the 5\\% level and conclude that \\mu < 515.\n\n\n2.1.2 Two sided test\nA two sided test (or two tailed test) can be used to test if \\mu=500 for instance\n\nmu0 = 500\nt.test(x, alternative = \"two.sided\", mu  =mu0)\n\n\n    One Sample t-test\n\ndata:  x\nt = 1.2708, df = 77, p-value = 0.2076\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 496.4727 515.9709\nsample estimates:\nmean of x \n 506.2218 \n\n\nMore generally, we can test H_0: \\ ``\\mu = \\mu_0\" \\quad \\text{versus} \\quad H_1: \\ ``\\mu \\neq \\mu_0\"  The test also uses the statistic T_{\\rm stat} = \\sqrt{n}(\\bar{x}-\\mu_0)/s, but the rejection region has now two parts: we reject H_0 if |T_{\\rm stat}| > qt_{1-\\alpha/2}. Indeed,\n \\begin{aligned}\n\\mathbb{P}(\\text{reject } H_0 \\ | \\ H_0 \\ \\text{true}) &= \\mathbb{P}(|T_{\\rm stat}| > qt_{1 -\\frac{\\alpha}{2},n-1} \\ | \\ \\mu = \\mu_0) \\\\\n& = \\mathbb{P}(T_{\\rm stat} < qt_{\\frac{\\alpha}{2},n-1} \\ | \\ \\mu = \\mu_0) +\n \\mathbb{P}(T_{\\rm stat} > qt_{1-\\frac{\\alpha}{2},n-1} \\ | \\ \\mu = \\mu_0)\\\\\n&= \\mathbb{P}{t_{n-1} \\leq qt_{\\frac{\\alpha}{2},n-1}} +  \\mathbb{P}{t_{n-1} \\geq qt_{1-\\frac{\\alpha}{2},n-1}} \\\\\n &= \\frac{\\alpha}{2} + \\frac{\\alpha}{2} \\\\\n& = \\alpha\n\\end{aligned} \nThe p-value of the test is now \\begin{aligned}\np_{\\rm value} & = \\mathbb{P}_{H_0}(|T_{\\rm stat}| > |T_{\\rm stat}^{\\rm obs}|) \\\\\n& = \\mathbb{P}_{H_0}(T_{\\rm stat} < -|T_{\\rm stat}^{\\rm obs}|)  + \\mathbb{P}_{H_0}(T_{\\rm stat} > |T_{\\rm stat}^{\\rm obs}|)\\\\\n&= \\mathbb{P}{t_{n-1} \\leq -|T_{\\rm stat}^{\\rm obs}|} +  \\mathbb{P}{t_{n-1} \\geq |T_{\\rm stat}^{\\rm obs}|} \\\\\n&= 2 \\,\\mathbb{P}{t_{n-1} \\leq -|T_{\\rm stat}^{\\rm obs}|}\n\\end{aligned}\n\nt.stat <- sqrt(n)*(x_mean-mu0)/x_sd\np.value <- 2*pt(-abs(t.stat),df) \nc(t.stat, df, p.value)\n\n[1]  1.2708058 77.0000000  0.2076238\n\n\n\n\n\nHere, p_{\\rm value}= 0.208. Then, for any significance level less than 0.208, we cannot reject the hypothesis that \\mu = 500.\n\n\n2.1.3 Confidence interval for the mean\nWe have just seen that the data doesn’t allow us to reject the hypothesis that \\mu = 500. But we would come to the same conclusion with other values of \\mu_0. In particular, we will never reject the hypothesis that \\mu = \\bar{x}:\n\nt.test(x, mu=x_mean, conf.level = 1 - alpha)$p.value\n\n[1] 1\n\n\nFor a given significance level (\\alpha = 0.05 for instance), we will not reject the null hypothesis for values of \\mu_0 close enough to \\bar{x}.\n\npv.510 <- t.test(x, mu = 510, conf.level = 1-alpha)$p.value\npv.497 <- t.test(x, mu = 497, conf.level = 1-alpha)$p.value\nc(pv.510, pv.497)\n\n[1] 0.44265350 0.06340045\n\n\nOn the other hand, we will reject H_0 for values of \\mu_0 far enough from \\bar{x}:\n\npv.520 <- t.test(x, mu = 520, conf.level = 1-alpha)$p.value\npv.490 <- t.test(x, mu = 490, conf.level = 1-alpha)$p.value\nc(pv.520, pv.490)\n\n[1] 0.006204188 0.001406681\n\n\nThere exist two values of \\mu_0 for which the decision is borderline\n\npv1 <- t.test(x, mu = 496.47, conf.level = 1-alpha)$p.value\npv2 <- t.test(x, mu = 515.97, conf.level = 1-alpha)$p.value\nc(pv1,pv2)\n\n[1] 0.04993761 0.05001986\n\n\nIn fact, for a given \\alpha, these two values \\mu_{\\alpha,{\\rm lower}} and \\mu_{\\alpha,{\\rm upper}} define a confidence interval for \\mu: We are ``confident’’ at the level 1-\\alpha that any value between \\mu_{\\alpha,{\\rm lower}} and \\mu_{\\alpha,{\\rm upper}} is a possible value for \\mu.\n\n\nShow the code\nmu     <- seq(490, 520, by = 0.25)\nt_stat <- (x_mean - mu) / x_sd * sqrt(n)\npval   <- pmin(pt(-t_stat, df) + (1 - pt(t_stat, df)),\n               pt(t_stat, df) + (1 - pt(-t_stat, df)))\nCI     <- x_mean + x_sd/sqrt(n) * qt(c(alpha/2, 1-alpha/2), df)\ndata.frame(mu = mu, p.value = pval) %>% \n  ggplot() + geom_line(aes(x = mu, y = p.value)) + \n  geom_vline(xintercept = x_mean, colour=\"red\", linetype=2)+\n  geom_hline(yintercept = alpha, colour=\"green\", linetype=2)+\n  geom_vline(xintercept = CI, colour=\"red\") +\n  scale_x_continuous(breaks = round(c(490,500,510,520,CI,x_mean),2)) \n\n\n\n\n\nBy construction,\n\\begin{aligned}\n1-\\alpha &=  \\mathbb{P}\\left(qt_{\\frac{\\alpha}{2},n-1} < \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} < qt_{1-\\frac{\\alpha}{2},n-1} \\right) \\\\\n&= \\mathbb{P}\\left(\\bar{x} +\\frac{s}{\\sqrt{n}}qt_{\\frac{\\alpha}{2},n-1} < \\mu < \\bar{x} +\\frac{s}{\\sqrt{n}}qt_{1-\\frac{\\alpha}{2},n-1} \\right) \n\\end{aligned}\nThe confidence interval of level 1-\\alpha for \\mu is therefore the interval {\\rm CI}_{1-\\alpha} = \\left[\\bar{x} +\\frac{s}{\\sqrt{n}}qt_{\\frac{\\alpha}{2},n-1} \\ \\ , \\ \\ \n\\bar{x} +\\frac{s}{\\sqrt{n}}qt_{1-\\frac{\\alpha}{2},n-1}\\right] \n\n(CI <- x_mean + x_sd / sqrt(n) * qt(c(alpha/2, 1-alpha/2), df))\n\n[1] 496.4727 515.9709\n\n\nRemark 1: The fact that \\mathbb{P}( \\mu \\in {\\rm CI}_{1-\\alpha}) = 1- \\alpha does not mean that \\mu is a random variable! It is the bounds of the confidence interval that are random because they are function of the data.\nA confidence interval of level 1-\\alpha should be interpreted like this: imagine that we repeat the same experiment many times, with the same experimental conditions, and that we build a confidence interval for \\mu for each of these replicate. Then, the true mean \\mu will lie in the confidence interval (1-\\alpha)100\\% of the times.\nLet us check this property with a Monte Carlo simulation.\n\nn_replicate <- 1e5\nn <- 100; mu <- 500; sd <- 40\nR <- replicate(n_replicate, {\n  x  <- rnorm(n, mu, sd)\n  ci <- mean(x) + sd(x)/sqrt(n)*qt(c(alpha/2, 1-alpha/2), n-1)\n  (mu > ci[1] & mu < ci[2])\n})\nmean(R)\n\n[1] 0.94951\n\n\nRemark 2:\nThe decision rule to reject or not the null hypothesis can be derived from the confidence interval. Indeed, the confidence interval plays the role of an acceptance region: we reject H_0 if \\mu_0 does not belong to {\\rm CI}_{1-\\alpha}.\nIn the case of a one sided test, the output of t.test called confidence interval is indeed an acceptance region for \\mu, but not a ``confidence interval’’ (we cannot seriouly consider that \\mu can take any value above 500 for instance )\n\nrbind(\nc(x_mean + x_sd/sqrt(n)*qt(alpha,df) , Inf),\nc(-Inf, x_mean + x_sd/sqrt(n)*qt(1-alpha,df)))\n\n         [,1]     [,2]\n[1,] 499.0229      Inf\n[2,]     -Inf 513.4207"
  },
  {
    "objectID": "docs/tests/map566-lecture-single.html#two-samples-t-test",
    "href": "docs/tests/map566-lecture-single.html#two-samples-t-test",
    "title": "Statistical Tests",
    "section": "2.2 Two samples t-test",
    "text": "2.2.1 What should we test?\nLet us now compare the weights of the male and female rats. ::: {.cell hash=“map566-lecture-single_cache/html/unnamed-chunk-22_05196d190340aa6cfce31309eb9c05cc”}\nrat_weight %>% \n  ggplot() + aes(x = gender, y = weight) +\n  geom_violin(aes(fill = gender)) + geom_jitter(alpha = 0.5) +  ylab(\"weight (g)\") +\n  scale_fill_viridis(discrete = TRUE)\n\n\n\n:::\nLooking at the data is more than enough for concluding that the mean weight of the males is (much) larger than the mean weight of the females Computing a p-value here is of little interest \n\nrat_weight %>% group_by(gender) %>% summarize(average_weight = mean(weight)) %>% \n  kable() %>% kable_classic()\n\n\n \n  \n    gender \n    average_weight \n  \n \n\n  \n    Female \n    282.8025 \n  \n  \n    Male \n    506.2218 \n  \n\n\n\n\nx <- rat_weight %>% filter(gender == \"Male\") %>% pull(\"weight\")\ny <- rat_weight %>% filter(gender == \"Female\") %>% pull(\"weight\")\nt.test(x, y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 40.35, df = 117.08, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 212.4535 234.3851\nsample estimates:\nmean of x mean of y \n 506.2218  282.8025 \n\n\nLet us see now what happens if we compare the control and GMO groups for the male rats. ::: {.cell hash=“map566-lecture-single_cache/html/unnamed-chunk-24_77eae71bc18f9797e3e6fe48143013de”}\nrat_weight %>% filter(gender == \"Male\") %>% \n  ggplot() + aes(x = regime, y = weight) +\n  geom_boxplot(aes(fill = regime)) + geom_jitter(alpha = 0.5) +  ylab(\"weight (g)\") +\n  scale_fill_viridis(discrete = TRUE)\n\n\n\nx <- rat_weight %>% filter(gender == \"Male\" & regime == \"Control\") %>% pull(\"weight\")\ny <- rat_weight %>% filter(gender == \"Male\" & regime == \"GMO\") %>% pull(\"weight\")\n:::\nWe observe a difference between the two empirical means (the mean weight after 14 weeks is greater in the control group), but we cannot say how significant this difference is by simply looking at the data. Performing a statistical test is now necessary.\nLet x_{1}, x_{2}, \\ldots, x_{n_x} be the weights of the n_x male rats of the control group and y_{1}, y_{2}, \\ldots, y_{n_x} the weights of the n_y male rats of the GMO group. We will assume normal distributions for both (x_{i}) and (y_{i}):\n x_{i} \\sim^{\\mathrm{iid}} \\mathcal{N}(\\mu_x \\ , \\ \\sigma^2_x) \\quad ; \\quad y_{i} \\sim^{\\mathrm{iid}} \\mathcal{N}(\\mu_y \\ , \\ \\sigma^2_y)\nWe want to test\nH_0: \\ ``\\mu_x = \\mu_y\" \\quad \\text{versus} \\quad H_1: \\ ``\\mu_x \\neq \\mu_y\" \n\n\n2.2.2 Assuming equal variances\nWe can use the function t.test assuming first equal variances (\\sigma^2_x=\\sigma_y^2) ::: {.cell hash=“map566-lecture-single_cache/html/unnamed-chunk-25_83a8b1c0b913df4a6c53d0ccd4916587”}\nalpha <- 0.05\nt.test(x, y, conf.level = 1-alpha, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  x and y\nt = 1.5426, df = 76, p-value = 0.1271\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.358031 34.301621\nsample estimates:\nmean of x mean of y \n 513.7077  498.7359 \n\n:::\nThe test statistic is T_{\\rm stat} = \\frac{\\bar{x} - \\bar{y}}{s_p \\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}}   where s_p^2 is the pooled variance:\ns_p^2 = \\frac{1}{n_x+n_y-2} \\left(\\sum_{i=1}^{n_x} (x_{i}-\\bar{x})^2 + \\sum_{i=1}^{n_y} (y_{i}-\\bar{y})^2 \\right)  \nUnder the null hypothesis, T_{\\rm stat} follows a t-distribution with n_x+n_y-2 degree of freedom. The p-value is therefore\n\\begin{aligned}\np_{\\rm value} & = \\mathbb{P}_{H_0}(|T_{\\rm stat}| > |T_{\\rm stat}^{\\rm obs}|) \\\\\n&= \\mathbb{P}{t_{n_x+n_y-2} \\leq -T_{\\rm stat}^{\\rm obs}} + 1 - \\mathbb{P}{t_{n_x+n_y-2} \\leq T_{\\rm stat}^{\\rm obs}}\n\\end{aligned}\n\nnx <- length(x)\nny <- length(y)\nx_mean <- mean(x)\ny_mean <- mean(y)\nx_sc <- sum((x-x_mean)^2)\ny_sc <- sum((y-y_mean)^2)\nxy_sd <- sqrt((x_sc+y_sc)/(nx+ny-2))\nt.stat <- (x_mean-y_mean)/xy_sd/sqrt(1/nx+1/ny)\ndf <- nx + ny -2\np.value <- pt(-t.stat, df) + (1- pt(t.stat, df))\nc(t.stat, df, p.value)\n\n[1]  1.5426375 76.0000000  0.1270726\n\n\nThe confidence interval for the mean difference \\mu_x-\\mu_y is computed as {\\rm CI}_{1-\\alpha} = [\\bar{x} - \\bar{y} +s_p \\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}qt_{\\frac{\\alpha}{2},n_x+n_y-2} \\ \\ , \\ \\ \n\\bar{x} - \\bar{y} +s_p \\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}qt_{1-\\frac{\\alpha}{2},n_x+n_y-2} ] \n\nx_mean - y_mean  + xy_sd*sqrt(1/nx+1/ny)*qt(c(alpha/2,1-alpha/2), df)\n\n[1] -4.358031 34.301621\n\n\n\n\n2.2.3 Assuming different variances\nAssuming equal variances for the two groups may be disputable.\n\nrat_weight %>% filter(gender == \"Male\") %>% \n  group_by(regime) %>% \n  summarise(mean = mean(weight), sd = sd(weight))\n\n# A tibble: 2 × 3\n  regime   mean    sd\n  <chr>   <dbl> <dbl>\n1 Control  514.  43.2\n2 GMO      499.  42.5\n\n\nWe can then use the t.test function with different variances (which is the default)\n\nt.test(x, y, conf.level = 1-alpha)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 1.5426, df = 75.976, p-value = 0.1271\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.358129 34.301719\nsample estimates:\nmean of x mean of y \n 513.7077  498.7359 \n\n\n\n\n\nThe Welch (or Satterthwaite) approximation to the degrees of freedom is used instead of n_x+n_y-2= 76:\n\\mathrm{df}_W = \\frac{(c_x + c_y)^2}{{c_x^2}/{(n_x-1)} + {c_y^2}/{(n_y-1)}} where c_x = \\sum (x_{i}-\\bar{x})^2/(n_x(n_x-1)) and c_y = \\sum (y_{i}-\\bar{y})^2/(n_y(n_y-1)).\nFurthermore, unlike in Student’s t-test with equal variances, the denominator is not based on a pooled variance estimate:\nT_{\\rm stat} = \\frac{\\bar{x} - \\bar{y}}{ \\sqrt{{s_x^2}/{n_x}+{s_y^2}/{n_y}}}   where s_x^2 and s_y^2 are the empirical variances of (x_i) and (y_i):  s_x^2 = \\frac{1}{n_x-1}\\sum_{i=1}^{n_x} (x_{i}-\\bar{x})^2  \\quad ; \\quad\ns_y^2 = \\frac{1}{n_y-1}\\sum_{i=1}^{n_y} (y_{i}-\\bar{y})^2\n\nsbar.xy <- sqrt(var(x)/nx+var(y)/ny)\nt.stat <- (x_mean-y_mean)/sbar.xy\ncx <- x_sc/(nx-1)/nx\ncy <- y_sc/(ny-1)/ny\ndfw <- (cx + cy)^2 / (cx^2/(nx-1) + cy^2/(ny-1))\np.value <- pt(-t.stat,dfw) + (1- pt(t.stat,dfw))\nc(t.stat, dfw, p.value)\n\n[1]  1.5426375 75.9760868  0.1270739\n\n\nThe confidence interval for \\mu_x-\\mu_y is now computed as {\\rm CI}_{1-\\alpha} = [\\bar{x} - \\bar{y} +\\sqrt{\\frac{s_x^2}{n_x}+\\frac{s_y^2}{n_y}} \\ qt_{\\frac{\\alpha}{2},\\mathrm{df}_W} \\ \\ , \\ \\ \n\\bar{x} - \\bar{y} +\\sqrt{\\frac{s_x^2}{n_x}+\\frac{s_y^2}{n_y}} \\ qt_{1-\\frac{\\alpha}{2},\\mathrm{df}_W} ] \n\nx_mean-y_mean  + sbar.xy*qt(c(alpha/2,1-alpha/2),dfw)\n\n[1] -4.358129 34.301719"
  },
  {
    "objectID": "docs/tests/map566-lecture-single.html#power-of-a-t-test",
    "href": "docs/tests/map566-lecture-single.html#power-of-a-t-test",
    "title": "Statistical Tests",
    "section": "4.1 Power of a t-test",
    "text": "Until now, we have demonstrated that the experimental data does not highlight any significant difference in weight between the control group and the GMO group.\nOf course, that does not mean that there is no difference between the two groups. Indeed, absence of evidence is not evidence of absence. In fact, no experimental study would be able to demonstrate the absence of effect of the diet on the weight.\nNow, the appropriate question is rather to evaluate what the experimental study can detect. If feeding a population of rats with GMOs has a signicant biological effect on the weight, can we ensure with a reasonable level of confidence that our statistical test will reject the null hypothesis and conclude that there is indeed a difference in weight between the two groups?\nA power analysis allows us to determine the sample size required to detect an effect of a given size with a given degree of confidence. Conversely, it allows us to determine the probability of detecting an effect of a given size with a given level of confidence, under sample size constraints.\nFor a given \\delta \\in {\\mathbb R}, let \\beta(\\delta) be the type II error rate, i.e. the probability to fail rejecting H_0 when \\mu_x-\\mu_y = \\delta, with \\delta\\neq 0.\nThe power of the test is the probability to reject the null hypothesis when it is false. It is also a function of \\delta =\\mu_x-\\mu_y defined as\n \\begin{aligned}\n\\eta(\\delta) &= 1 - \\beta(\\delta) \\\\\n&= \\mathbb{P}{\\text{reject } H_0 \\ | \\ \\mu_x-\\mu_y=\\delta } \n\\end{aligned} \nRemember that, for a two sided test, we reject the null hypothesis when |T_{\\rm stat}| > qt_{1-\\alpha/2, \\mathrm{df}}, where \\mathrm{df} is the appropriate degree of freedom.\nOn the other hand, {(\\bar{x} - \\bar{y} - \\delta)}/{s_{xy}}, where s_{xy} = \\sqrt{{s_x^2}/{n_x}+{s_y^2}/{n_y}}, follows a t-distribution with \\mathrm{df} degrees of freedom. Thus,\n \\begin{aligned}\n\\eta(\\delta) &= 1 - \\mathbb{P}(qt_{\\frac{\\alpha}{2},\\mathrm{df}} < T_{\\rm stat} < qt_{1 -\\frac{\\alpha}{2},\\mathrm{df}} \\ | \\ \\mu_x-\\mu_y=\\delta) \\\\\n& = 1- \\mathbb{P}(qt_{\\frac{\\alpha}{2},\\mathrm{df}} < \\frac{\\bar{x} - \\bar{y}}{s_{xy}} < qt_{1-\\frac{\\alpha}{2},\\mathrm{df}} \\ | \\ \\mu_x-\\mu_y=\\delta) \\\\\n&= 1- \\mathbb{P}(qt_{\\frac{\\alpha}{2},\\mathrm{df}} - \\frac{\\delta}{s_{xy}} < \\frac{\\bar{x} - \\bar{y} - \\delta}{s_{xy}} < qt_{1-\\frac{\\alpha}{2},\\mathrm{df}} - \\frac{\\delta}{s_{xy}} \\ | \\ \\mu_x-\\mu_y=\\delta) \\\\\n&= 1 - Ft_{\\mathrm{df}}(qt_{1-\\frac{\\alpha}{2},\\mathrm{df}} - \\frac{\\delta}{s_{xy}}) + Ft_{\\mathrm{df}}(qt_{\\frac{\\alpha}{2},\\mathrm{df}} - \\frac{\\delta}{s_{xy}})\n\\end{aligned} \nAs an example, let us compute the probability to detect a difference in weight of 10g with two groups of 80 rats each and assuming that the standard deviation is 30g in each group.\n\nalpha=0.05\nnx_new <- ny_new <- 80\ndelta_mu <- 10\nx_sd <- 30\ndf <- nx_new + ny_new-2\ndt <- delta_mu/x_sd/sqrt(1/nx_new+1/ny_new)\n1-pt(qt(1-alpha/2,df)-dt,df) + pt(qt(alpha/2,df)-dt,df) \n\n[1] 0.5528906\n\n\nThe function pwr.t.test allows to compute this power:\n\nlibrary(pwr)\npwr.t.test(n = nx_new, d = delta_mu/x_sd, type = \"two.sample\",\n           alternative=\"two.sided\", sig.level = alpha)\n\n\n     Two-sample t test power calculation \n\n              n = 80\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.5538758\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nLet us perform a Monte Carlo simulation, to check this result and better understand what it means. Imagine that the ``true’’ difference in weight is \\delta=10g. Then, if could repeat the same experiment a (very) large number of times, we would reject the null hypothesis in 55\\% of cases.\n\nn_replicate <- 1e5\nmu_x <- 500\nmu_y <- mu_x + delta_mu\nR <- replicate(n_replicate, {\n  x_new <- rnorm(nx_new, mu_x, x_sd)\n  y_new <- rnorm(ny_new, mu_y, x_sd)\n  t.test(x_new, y_new, alternative=\"two.sided\")$p.value < alpha\n})\nmean(R)\n\n[1] 0.55347\n\n\nWe may consider this probability as too small. If our objective is a power of 80% at least, with the same significance level, we need to increase the sample size.\n\npwr.t.test(power = 0.8, d = delta_mu/x_sd, sig.level=alpha) \n\n\n     Two-sample t test power calculation \n\n              n = 142.2462\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\nIndeed, we see that n\\geq 143 animals per group are required in order to reach a power of 80%.\n\nnx_new <- ny_new <- ceiling(pwr.t.test(power=0.8, d = delta_mu/x_sd, sig.level=alpha)$n)\ndf <- nx_new + ny_new - 2\ndt <- delta_mu/x_sd/sqrt(1/nx_new + 1/ny_new)\n1-pt(qt(1-alpha/2,df)-dt,df) + pt(qt(alpha/2,df)-dt,df)\n\n[1] 0.8020466\n\n\nAn alternative for increasing the power consists in increasing the type I error rate\n\npwr.t.test(power=0.8, d=delta_mu/x_sd, n = 80, sig.level = NULL) \n\n\n     Two-sample t test power calculation \n\n              n = 80\n              d = 0.3333333\n      sig.level = 0.2067337\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIf we accept a significance level of about 20%, then we will be less demanding for rejecting H_0: we will reject the null hypothesis when |T_{\\rm stat}|>qt_{0.9,158}= 1.29, instead of |T_{\\rm stat}|>qt_{0.975,158}= 1.98. This strategy will therefore increase the power, but also the type I error rate."
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#preliminary",
    "href": "docs/tests/map566-lab-tests.html#preliminary",
    "title": "Statistical tests: exercices",
    "section": "Preliminary",
    "text": "Only functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation:\n\nlibrary(tidyverse)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#gene-expression",
    "href": "docs/tests/map566-lab-tests.html#gene-expression",
    "title": "Statistical tests: exercices",
    "section": "1 Gene expression",
    "text": "The dataset geHT.csv consists of gene expression measurements for ten genes under control and treatment conditions, with four replicates each.\n\ngene_data  <- readr::read_csv(\"../../data/geHT.csv\")\ncontrols   <- gene_data %>% dplyr::select(starts_with(\"c\")) %>% unlist()\ntreatments <- gene_data %>% dplyr::select(starts_with(\"t\")) %>% unlist()\n\n\nPlot the data using boxplots to compare control versus treatment\nTest the hypothesis that the mean of the control expression values is 2000.\n\nHint: Use the t.test function to perform the test (> ?t.test for more information about this function)\n\nTest that there is no difference overall between the treatments and controls for any of the genes (test that the whole experiment didn’t work or there are no differentially expressed genes)\n\nHint: Compare the results ignoring/knowing that the data are paired data.\n\nTest if the variances for the gene expression are the same under treatment or control conditions\n\nHint: Perform a F test to compare the two variances using the var.test function (> ?var.test for more information about this function)"
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#smoking-no-smoking",
    "href": "docs/tests/map566-lab-tests.html#smoking-no-smoking",
    "title": "Statistical tests: exercices",
    "section": "2 Smoking, no smoking",
    "text": "There are 88 smokers among a group of 300 people of a same population. Test that the proportion of smokers in this population is less than or equal to 0.25, greater than or equal to 0.25, equal to 0.25. Show that we can use an exact test, or a test relying on an approximation.\n\nHint: What is the probability distribution of the number X_1 of smokers in this group of n_1=300 people ? Look at the binom.test and prop.test functions.\n\nThere are 90 smokers in another group of 400 people, coming from another population. Can we conclude that the proportion of smokers are different in these two populations?\n\nHint: look at the fisher.test and prop.test functions.\n\n## help in building thecontingency table\nsmokers     <- c(88, 90)\nnon.smokers <- c(300-88, 400-90)\nsmoke <- matrix(c(smokers,non.smokers), nrow = 2)\nsmoke\n\n     [,1] [,2]\n[1,]   88  212\n[2,]   90  310"
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#efficacy-of-the-bnt162b2-mrna-covid-19-vaccine",
    "href": "docs/tests/map566-lab-tests.html#efficacy-of-the-bnt162b2-mrna-covid-19-vaccine",
    "title": "Statistical tests: exercices",
    "section": "3 Efficacy of the BNT162b2 mRNA Covid-19 Vaccine",
    "text": "In an ongoing multinational, placebo-controlled, observer-blinded, pivotal efficacy trial, a total of 43,548 participants underwent randomization, of whom 43,448 received injections: 21,720 with BNT162b2 and 21,728 with placebo. There were 8 cases of Covid-19 with onset at least 7 days after the second dose among participants assigned to receive BNT162b2 and 162 cases among those assigned to placebo.\nBased on this result, Pfizer concludes that BNT162b2 was 95% effective in preventing Covid-19.\n\nHow was this value obtained?\n\nHint: The idea is to compare the observed number of BNT162b2 participants who were really infected with the expected number of BNT162b2 participants who would have been infected if the vaccine was not more effective than the placebo.\nLet p_0 and p_1 be the probabilities to be infected in the placebo and BNT162b2 groups. How can p_0 and p_1 be estimated? How can the expected number of BNT162b2 participants be estimated under the null hypothesis (p_0=p_1)?\n\nDerive a 95% confidence interval for the vaccine efficacy VE using a normal approximation for \\log(\\hat{p}_1/\\hat{p}_0) where \\hat{p}_0 and \\hat{p}_1 are the empirical proportions of cases of Covid-19 in the two groups,\n\nHint: What are the asymptotic distributions of \\hat{p}_0 and \\hat{p}_1? Using the delta-method, compute the asymptotic distribution for \\log(\\hat{p}_0) and \\log(\\hat{p}_1) and \\log(\\hat{p}_1/\\hat{p}_0). Derive a confidence interval for \\log(p_1/p0) and then for VE.\n\nDerive a 95% confidence interval for the vaccine efficacy approximating a confidence interval for {p}_1/{p}_0 by a prediction interval for \\hat{p}_1/\\hat{p}_0 and using a Monte-Carlo simulation for estimating this prediction interval.\n\nHint:\n\nK <- 1000000\nx0 <- rbinom(K, n0, hat.p0)\nx1 <- rbinom(K, n1, hat.p1)\n## just complete to estimate the target quantities"
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#identification-of-genes",
    "href": "docs/tests/map566-lab-tests.html#identification-of-genes",
    "title": "Statistical tests: exercices",
    "section": "4 Identification of genes",
    "text": "Breast cancer is the most common malignant disease in Western women. In these patients, it is not the primary tumour, but its metastases at distant sites that are the main cause of death.\nPrognostic markers are needed to identify patients who are at the highest risk for developing metastases, which might enable oncologists to begin tailoring treatment strategies to individual patients. Gene-expression signatures of primary breast tumours might be one way to identify the patients who are most likely to develop metastatic cancer.\nThe datafile geneMFS.csv contains the expression level of 11 genes and the metastasis-free survival (the period until metastasis is detected) for 527 patients.\n\ncancer <- read_csv(\"../../data/geneMFS.csv\") \n\nThe objective of this study is to identify which genes may be good or poor prognosis for the development of metastasis.\n\nGraphically compare the distribution of the gene expressions in the groups of patients with early metastasis (MFS <1000) and late metastasis (MFS>1000).\n\nHint: Consider using scale_y_log10()\n\nCompare the gene expression levels in these two groups using a parametric test.\n\nHint: Perform a t-test for comparing the log-expression of each gene. Use the p.adjust function to apply the Bonferroni correction and the Benjamini-Hochberg correction to these p-values.\n\nCompare these results with those obtained using a non parametric test.\n\nHint: Perform Wilcoxon rank sum tests (using the wilcox.test function) instead of t tests."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#preliminary",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#preliminary",
    "title": "Polynomial regression models",
    "section": "Preliminary",
    "text": "Only functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. You could also try the package broom that standardizes the output of built-in R functions for statistical modelling\n\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(ggfortify)  # extend some ggplot2 features\nlibrary(rsample)    # create training/test sets \nlibrary(kableExtra) # fancy table display\nlibrary(broom)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#the-cars-data-example",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#the-cars-data-example",
    "title": "Polynomial regression models",
    "section": "1 The ‘cars’ data example",
    "text": "The data set cars gives the speed of cars and the distances taken to stop (Note that the data were recorded in the 1920s). The data frame consists of 50 observations (rows) and 2 variables (columns): speed (mph), stopping distance (ft)\n\ndata(cars)\nhead(cars) %>% kable() %>% kableExtra::kable_classic()\n\n\n \n  \n    speed \n    dist \n  \n \n\n  \n    4 \n    2 \n  \n  \n    4 \n    10 \n  \n  \n    7 \n    4 \n  \n  \n    7 \n    22 \n  \n  \n    8 \n    16 \n  \n  \n    9 \n    10 \n  \n\n\n\n\n\nScatter plots can help visualize any relationship between the explanatory variable speed (also called regression variable, or predictor) and the response variable dist.\n\n\nShow the code\ncars_plot <-\n  cars %>% \n  ggplot() +  aes(x = speed, y = dist) + \n  geom_point(size = 2, colour=\"#993399\") + xlab(\"Speed (mph)\") + ylab(\"Stopping distance (ft)\")  \ncars_plot\n\n\n\n\n\nBased on this data, our objective is to build a regression model of the form\ny_j = f(x_j) + \\varepsilon_j \\quad ; \\quad 1 \\leq j \\leq n\nwhere (x_j, 1 \\leq j \\leq n) and (y_j, 1 \\leq j \\leq n) represent, respectively, the n measured speeds and distances and where (\\varepsilon_j, 1 \\leq j \\leq n) is a sequence of residual errors. In other words, \\varepsilon_j represents the difference between the distance predicted by the model f(x_j) and the observed distance y_j.\nWe will restrict ourselves to polynomial regression, by considering functions of the form \n\\begin{aligned}\nf(x) &= f(x ; c_0, c_1, c_2, \\ldots, c_d) \\\\\n&= c_0 + c_1 x + c_2 x^2 + \\ldots + c_d x^d\n\\end{aligned}\n\nBuilding a polynomial regression model requires to perform the following tasks:\n\nFor a given degree d,\n\nestimate the parameters of the model,\nassess the validity of the model,\nevaluate the predictive performance of the model\n\nCompare the different possible models and select the best one(s)."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#fitting-polynomial-models",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#fitting-polynomial-models",
    "title": "Polynomial regression models",
    "section": "2 Fitting polynomial models",
    "text": "2.1 Fitting a polynomial model as a linear model\nRemark that the polynomial regression model \ny_j = c_0 + c_1 x_j + c_2 x^2_j + \\ldots + c_{d} x^{d}_j + \\varepsilon_j\n can be written in a matrix form y = X\\beta + varepsilon  where \ny = \\left( \\begin{array}{c}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \n\\end{array}\\right), \\quad \nX = \\left( \\begin{array}{cccc} \n1 & x_1 & \\cdots & x_1^{d} \\\\ \n1 & x_2 & \\cdots & x_2^{d} \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & \\cdots & x_n^{d} \n\\end{array}\\right), \\quad \n\\beta = \\left( \\begin{array}{c}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_{d} \n\\end{array}\\right), \\quad \n\\varepsilon = \\left( \\begin{array}{c}\n\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \n\\end{array}\\right)\n\nThen, X\\beta is the vector of predicted values and varepsilon the vector of residual errors. Then, all what is said in the lecture about linear regression model can be applied.\n\n\n2.2 Fitting a polynomial of degree 0\nA very basic model considers that the observations (y_j) fluctuates around a constant value c_0:\ny_j = c_0 + \\varepsilon_j\n\nlm0 <- lm(dist ~ 1, data = cars)\nlm0\n\n\nCall:\nlm(formula = dist ~ 1, data = cars)\n\nCoefficients:\n(Intercept)  \n      42.98  \n\n\nThe coefficient c_0 is the empirical mean\n\nmean(cars$dist)\n\n[1] 42.98\n\n\nLooking how the model fits the data is more than enough for concluding that this model is miss-specified.\n\n\nShow the code\ncars_plot <- cars_plot + \n  geom_smooth(method = \"lm\", formula = y ~ 1, se = FALSE)\ncars_plot\n\n\n\n\n\nIndeed, a clear increasing trend is visible in the data while the model assumes that there is no trend. We therefore reject this first model.\n\n\n2.3 Fitting a polynomial of degree 1\n\n2.3.1 Numerical results\nThe scatter plot above rather suggests a linearly increasing relationship between the explanatory and response variables. Let us therefore assume now a linear trend which is mathematically represented by a polynomial of degree 1:\ny_j = c_0 + c_1 x_j + \\varepsilon_j \\quad ; \\quad 1 \\leq j \\leq n\nWe can then fit this model to our data using the function lm:\n\nlm1 <- lm(dist ~ speed, data = cars)\ncoef(lm1)\n\n(Intercept)       speed \n -17.579095    3.932409 \n\n\nThese coefficients are the intercept and the slope of the regression line, but more informative results about this model are available:\n\nsummary(lm1)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nThe slope c_1 is clearly statistically significant while the model explains about 65\\% of the variability of the data. The confidence interval for c_1 confirms that an increase of the speed leads to a significant increase of the stopping distance.\n\nconfint(lm1)\n\n                 2.5 %    97.5 %\n(Intercept) -31.167850 -3.990340\nspeed         3.096964  4.767853\n\n\nSee how these quantities are computed.\n\n\n2.3.2 Some diagnostic plots\nThe fact that the slope is significantly different from zero does not imply that this polynomial model of degree 1 correctly describes the data: at this stage, we can only conclude that a polynomial of degree 1 better explains the variability of the data than a constant model.\nDiagnostic plots are visual tools that allows one to ``see’’ if something is not right between a chosen model and the data it is hypothesized to describe.\nFirst, we can add the regression line to the plot of the data.\n\ncars_plot <- cars_plot + \n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, colour=\"#339900\")\ncars_plot\n\n\n\n\nThe regression line describes pretty well the global trend in the data: based on this graphic, there is no reason to reject the model.\nSeveral diagnostic plots are available for a lm object. The first two are a plot of residuals against fitted values and a normal QQ plot.\n\nautoplot(lm0, which = 1:2) ## autoplot comes from ggfortify\n\n\n\n\nThe residual plot shows a slight (decreasing and increasing) trend which suggests that the residuals are not identically distributed around 0. Furthermore, the QQ plot shows that the extreme residual values are not the extreme values of a normal distribution. It may be therefore necessary to improve the regression model.\n\n\n2.3.3 The predictive performance of the model\nEven if the model it somewhat miss-specified, it may have good predictive properties.\nA common practice is to split the dataset into a 80:20 sample (training:test), then, build the model on the 80% sample and then use the model thus built to predict the reponse variable on test data.\nDoing it this way, we will have the model predicted values for the 20% data (test). We can then see how the model will perform with this ``new’’ data, by comparing these predicted values with the original ones. We can alo check the stability of the prediction given by the model, by comparing these predicted values with those obtained previouly, when the complete data were used for building the model.\nLet us first randomly define the training and test samples: ::: {.cell hash=“map566-lecture-polynomial-regression_cache/html/unnamed-chunk-11_3176fee90730f120174f51b2dcf4dd5f”}\ncars_split    <- rsample::initial_split(cars, prop = 0.8)\ncars_training <- rsample::training(cars_split)\ncars_test     <- rsample::testing(cars_split)\n:::\n\npred_testing_set  <- \n  lm(dist ~ speed, data = cars_training) %>% \n  predict(cars_test)\n\n\n\nShow the code\ncars_pred_plot <-\n  cars_training %>% ggplot() +  aes(x = speed, y = dist) + \n  geom_point(size = 2, colour=\"#993399\") +  xlab(\"Speed mph)\") + ylab(\"Stopping distance (ft)\") + \n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  geom_point(data = data.frame(cars_test, testing = pred_testing_set),\n             mapping = aes(x = speed, y = dist), size = 2, shape = 3, colour=\"red\")\ncars_pred_plot\n\n\n\n\n\nOn one hand, it is reassuring to see that removing part of the data has a very little impact on the predictions. On the other hand, the predictive performance of the model remains limited because of the natural variability of the data. Indeed, this model built with the training sample only explains 65 % of the variabilility of the new test sample.\n\nR2_test <- cor(pred_testing_set, cars_test$dist)^2\nR2_test\n\n[1] 0.6510403\n\n\nLet us see now how the model behaves with extreme values by defining as test sample the data points with the 5 smallest and the 5 largest speeds.\n\ncars_training <- cars[c(6:45) ,]\ncars_test     <- cars[-c(6:45),]\npred_testing_set  <- \n  lm(dist ~ speed, data = cars_training) %>% \n  predict(cars_test)\n\n\n\nShow the code\ncars_pred_plot <-\n  cars_training %>% ggplot() +  aes(x = speed, y = dist) + \n  geom_point(size = 2, colour=\"#993399\") +  xlab(\"Speed mph)\") + ylab(\"Stopping distance (ft)\") + \n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  geom_point(data = data.frame(cars_test, testing = pred_testing_set),\n             mapping = aes(x = speed, y = dist), size = 2, shape = 3, colour=\"red\")\ncars_pred_plot\n\n\n\n\n\nWe see that the model now underestimate the stopping distance for the largest speeds. In conclusion, the model may be used with confidence for predicting the stopping distance for given speeds which are central values. Another model should probably be developed for predicting stopping distances for extreme speed values.\n\n\n2.3.4 Confidence interval and prediction interval\nImagine we aim to predict the stopping distance for a speed x. Using the estimated coefficients of our polynomial model, the prediction will be\n\\hat{f}(x) = \\hat{c}_0 + \\hat{c}_1x \nThis prediction is an estimation since it is based on estimated coefficients \\hat{c}_0 and \\hat{c}_1. Then, the uncertainty on the prediction should be quantified by providing a confidence interval for f(x).\nSince the model can be used for predicting the stopping distance for non extreme speeds, we will restrict ourselves to speeds between 6 and 23 mph.\n\nalpha <- 0.05\nnew_x <- data.frame( speed = (6:23))\nconf_inter <- \n  setNames(cbind(new_x,\n    predict(lm1, newdata = new_x, interval = \"confidence\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\nhead(conf_inter) %>% kable() %>% kable_classic()\n\n\n \n  \n    speed \n    dist \n    lwr \n    upr \n  \n \n\n  \n    6 \n    6.015358 \n    -2.973341 \n    15.00406 \n  \n  \n    7 \n    9.947766 \n    1.678977 \n    18.21656 \n  \n  \n    8 \n    13.880175 \n    6.307527 \n    21.45282 \n  \n  \n    9 \n    17.812584 \n    10.905121 \n    24.72005 \n  \n  \n    10 \n    21.744993 \n    15.461917 \n    28.02807 \n  \n  \n    11 \n    25.677401 \n    19.964525 \n    31.39028 \n  \n\n\n\n\n\nA prediction interval for a new measured distance y=f(x)+e can also be computed. This prediction interval takes into account both the uncertainty on the predicted distance f(x) and the variability of the measure, represented in the model by the residual error e.\n\npred_inter <-    setNames(cbind(new_x,\n    predict(lm1, newdata = new_x, interval = \"prediction\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\nhead(pred_inter) %>% kable() %>% kable_classic()\n\n\n \n  \n    speed \n    dist \n    lwr \n    upr \n  \n \n\n  \n    6 \n    6.015358 \n    -26.187314 \n    38.21803 \n  \n  \n    7 \n    9.947766 \n    -22.061423 \n    41.95696 \n  \n  \n    8 \n    13.880175 \n    -17.956287 \n    45.71664 \n  \n  \n    9 \n    17.812584 \n    -13.872245 \n    49.49741 \n  \n  \n    10 \n    21.744993 \n    -9.809601 \n    53.29959 \n  \n  \n    11 \n    25.677401 \n    -5.768620 \n    57.12342 \n  \n\n\n\n\n\nLet us plot these two intervals.\n\n\nShow the code\ncars_training %>% ggplot() +  aes(x = speed, y = dist) + geom_point() +\n  geom_ribbon(data = conf_inter, aes(ymin = lwr, ymax = upr), fill = \"red\" , alpha = 0.75) +\n  geom_ribbon(data = pred_inter, aes(ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.25)\n\n\n\n\n\nThese intervals are of very little interest and should not be used in practice. Indeed, we can see that both the confidence interval and the prediction interval contain negative values for small or moderate speeds, which is obviously unrealistic… We’ll see later how to transform the model and/or the data in order to take into account some constraints about the data.\nSee how these intervals are computed.\n\n\n\n2.4 Fitting a polynomial of degree 2\nWe can expect to better describe the extreme values by using a polynomial of higher degree. Let us therefore fit a polynomial of degree 2 to the data.\n\nlm2 <- lm(dist ~  speed + I(speed^2), data = cars)\nsummary(lm2)\n\n\nCall:\nlm(formula = dist ~ speed + I(speed^2), data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.720  -9.184  -3.188   4.628  45.152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  2.47014   14.81716   0.167    0.868\nspeed        0.91329    2.03422   0.449    0.656\nI(speed^2)   0.09996    0.06597   1.515    0.136\n\nResidual standard error: 15.18 on 47 degrees of freedom\nMultiple R-squared:  0.6673,    Adjusted R-squared:  0.6532 \nF-statistic: 47.14 on 2 and 47 DF,  p-value: 5.852e-12\n\n\nSurprisingly, the 3 t-tests seem to indicate that none of the coefficients is statistically significant. This result should be considered with caution.Indeed, it does not mean that the three coefficients can be replaced by 0 (i.e. removed) in the model.\nThe difficulty of interpretating these p-values is due to the strong correlations that exist between the estimates \\hat{c}_0, \\hat{c}_1 and \\hat{c}_2. It is recalled that the variance covariance matrix of the vector of estimates is \\sigma^2(X^\\prime X)^{-1}. We can then easily derive the correlation matrix from covariance matrix.\n\nX <- model.matrix(lm2)\nS <- solve(crossprod(X))\nR <- cov2cor(S)\nR\n\n            (Intercept)      speed I(speed^2)\n(Intercept)   1.0000000 -0.9605503  0.8929849\nspeed        -0.9605503  1.0000000 -0.9794765\nI(speed^2)    0.8929849 -0.9794765  1.0000000\n\n\nWe will see below how to use a sequence of orthogonal polynomials in order to get uncorrelated estimates.\nThe R-squared indicate that 66.7% of the variability of the data is explained by a polynomial of degree 2, instead of 65.1% with a polynomial of degree 1. On the other hand, the standard deviation of the residual error is 15.18 ft with this model instead of 15.38 ft with the previous one. These improvements are really too small to justify the use of a quadratic polynomial.\nNevertheless, plotting the data together with the fitted polynomial suggests that the extreme values are better predicted with a polynomial of degree 2 than with a straight line.\n\n\nShow the code\ncars_plot + \n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2, raw = TRUE), se = FALSE, colour=\"#339900\")\n\n\n\n\n\nShow the code\ncars_plot\n\n\n\n\n\nThe predictive performance of the model need to be assesed in order to confirm this property of the model. We can again use the 80% most central data points to build the model and test it on the 20% remaining data points.\n\n\nShow the code\ncars_plot + \n  geom_smooth(data = cars_training, method = \"lm\", formula = y ~ poly(x, 2, raw = TRUE), se = FALSE, colour=\"#339900\")\n\n\n\n\n\nThe shape of the polynomial is totally different when it is built using the training data only. That means that the model is quite unstable, depending strongly on some data values. Then, repeating the same experiment (under the same experimental conditions) would probably lead to significantly different results.\nSuch lack of reproducibility of the predictions reinforces our idea that this model should be rejected.\n\n\n2.5 Fitting a polynomial without intercept\nThe stopping distance predicted by the model should be null when the speed is null. This constraint can easily be achieved by removing the intercept from the regression model: \nf(x) =  c_1 x + c_2 x^2 + \\ldots + c_d x^d\n Let us fit a regression model of degree 2 without intercept to our data.\n\nlm2_no_inter <- lm(dist ~ 0 + speed + I(speed^2), data = cars)\ncoef(lm2_no_inter)\n\n     speed I(speed^2) \n1.23902996 0.09013877 \n\n\nWe can check that the design matrix consists of the 2 columns (x_j) and (x_j^2).\n\nX <- model.matrix(lm2_no_inter)\nhead(X)\n\n  speed I(speed^2)\n1     4         16\n2     4         16\n3     7         49\n4     7         49\n5     8         64\n6     9         81\n\n\nWe can plot the data with the predicted response f(x), confidence interval for f(x) and prediction intervals for new observations y.\n\n\nShow the code\nalpha <- 0.05\nnew_x <- data.frame( speed = (6:23))\nfit <- data.frame(new_x, dist = predict(lm2_no_inter, newdata = new_x))\nconf_inter <- \n  setNames(cbind(new_x,\n    predict(lm2_no_inter, newdata = new_x, interval = \"confidence\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\npred_inter <- \n  setNames(cbind(new_x,\n    predict(lm2_no_inter, newdata = new_x, interval = \"prediction\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\ncars_training %>% ggplot() +  aes(x = speed, y = dist) + geom_point() +\n  geom_ribbon(data = conf_inter, aes(ymin = lwr, ymax = upr), fill = \"red\" , alpha = 0.75) +\n  geom_ribbon(data = pred_inter, aes(ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.25) +  \n  geom_line(data = fit, aes(x = speed, y = dist), colour=\"#339900\", size = 1)\n\n\n\n\n\nConfidence interval for f(x) now only contain positive values for any x>0 and is reduce to 0 when x=0.\nOn the other hand, prediction intervals for new observations still contain negative values. This is due to the fact that the residual error model is a constant error model y=f(x)+e, assuming that the variance of the error e does not depend on the predicted value f(x). This hypothesis doe not reflect the true phenomena and should be rejected. We will see that an appropriate alternative may consist in transforming the data.\n\n\n2.6 Using orthogonal polynomials\nInstead of defining each component of the regression model lm(dist ~ speed + I(speed^2) ), it is equivalent to define explicitly the regression model as a polynomial of degree 2\n\nlm2_poly <- lm(dist ~ poly(speed, degree = 2, raw = TRUE), data = cars)\ndesign_raw <- model.matrix(lm2_poly)\nhead(design_raw)\n\n  (Intercept) poly(speed, degree = 2, raw = TRUE)1\n1           1                                    4\n2           1                                    4\n3           1                                    7\n4           1                                    7\n5           1                                    8\n6           1                                    9\n  poly(speed, degree = 2, raw = TRUE)2\n1                                   16\n2                                   16\n3                                   49\n4                                   49\n5                                   64\n6                                   81\n\n\nResults obtained with both methods are absolutely identical ::: {.cell hash=“map566-lecture-polynomial-regression_cache/html/unnamed-chunk-28_5b6f0c072339c2f7f20dcc4b6f7e4d66”}\ncoef(lm2_poly)\n\n                         (Intercept) poly(speed, degree = 2, raw = TRUE)1 \n                           2.4701378                            0.9132876 \npoly(speed, degree = 2, raw = TRUE)2 \n                           0.0999593 \n\n:::\nWe have seen that this model leads to highly correlated estimators of the coefficients of the model which make it difficult the interpretation of the results. Indeed, let X be the design matrix for a polynomial model of degree d, \nX = \\left( \\begin{array}{cccc}\n1 & x_1 & \\cdots & x_1^{d} \\\\\n1 & x_2 & \\cdots & x_2^{d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & \\cdots & x_n^{d}\n\\end{array}\\right)\n Then, the columns of X are highly correlated since \n(X^\\prime X)_{kl} = \\sum_{j=1}^n x_j^{k+l-2}\n\nThe option raw = FALSE (which is the default) allows to use an orthogonal basis of polynomial.\n\n2.6.1 The Gram-Schmidt procedure\nOrthogonal polynomials can be obtained by applying the Gram-Schmidt orthogonalization process to the basis 1, x, x^2, , x^{d}:\n\n\\begin{aligned}\np_0(x) &= 1 \\\\\np_1(x) &= x - \\frac{\\langle x, p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(x) \\\\\np_2(x) &= x^2\n- \\frac{\\langle x^2 , p_1 \\rangle}{\\langle p_1, p_1 \\rangle} p_1(x)\n- \\frac{\\langle x^2 , p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(x)  \\\\\n&\\vdots \\\\\np_{d}(x) &= x^{d}\n- \\frac{\\langle x^{d} , p_{d-1} \\rangle}{\\langle p_{d-1}, p_{d-1} \\rangle} p_{d-1}(x)\n- \\cdots\n - \\frac{\\langle x^{d} , p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(x)\n\\end{aligned}\n\nLet \\tilde{X} = (p_0(x) , p_1(x), \\cdots , p_{d}(x)) be the new design matrix. Then, \n\\tilde{X}^\\prime \\tilde{X} = \\left( \\begin{array}{cccc}\nn & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots& \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{array} \\right)\n\n\n\n2.6.2 Orthogonal polynomial basis with degree 1\nLet us see the results obtained with a polynomial of degree 1\n\nlm1_ortho <- lm(dist ~ poly(speed, degree = 1), data = cars)\ndesign_ortho <- model.matrix(lm1_ortho)\nhead(design_ortho)\n\n  (Intercept) poly(speed, degree = 1)\n1           1              -0.3079956\n2           1              -0.3079956\n3           1              -0.2269442\n4           1              -0.2269442\n5           1              -0.1999270\n6           1              -0.1729098\n\ncrossprod(design_ortho)\n\n                         (Intercept) poly(speed, degree = 1)\n(Intercept)             5.000000e+01            1.665335e-16\npoly(speed, degree = 1) 1.665335e-16            1.000000e+00\n\n\nHere, the estimated intercept \\hat{c}_0 is the empirical mean \\bar{y}= NA\n\ncoef(lm1_ortho)\n\n            (Intercept) poly(speed, degree = 1) \n                42.9800                145.5523 \n\n\nAdding a term of degree 2 keeps the first two column of the design matrix unchanged\n\nlm2_ortho <- lm(dist ~ poly(speed, degree = 2), data = cars)\ndesign_ortho <- model.matrix(lm2_ortho)\nhead(design_ortho)\n\n  (Intercept) poly(speed, degree = 2)1 poly(speed, degree = 2)2\n1           1               -0.3079956               0.41625480\n2           1               -0.3079956               0.41625480\n3           1               -0.2269442               0.16583013\n4           1               -0.2269442               0.16583013\n5           1               -0.1999270               0.09974267\n6           1               -0.1729098               0.04234892\n\ncrossprod(design_ortho)\n\n                          (Intercept) poly(speed, degree = 2)1\n(Intercept)              5.000000e+01             1.665335e-16\npoly(speed, degree = 2)1 1.665335e-16             1.000000e+00\npoly(speed, degree = 2)2 1.665335e-16             1.301786e-16\n                         poly(speed, degree = 2)2\n(Intercept)                          1.665335e-16\npoly(speed, degree = 2)1             1.301786e-16\npoly(speed, degree = 2)2             1.000000e+00\n\n\nLet us look at the results obtained with this model:\n\nsummary(lm2_ortho)\n\n\nCall:\nlm(formula = dist ~ poly(speed, degree = 2), data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.720  -9.184  -3.188   4.628  45.152 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                42.980      2.146  20.026  < 2e-16 ***\npoly(speed, degree = 2)1  145.552     15.176   9.591 1.21e-12 ***\npoly(speed, degree = 2)2   22.996     15.176   1.515    0.136    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.18 on 47 degrees of freedom\nMultiple R-squared:  0.6673,    Adjusted R-squared:  0.6532 \nF-statistic: 47.14 on 2 and 47 DF,  p-value: 5.852e-12\n\n\nThese results give rise to several comments:\n\nthe first two estimated coefficients \\hat{c}_0 and \\hat{c}_1 are those obtained previously with a polynomial of degree 1\nthe p-values of the t-test can now properly be interpreted and confirm our previous analyis: the first two coefficients are statistically significant. The need of a quadratic term is much less obvious (p=0.136). Furthermore, confidence intervals can be derived for each coefficient\n\n\nconfint(lm2_ortho)\n\n                              2.5 %    97.5 %\n(Intercept)               38.662361  47.29764\npoly(speed, degree = 2)1 115.021940 176.08257\npoly(speed, degree = 2)2  -7.534552  53.52608\n\n\n\nThe estimated variance of the residuals \\hat{\\sigma}^2, the R-squared value and the adjusted R-squared value are identical when orthogonal or non orthogonal polynomial are used. Indeed, even if the parameterization is different, the two models are the same polynomials of degree 2 and therefore lead to the same predictions.\n\nSee how these quantities are computed."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#model-comparison",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#model-comparison",
    "title": "Polynomial regression models",
    "section": "3 Model comparison",
    "text": "3.1 t-test\nSeveral quantitative criteria and several statistical tests are available for comparing models.\nFirst, we have seen that t-test are performed for each coefficient of the model\n\ntidy(lm1_ortho)\n\n# A tibble: 2 × 5\n  term                    estimate std.error statistic  p.value\n  <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)                 43.0      2.18     19.8  1.06e-24\n2 poly(speed, degree = 1)    146.      15.4       9.46 1.49e-12\n\ntidy(lm2_ortho)\n\n# A tibble: 3 × 5\n  term                     estimate std.error statistic  p.value\n  <chr>                       <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)                  43.0      2.15     20.0  1.19e-24\n2 poly(speed, degree = 2)1    146.      15.2       9.59 1.21e-12\n3 poly(speed, degree = 2)2     23.0     15.2       1.52 1.36e- 1\n\n\nBased on these results, we can conclude that c_0 \\neq 0 and c_1 \\neq 0. On the other hand, the data does not allow us to conclude that c_2 \\neq 0.\n\n\n3.2 Analysis-of-variance (anova)\nBy construction, the F-statistics provided with the summary of a fitted model is the F-statistics of an anova for testing this fitted model against the model without explanatory variable lm0. This test is known as the overall F-test for regression.\n\nanova(lm0, lm1)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ 1\nModel 2: dist ~ speed\n  Res.Df   RSS Df Sum of Sq      F   Pr(>F)    \n1     49 32539                                 \n2     48 11354  1     21186 89.567 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm0, lm2)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ 1\nModel 2: dist ~ speed + I(speed^2)\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1     49 32539                                  \n2     47 10825  2     21714 47.141 5.852e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNot surprisingly, both lm1 and lm2 are clearly preferred to lm0. As already stated above the t-test and the F-test for testing lm0 against lm1 are equivalent.\nWe can also perform an ANOVA to test lm1 against lm2:\n\nanova(lm1, lm2)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ speed\nModel 2: dist ~ speed + I(speed^2)\n  Res.Df   RSS Df Sum of Sq     F Pr(>F)\n1     48 11354                          \n2     47 10825  1    528.81 2.296 0.1364\n\n\nlm1 is slightly preferred to lm2 for this criterion.\n\n\n3.3 Likelihood ratio test (LRT)\nThe likelihood ratio test can be used for testing 2 nested models. Since our 3 polynomial models are nested, we can use the LRT fo testing one of these model against another one.\nLog-likelihoods \\log\\ell_0(\\hat\\theta_0), \\log\\ell_1(\\hat\\theta_1) and \\log\\ell_2(\\hat\\theta_2) are available, where \\hat\\theta_k is the maximum likelihood estimate of the parameter of model lmk for k=0, 1, 2.\n\nlogLik(lm0)\nlogLik(lm1)\nlogLik(lm2)\n\n\n\n'log Lik.' -232.9012 (df=2)\n\n\n'log Lik.' -206.5784 (df=3)\n\n\n'log Lik.' -205.386 (df=4)\n\n\nFor testing lm0 against lm1, we compute the test statistic 2(\\log\\ell_1(\\hat\\theta_1) - \\log\\ell_0(\\hat\\theta_0) which has a \\chi^2 distribution with 1 degree of freedom under lm0. Then, the p-value of this test can easily be computed:\n\ndl <- 2*as.numeric(logLik(lm1) - logLik(lm0))\n1-pchisq(dl,1)\n\n[1] 3.995693e-13\n\n\nThe test statistic 2(\\log\\ell_2(\\hat\\theta_2) - \\log\\ell_1(\\hat\\theta_1) is used for testing lm1 against lm2:\n\ndl <- 2 * as.numeric(logLik(lm2) - logLik(lm1))\n1 - pchisq(dl,1)\n\n[1] 0.122521\n\n\nWe can see with this example that the ANOVA and the LRT give very similar results and lead to the same conclusion: the constant model can be rejected with high confidence (i.e. the stopping distance depends on the speed), while a model with both a linear and a quadratic component is not preferred to a model with a linear component only (i.e. we don’t reject the hypothesis that the stopping distance increase linearly with the speed).\n\n\n3.4 Information criteria\nInformation criteria such as the Akaike information criterion (AIC) and the Bayesian information criaterion (BIC) can also be used for comparing models which are not necessarily nested.\n\nAIC(lm0, lm1, lm2)\n\n    df      AIC\nlm0  2 469.8024\nlm1  3 419.1569\nlm2  4 418.7721\n\nBIC(lm0, lm1, lm2)\n\n    df      BIC\nlm0  2 473.6265\nlm1  3 424.8929\nlm2  4 426.4202\n\n\nModels with lowest AIC and/or BIC will be preferred. Here, both criteria agree for rejecting lm0 with high confidence. AIC has a very slight preference for lm2 while BIC penalizes a little bit more polynomial with higher degree and prefers lm1. Nevertheless, these differences are not large enough for selecting definitely any of these 2 models.\nSee how these quantities are computed."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#data-transformation",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#data-transformation",
    "title": "Polynomial regression models",
    "section": "4 Data transformation",
    "text": "When fitting a linear regression model one assumes that there is a linear relationship between the response variable and each of the explanatory variables. However, in many situations there may instead be a non-linear relationship between the variables. This can sometimes be remedied by applying a suitable transformation to some (or all) of the variables, such as power transformations or logarithms.\nIn addition, transformations can be used to correct violations of model assumptions such as constant error variance and normality.\nApplying suitable transformations to the original data prior to performing regression may be sufficient to make linear regression models appropriate for the transformed data."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#log-based-tranformations",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#log-based-tranformations",
    "title": "Polynomial regression models",
    "section": "5 Log based tranformations",
    "text": "Let’s plot the data using semi-log scales: we use a logarithmic scale for the x-axis and a linear scale for the y-axis in the first plot, a linear scale for the x-axis and a logarithmic scale for the y-axis in the second plot.\n\ngrid.arrange(\n  cars_plot + scale_x_log10(),\n  cars_plot + scale_y_log10(), nrow = 1)\n\n\n\n\nNone of thee two plots show any linear trend. Then, let us try with a log-log transformation.\n\ncars_plot + scale_x_log10() + scale_y_log10()\n\n\n\n\nLooks much better!\nWe can then try to fit the following model to the data:  \\log(y_j) = c_0 + c_1\\log(x_j) + \\varepsilon_j  Another representation of this model exhibits a power relationship between the explanatory and response variables  y_j = A \\, x_j^{c_1} \\, e^{\\varepsilon_j}  where A=e^{c_0}. This model ensures that the response y can only take positive values. Furthermore, the variability of the response increases with the explanatory variable. Indeed, for small \\varepsilon_j,  y_j \\approx A \\, x_j^{c_1}(1 + \\varepsilon_j)\n\nlm1_log <- lm(log(dist) ~ log(speed), data = cars)\nsummary(lm1_log)\n\n\nCall:\nlm(formula = log(dist) ~ log(speed), data = cars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.00215 -0.24578 -0.02898  0.20717  0.88289 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.7297     0.3758  -1.941   0.0581 .  \nlog(speed)    1.6024     0.1395  11.484 2.26e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4053 on 48 degrees of freedom\nMultiple R-squared:  0.7331,    Adjusted R-squared:  0.7276 \nF-statistic: 131.9 on 1 and 48 DF,  p-value: 2.259e-15\n\n\nThis model now explains 73% of the (transformed) data.\n\n5.1 Diagnostic plots\nLet us plot the data with the regression line:\n\ncars_plot_log <-\n  ggplot(cars) + aes(x = log(speed), y = log(dist)) + \n  geom_point(size = 2, colour=\"#993399\") \ncars_plot_log + geom_smooth(method = \"lm\", formula = y ~ x)\n\n\n\n\nThis plot, plus the two following diagnostic plots, show that the model fits quite well the data.\n\nautoplot(lm1_log, which = 1:2)\n\n\n\n\nIn the next plot, we compare the fits obtained when the model is built using the complete data (green) and when only the training sample is used (red).\n\n\nShow the code\ntraining <- c(6:45)\ncars_test     <- cars[-training, ]\ncars_training <- cars[ training , ]\ncars_plot_log +\n  geom_smooth(data = cars_training, \n    mapping = aes(x = log(speed), y = log(dist)),\n    method = \"lm\", formula = y ~ x, se = FALSE, color = \"red\") +\n  geom_smooth(data = cars,\n    mapping = aes(x = log(speed), y = log(dist)),\n    method = \"lm\", formula = y ~ x, se = FALSE)\n\n\n\n\n\nThese two models are very similar: the estimation is relatively insensible to extreme values.\n\n\n5.2 Confidence interval and prediction interval\nBased on the previous analysis, the model can be used for predicting stopping distances for a wide range of speeds. Let us compute and display the confidence interval for the regression line (of the model lm(log(dist) ~ log(speed))) and the prediction interval for new observed log-distances.\n\n\nShow the code\nalpha <- 0.05\nnew_x <- data.frame( speed = 3:30)\nfit <- data.frame(new_x, log_dist = predict(lm1_log, newdata = new_x))\nconf_inter <- \n  setNames(cbind(new_x,\n    predict(lm1_log, newdata = new_x, interval = \"confidence\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\npred_inter <- \n  setNames(cbind(new_x,\n    predict(lm1_log, newdata = new_x, interval = \"prediction\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\ncars_training %>% ggplot() +  aes(x = log(speed), y = log(dist)) + geom_point() +\n  geom_ribbon(data = conf_inter, aes(ymin = lwr, ymax = upr), fill = \"red\" , alpha = 0.75) +\n  geom_ribbon(data = pred_inter, aes(ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.25) +\n  geom_line(data = fit, aes(x = log(speed), y = log_dist), colour=\"#339900\", size = 1) +\n      xlab(\"speed (mph)\") + ylab(\"stopping distance (ft)\")  \n\n\n\n\n\nWe can derive from these intervals a confidence interval and a prediction interval for the non transformed data (i.e. using the original scale).\n\n\nShow the code\ncars_training %>% ggplot() +  aes(x = speed, y = dist) + geom_point() +\n  geom_ribbon(data = conf_inter, aes(ymin = exp(lwr), ymax = exp(upr)), fill = \"red\" , alpha = 0.75) +\n  geom_ribbon(data = pred_inter, aes(ymin = exp(lwr), ymax = exp(upr)), fill = \"blue\", alpha = 0.25) +\n  geom_line(data = fit, aes(x = speed, y = exp(log_dist)), colour=\"#339900\", size = 1) +\n      xlab(\"speed (mph)\") + ylab(\"stopping distance (ft)\")  \n\n\n\n\n\nThe general trend describes quite well how the stopping distance increases with the speed. Nevertheless, the residual error model seems to over-estimate the variability of the data for the greatest speeds.\n\n\n5.3 Model comparison\nWe will investigate now if the numerical criteria for model comparison confirm that this model based on a log-log transformation should be preferred to the polynomial models.\nLet us compute first the log-likelihood for this model.\n\nlogLik(lm1_log)\n\n'log Lik.' -24.76592 (df=3)\n\n\nWARNING: This value of the log-likelihood cannot be compared as it is to the log-likelihoods computed previously. Indeed, by definition, the log-likelihood for model lm1.log is the probability density function (pdf) of log(y).\n\n\\begin{aligned}\n\\log\\ell_{\\rm lm1.log} &= \\log\\ell(\\hat{\\theta}_{\\rm lm1.log}) \\\\\n&= \\mathbb{P} (\\log(y) ; \\hat{\\theta}_{\\rm lm1.log})\n\\end{aligned}\n where \\hat{\\theta}_{\\rm lm1.log} is the ML estimator of \\theta for model lm1.log.\nThen, the pdf of \\log(y) under this model cannot be directly compared to the pdf of y under another model. We need instead to compute the pdf of y under this model. For any \\theta and any 1 \\leq j \\leq n,\n\n\\begin{aligned}\n\\mathbb{P}(y_j ; \\theta) &= \\mathbb{P}(\\log(y_j) ; \\theta)\\times \\frac{1}{y_j}\n\\end{aligned}\n and  \\log (\\mathbb{P}(y ; \\theta)) = \\log (\\mathbb{P} (\\log(y) ; \\theta)) - \\sum_{j=1}^n \\log(y_j) \n\nlogLik(lm1_log) - sum(log(cars$dist))\n\n'log Lik.' -201.5613 (df=3)\n\n\nThe information criteria should also take into account this transformation\n\nAIC(lm1_log) + 2 * sum(log(cars$dist))\n\n[1] 409.1226\n\nBIC(lm1_log) + 2 * sum(log(cars$dist))\n\n[1] 414.8587\n\n\nBoth criteria prefer this model based on a log-log transformation than any polynomial model without any transformation.\nRemark: Since models lm1 and lm1_log have the same number of parameters, log-likelihoods without penalisation can be used for comparing these 2 models."
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#regression-models",
    "href": "docs/regression/map566-lecture-regression-background.html#regression-models",
    "title": "Linear regression: quick recap",
    "section": "Regression models",
    "text": "A regression model relates a response variable y to a set of explanatory variables x. Assuming that we have access to n set of values (x_j, y_j), 1 \\leq j \\leq n), of these variable, the regression model is assumed to take the form y_j = f(x_j,\\beta) + \\varepsilon_j \\quad ; \\quad 1\\leq j \\leq n\nwhere f is a structural model which depends on a p-vector of parameters \\beta. We will assume that the residuals (\\varepsilon_j) are independent random variables with mean 0 and variance \\sigma^2:  \n\\mathbb{E}{\\varepsilon_j} = 0 \\quad ; \\quad  \\mathbb{E}{\\varepsilon^2_j} = \\sigma^2 \\quad ; \\quad \\mathbb{E}{\\varepsilon_j \\varepsilon_k} = 0  \\ (j \\neq k)"
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#ordinary-least-squares",
    "href": "docs/regression/map566-lecture-regression-background.html#ordinary-least-squares",
    "title": "Linear regression: quick recap",
    "section": "1 Ordinary least squares",
    "text": "1.1 Least squares estimator\nA method for choosing automatically the “best parameters” \\beta consists in minimizing the sum of squared errors of prediction, i.e. the residual sum of squares (RSS) :\n \nRSS(\\beta) = \\sum_{j=1}^n (y_j - f(x_j))^2 = \\|y - X\\beta \\|^2\n\nThen, \n\\hat{\\beta} = \\arg\\min_{\\beta} \\|y - X\\beta \\|^2 = (X^\\prime X)^{-1}X^\\prime y \n\n\n\n1.2 Estimation of the residual error variance\nAn unbiased estimate of \\sigma^2 is \n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\|y - X\\hat{\\beta} \\|^2 \n Indeed,\n\n\\begin{aligned}\n\\|y - X\\hat{\\beta} \\|^2 &= \\| y - X(X^\\prime X)^{-1}X^\\prime y\\|^2   \\\\\n&= \\| \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon\\|^2 \\\\\n&= \\varepsilon^\\prime \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right)^\\prime \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon \\\\\n&= \\varepsilon^\\prime \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon \\\\\n&= {\\rm trace} \\left\\{ \\varepsilon^\\prime \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon \\right\\} \\\\\n&= {\\rm trace} \\left\\{  \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon \\varepsilon^\\prime \\right\\} \n\\end{aligned}\n Then,\n\n\\begin{aligned}\n\\mathbb{E}{\\|y - X\\hat{\\beta} \\|^2} &= {\\rm trace} \\left(  \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\mathbb{E}{\\varepsilon \\varepsilon^\\prime} \\right) \\\\\n&= \\sigma^2 {\\rm trace} \\left(  {\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\\\\n&= \\sigma^2 \\left( {\\rm trace} \\left( {\\rm I}_n \\right)  - {\\rm trace} \\left(X(X^\\prime X)^{-1}X^\\prime \\right) \\right) \\\\\n&= \\sigma^2 \\left( n  - {\\rm trace} \\left((X^\\prime X)^{-1}X^\\prime X \\right) \\right) \\\\\n&= \\sigma^2 \\left( n  - {\\rm trace} \\left({\\rm I}_d \\right) \\right) \\\\\n&= \\sigma^2 ( n - p)\n\\end{aligned}\n\nThe standard deviation of the residual errors, called residual standard error in R, is the square root of this estimated variance\n\n\n1.3 The standard errors of the estimates\nWe can remark that \n\\begin{aligned}\n\\hat{\\beta} &=  (X^\\prime X)^{-1}X^\\prime y \\\\\n&= \\beta + (X^\\prime X)^{-1}X^\\prime \\varepsilon\n\\end{aligned}\n\nThen, since \\mathbb{E}{e}=0,  \\mathbb{E}{\\hat{\\beta}} = \\beta\nand \n\\begin{aligned}\n\\mathbb{V}{\\hat\\beta} &=  \\mathbb{V}{(X^\\prime X)^{-1}X^\\prime \\varepsilon} \\\\\n&= (X^\\prime X)^{-1}X^\\prime \\mathbb{V}{\\varepsilon} X (X^\\prime X)^{-1} \\\\\n&= \\sigma^2 (X^\\prime X)^{-1}\n\\end{aligned}\n\nWe can therefore use this formula to compute the variance covariance matrix of \\hat\\beta. Then, the standard error of each component of \\hat\\beta is defined as the square root of the diagonal elements of the variance-covariance matrix V=\\mathbb{V}{\\hat\\beta}:\n{\\rm se}(\\hat\\beta_k) = \\sqrt{V_{k k}}"
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#maximum-likelihood-approach",
    "href": "docs/regression/map566-lecture-regression-background.html#maximum-likelihood-approach",
    "title": "Linear regression: quick recap",
    "section": "2 Maximum likelihood approach",
    "text": "If we assume that (\\varepsilon_j, 1 \\leq j \\leq n) is a sequence of independent and normally distributed random variables with mean 0 and variance 1:  \n\\varepsilon_j \\sim^{\\mathrm{iid}} \\mathcal{N}(0, 1),\n then the y_j are also independent and normally distributed:\n y_j \\sim \\mathcal{N}(f(x_j, \\beta),\\sigma^2). The vector y=(y_1,y_2,\\ldots,y_n) is therefore a Gaussian vector which probability density function (pdf) depends on a vector of parameters \\theta=(\\beta,\\sigma^2):\n\\begin{aligned}\n\\mathbb{P}(y ; \\theta) = \\prod_{j=1}^n \\mathbb{P}(y_j; \\theta) & = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\text{exp}\\left(-\\frac{1}{2\\sigma^2}(y_j - f(x_j, \\beta))^2 \\right) \\\\\n& =  (2\\pi \\sigma^2)^{-\\frac{n}{2}} \\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j, \\beta))^2\\right).\n\\end{aligned}\n\n2.1 Likelihood\nFor a given vector of observations y, the likelihood \\ell is the function of the parameter \\theta=(\\beta,\\sigma^2) defined as:\n \n\\ell(\\theta) = \\mathbb{P}(y ; \\theta) \n The log-likelihood is therefore \n\\log\\ell(\\theta) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2\n\n\n\n2.2 Maximum likelihood estimator\nAssume that \\theta takes its values in a subset \\Theta of \\mathbb{R}^p. Then, the Maximum Likelihood (ML) estimator of \\theta is a function of y that maximizes the likelihood function:\n \n\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta}\\ell(\\theta) = \\arg\\max_{\\theta \\in \\Theta}\\log\\ell(\\theta)\n\nMaximization of the log-likelihood can be performed in two steps:\n\n\\beta, the parameter of the structural model is estimated by minimizing the residual sum of squares:\n\n\\begin{aligned}\n\\hat{\\beta} &= \\arg\\min_{\\beta} \\left\\{\nn\\log(2\\pi) + n\\log(\\sigma^2) + \\frac{1}{\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2\n\\right\\} \\\\\n&= \\arg\\min_{\\beta}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2\n\\end{aligned}\nWe see that, for this model, the Maximum Likelihood estimator \\hat{\\beta} is also the Least Squares estimator of \\beta.\n\n\\sigma^2, the variance of the residual errors \\varepsilon_j is estimated in a second step:\n\n\\begin{aligned}\n\\hat{\\sigma}^2 &= \\arg\\min_{\\sigma^2 \\in \\mathbb{R}^+} \\left\\{\nn\\log(2\\pi) + n\\log(\\sigma^2) + \\frac{1}{\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j,\\hat{\\beta}))^2\n\\right\\} \\\\\n&= \\frac{1}{n}\\sum_{j=1}^n(y_j - f(x_j,\\hat{\\beta}))^2\n\\end{aligned}\nFinally, the log-likelihood computed with \\hat{\\theta}=(\\hat{\\beta},\\hat{\\sigma}^2) reduces to \n\\log\\ell(\\hat{\\theta}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{1}{n}\\sum_{j=1}^n(y_j - f(x_j,\\hat{\\beta}))^2\\right) -\\frac{n}{2}\n\n\n\n2.3 The Fisher Information matrix\n\n\n2.4 Some general definitions\nThe partial derivative of the log-likelihood with respect to \\theta is called the score. Under general regularity conditions, the expected value of the score is 0. Indeed, it is easy to show that\n\\mathbb{E}{\\frac{\\partial}{\\partial \\theta} \\log\\mathbb{P}(y;\\theta^\\star)} = 0.\nwhere \\theta^\\star is the ``true’’ unknown value of \\theta such that the observations y where generated with model \\mathbb{P}(\\cdot;\\theta^\\star).\nThe variance of the score is called the Fisher information matrix (FIM): \nI_n(\\theta^\\star) = \\mathbb{E}{\\left(\\frac{\\partial}{\\partial \\theta} \\log\\mathbb{P}(y;\\theta^\\star)\\right)\\left(\\frac{\\partial}{\\partial \\theta} \\log\\mathbb{P}(y;\\theta^\\star)\\right)^\\prime}.\n\nFurthermore, it can be shown that if \\log\\ell is twice differentiable with respect to \\theta,\n\n\\begin{aligned}\nI_n(\\theta^\\star) &= -  \\mathbb{E}{\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^\\prime} \\log\\mathbb{P}(y;\\theta^\\star)} \\\\\n&= - \\sum_{j=1}^n \\mathbb{E}{\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^\\prime} \\log\\mathbb{P}(y_j;\\theta^\\star)} \n\\end{aligned}\n\n\n\n2.5 The central limit theorem\nThe following central limit theorem (CLT) holds under certain regularity conditions:  \nI_n(\\theta^\\star)^{\\frac{1}{2}}(\\hat{\\theta}-\\theta^\\star) \\xrightarrow{n\\to \\infty} {\\mathcal N}(0,{\\rm Id}_n) .\n This theorem shows that under relevant hypotheses, the estimator \\hat{\\theta} is consistent and converges to \\theta^\\star at rate \\sqrt{n} since I_n={\\cal O}(n).\nThe normalizing term I_n(\\theta^\\star)^{-1} is unknown since it depends on the unknown parameter \\theta^\\star. We can use instead the observed Fisher information: \n\\begin{aligned}\n I_{y}(\\hat{\\theta}) &= - \\frac{\\partial^2}{\\partial \\theta^2} \\log\\ell(\\hat{\\theta}) \\\\\n&=-\\sum_{i=1}^n \\frac{\\partial^2}{\\partial \\theta^2} \\log \\mathbb{P}(y_i ; \\hat{\\theta}).\n\\end{aligned}\n We can then approximate the distribution of \\hat{\\theta} by a normal distribution with mean \\theta^\\star and variance-covariance matrix I_y(\\hat{\\theta})^{-1}: \n\\hat{\\theta} \\approx {\\mathcal N}(\\theta^\\star , I_y(\\hat{\\theta})^{-1}) .\n The square roots of the diagonal elements of I_y(\\hat{\\theta})^{-1} are called the standard errors (s.e.) of the elements of \\hat{\\theta}.\n\n\n2.6 The FIM for a regression model\nWe have seen that, for a regression model, \n\\begin{aligned}\n\\log\\ell(\\theta) &= \\log\\ell(\\beta,\\sigma^2) \\\\\n&= -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2\n\\end{aligned}\n By definition,\n\nI_n(\\theta) =  \\left( \\begin{array}{cc} \n-\\mathbb{E}{\\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\log\\ell(\\beta,\\sigma^2)} & \n-\\mathbb{E}{\\frac{\\partial^2}{\\partial \\beta \\partial \\sigma^2} \\log\\ell(\\beta,\\sigma^2)} \\\\ \n-\\mathbb{E}{\\frac{\\partial^2}{\\partial \\sigma^2 \\partial \\beta^\\prime } \\log\\ell(\\beta,\\sigma^2)} &  \n-\\mathbb{E}{\\frac{\\partial^2}{\\partial \\sigma^{2^2} } \\log\\ell(\\beta,\\sigma^2)}\n\\end{array} \\right)\n\nThen, \n\\begin{aligned}\n\\mathbb{E}{\\frac{\\partial^2}{\\partial \\beta \\partial \\sigma^2} \\log\\ell(\\beta,\\sigma^2)} &=\n -\\frac{1}{\\sigma^4} \\times\\frac{\\partial}{\\partial \\beta}f(x_j,\\beta) \\times\\mathbb{E}{y_j - f(x_j,\\beta)} \\\\\n &= 0\n\\end{aligned}\n\nand the FIM reduces to\n\nI_n(\\theta) =  \\left( \\begin{array}{cc} \n-  \\mathbb{E}{\\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\log\\ell(\\beta,\\sigma^2)} & 0 \\\\ \n0 &  -\\mathbb{E}{\\frac{\\partial^2}{\\partial \\sigma^{2^2} } \\log\\ell(\\beta,\\sigma^2)}\n\\end{array} \\right)\n\nBecause of the bloc structure of I_n(\\theta^\\star), the variance-covariance of \\hat{\\beta} can be estimated by I^{-1}_y(\\hat{\\beta}) where \n\\begin{aligned}\n I_y(\\hat{\\beta}) &= - \\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\log\\ell(\\hat{\\beta},\\hat{\\sigma}^2) \\\\\n&= \\frac{1}{2\\hat\\sigma^2}\\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\left(\\sum_{j=1}^n(y_j - f(x_j,\\hat\\beta))^2 \\right) \\\\\n&= \\frac{1}{\\hat\\sigma^2} \\sum_{j=1}^n \\left( \n\\left(\\frac{\\partial}{\\partial \\beta}f(x_j,\\hat\\beta)\\right)\\left(\\frac{\\partial}{\\partial \\beta}f(x_j,\\hat\\beta)\\right)^\\prime - \\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime}f(x_j,\\hat\\beta)y_j\n\\right)\n\\end{aligned}\n Remark: In the case of a linear model y=X\\beta+e, we find that I_y(\\hat{\\beta}) = (X^\\prime X)/\\hat\\sigma^2.\nThe variance of \\hat{\\sigma}^2 is estimated by I^{-1}_y(\\hat{\\sigma}^2) where \n\\begin{aligned}\nI_y(\\hat{\\sigma}^2) &= -\\frac{\\partial^2}{\\partial \\sigma^{2^2} } \\log\\ell(\\hat{\\beta},\\hat{\\sigma}^2) \\\\\n&= -\\frac{n}{2\\hat\\sigma^4} + \\frac{1}{\\hat\\sigma^6}\\sum_{j=1}^n(y_j - f(x_j,\\hat\\beta))^2 \\\\\n&= \\frac{n}{2\\hat\\sigma^4}\n\\end{aligned}\n\nThen, {\\rm se}(\\hat{\\sigma}^2) = \\hat{\\sigma}^2/\\sqrt{n/2}."
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#statistical-inference-and-diagnostics",
    "href": "docs/regression/map566-lecture-regression-background.html#statistical-inference-and-diagnostics",
    "title": "Linear regression: quick recap",
    "section": "3 Statistical inference and diagnostics",
    "text": "Suppose that residuals (\\varepsilon_j) are independent and normally distributed with mean 0 and variance \\sigma^2:  \n\\varepsilon_j \\sim^{\\mathrm{iid}} \\mathcal{N}(0 \\ , \\ \\sigma^2).\n\n\n3.1 Statistical tests for the model parameters\nIn this case, \\hat{\\beta} is also normally distributed:\n \n\\hat{\\beta} \\sim \\mathcal{N}(\\beta \\ , \\ \\sigma^2 (X^\\prime X)^{-1})\n and, for k=1, 2, \\ldots , p, t_k = \\frac{\\hat{\\beta}_k - \\beta_k}{{\\rm se}(\\hat{\\beta}_k)} follows a t-distribution with n-d degrees of freedom.\nFor each component \\beta_k of \\beta, we can then perform a t-test (known as the Wald test) to test \nH_{k,0} : ``\\beta_k = 0\"  \\quad \\text{versus} \\quad H_{k,1}: ``\\beta_k \\neq 0\" \nIndeed, under the null hypothesis H_{k,0}, t_{{\\rm stat}, k} = {\\hat{\\beta}_k}/{{\\rm se}(\\hat{\\beta}_k)} follows a t-distribution with n-d degrees of freedom.\nThe p-value for this test is therefore\n\np_k= \\mathbb{P}{|t_{n-d}| \\geq |t_{{\\rm stat}, k}^{\\rm obs}| } = 2(1 - \\mathbb{P}{t_{n-d} \\leq |t_{{\\rm stat}, k}^{\\rm obs}| } )\n\n\n\n3.2 Confidence interval for the model parameters\nUsing the fact that t_k follows a t-distribution with n-p degrees of freedom, we can build a confidence interval for \\beta_k of level 1-\\alpha:\n{\\rm CI}_{1-\\alpha}(\\beta_k) = [\\hat{\\beta}_k + qt_{\\alpha/2, n-d}\\ {\\rm se}(\\hat{\\beta}_k) \\ , \\ \\hat{\\beta}_k + qt_{1-\\alpha/2, n-d}\\ {\\rm se}(\\hat{\\beta}_k)] where qt_{\\alpha/2, n-d} and qt_{1-\\alpha/2, n-d} are the quantiles of order \\alpha/2 and 1-\\alpha/2 for a t-distribution with n-p df.\nIndeed, we can easily check that \\mathbb{P}{{\\rm CI}_{1-\\alpha}(\\beta_k) \\ni \\beta_k} = 1-\\alpha.\nThe function confint computes such confidence intervals for \\beta (default level = 95%))\n\n\n3.3 F-test of the overall significance\nA F-test is also performed to test if at least one of the coefficients \\beta_1, \\ldots , \\beta_{p} is non zero:\n\n\\begin{aligned}\nH_0 &:  \\quad (\\beta_1, \\beta_2, \\cdots ,\\beta_p) = (0, 0, \\cdots, 0) \\\\\nH_1 &:  \\quad (\\beta_1, \\beta_2, \\cdots ,\\beta_p) \\neq (0, 0, \\cdots, 0)\n\\end{aligned}\n\nThe test statistic for testing H_0 against H_1 is \n\\begin{aligned}\nF_{\\rm stat} &=  \\frac{\\|X\\hat{\\beta} - \\bar{y}\\|^2/(p-1)}{\\|y - X\\hat{\\beta} \\|^2/(n-p)}\n\\end{aligned}\n\nLet us show that, under the null hypothesis, the test statistic F_{\\rm stat} has a F distribution with (p-1,n-p) degrees of freedom:\nBy construction, \\|y - X\\hat{\\beta} \\|^2/\\sigma^2 has a \\chi^2 distribution with n-d df. On the other hand, under H_0, y_j=\\varepsilon_j \\sim^{\\mathrm{iid}} \\mathcal{N}(0,\\sigma^2) and \\|y - \\bar{y}\\|^2/\\sigma^2 has a \\chi^2 distribution with n-1 df.\nUsing the fact that \n\\|y - \\bar{y}\\|^2 = \\|X\\hat{\\beta} - \\bar{y}\\|^2 + \\|y - X\\hat{\\beta} \\|^2\n We deduce that, under H_0, \\|X\\hat{\\beta} - \\bar{y}\\|^2/\\sigma^2 has a \\chi^2 distribution with (n-1) - (n-p) = p-1 df which leads to the conclusion since (\\chi^2(\\nu_1)/\\nu1)/(\\chi^2(\\nu_2)/\\nu2) = F(\\nu_1,\\nu_2).\nThe p-value of the F-test is therefore \\text{p-value(F-test)} = \\mathbb{P}{F_{p-1,n-p} > F_{\\rm stat}}=1- \\mathbb{P}{F_{p-1,n-p} \\leq F_{\\rm stat}}\nRemark: t-test and F-test are equivalent for linear models with only one predictor. In the case of polynomial regression of degree d = 1, both tests can be used equally for testing if \\beta_1=0. Indeed, \n\\begin{aligned}\nF_{\\rm stat} &=  \\frac{\\|\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y}\\|^2}{\\|y - \\hat{\\beta}_0 - \\hat{\\beta}_1 x\\|^2/(n-2)} \\\\[1.5ex]\n&=  \\frac{\\hat{\\beta}_1^2 \\|x - \\bar{x}\\|^2}{\\hat{\\sigma}^2} = \\frac{\\hat{\\beta}_1^2}{se^2(\\hat{\\beta}_1)} = t_{\\rm stat}^2\n\\end{aligned}\n\nFurthermore, if t_{\\rm stat} has a t distribution with n-2 df, then t_{\\rm stat}^2 has a F distribution with (1,n-2) df. Both p-values are therefore equal.\n\n\n3.4 Coefficient of determination\nThe multiple R-squared R^2 is the proportion of variation in the response variable that has been explained by the model. Using the fact that \n\\|y - \\bar{y}\\|^2 = \\|X\\hat{\\beta} - \\bar{y}\\|^2 + \\|y - X\\hat{\\beta} \\|^2\n\n\nR^2 =  \\frac{\\|X\\hat{\\beta} - \\bar{y}\\|^2}{\\|y - \\bar{y}\\|^2} = 1 - \\frac{\\|y - X\\hat{\\beta} \\|^2}{\\|y - \\bar{y}\\|^2} \n\nBy construction, adding more predictors to the model, i.e. increasing the degree of the polynome, is always going to increase the R-squared value. Adjusted R-squared penalizes this effect by normalizing each term by the associated degree of freedom.\n\n\\begin{aligned}\nR^2_{\\rm adj} &=  1 - \\frac{\\|y - X\\hat{\\beta} \\|^2/(n-p)}{\\|y - \\bar{y}\\|^2/(n-1)} \n\\end{aligned}\n\nThe R-squared is a purely descriptive statistics. The adjusted R-squared should be preferably used to compare the explanatory power of models built from the same dataset.\n\n\n3.5 Some diagnostic plots\nSeveral diagnostic plots are available. The first one plots the estimated residuals y_j - \\hat{f}(x_j) versus the predicted (or fitted) values with a smooth line.\nThe second diagnostic plot is a normal QQ plot. The QQ plot is obtained by plotting the standardized residuals (i.e. the residuals divided by their standard deviation) versus the theoretical quantiles of order 1/(n+1), 2/(n+1), \\ldots, n/(n+1). If the residuals are normally distributed, then the points should be randomly distributed around the line y=x."
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#model-comparison",
    "href": "docs/regression/map566-lecture-regression-background.html#model-comparison",
    "title": "Linear regression: quick recap",
    "section": "4 Model comparison",
    "text": "Again, we assume in this part that the residual errors are independent and identically distributed (i.i.d.), with a normal distribution, mean 0 and variance \\sigma^2.\n\n4.1 ANOVA\nConsider two nested linear models with, respectively, p_1 and p_2 coefficients. Let \\hat{y}_1 and \\hat{y}_2 be the respective predicted values under. Cochran Theorem states that\n \\|y - \\hat{y}_1 \\|^2 = \\|\\hat{y}_2 - \\hat{y}_1\\|^2 + \\|y - \\hat{y}_2 \\|^2 \nThen, the statistics used for the test is\n\nF_{\\rm stat} =  \\frac{\\text{explained variance}}{\\text{unexplained variance}} = \\frac{\\|\\hat{y}_2 - \\hat{y}_1\\|^2/(p_2 - p_1)}{\\|y - \\hat{y}_2 \\|^2/(n-p_2)}\n\nUnder the null, the test statistics F_{\\rm stat} follows a F distribution with (p_2-p_1 , n-p_2) degrees of freedom.\nThis ANOVA test can be performed by means of the anova function.\n\n\n4.2 Likelihood ratio test\nWhen two models are nested, we can compare them by performing a likelihood ratio test (LRT).\nLet \\log\\ell_1 and \\log\\ell_2 be the log-likelihood functions of models \\mathcal{M}_1 and \\mathcal{M}_2. Then, for large n, the distribution of the test statistics \n\\begin{aligned}\nLRT_{\\rm stat} &= 2(\\log\\ell_2(\\hat\\theta_2) - \\log\\ell_1(\\hat\\theta_1) \\\\\n&= n \\log \\left( \\frac{\\sum_{j=1}^n(y_j - f_1(x_j,\\hat{\\beta_1}))^2}{\\sum_{j=1}^n(y_j - f_2(x_j,\\hat{\\beta_2}))^2} \\right)\n\\end{aligned}\n can be approximated by a \\chi^2 distribution with p_2-p_1=d_2-d_1 df.\n\n\n4.3 Deviance\nThe deviance for a given regression model and a given set of observations y, is a measure of goodness of fit defined, in R, as:\n\nD = \\sum_{j=1}^n(y_j - f(x_j,\\hat{\\beta}))^2\n\n\n\n4.4 Information criteria\nFunctions AIC and BIC compute the Akaike information criterion and Bayesian information criterion. AIC and BIC are penalized versions of the log-likelihood defined by:\n\n\\begin{aligned}\nAIC &= -2\\log\\ell(\\hat{\\theta}) + 2P \\\\\nBIC &= -2\\log\\ell(\\hat{\\theta}) + \\log(n)P \n\\end{aligned}\n where P is the number of parameters of the model, i.e. the length of \\theta.\nOn one hand, -2\\log\\ell(\\hat{\\theta}) decreases when P increases. On the other hand, the penalization term (2P or \\log(n)P) increases with P. The objective of these criteria is to propose a model with an optimal compromise between the goodness of fit (measured by the log-likelihood) and the complexity of the model (measured by the number of parameters P).\n\n\n\n4.5 Confidence and prediction intervals\nFor given values x^{\\mathrm{new}} of the explanatory variable, we can use the fitted model for estimating the predicted response f^{\\mathrm{new}}=f(x^{\\mathrm{new}}). This estimation is defined as\n\n\\hat{f^{\\mathrm{new}}} = f(x^{\\mathrm{new}} ,\\hat{\\beta})\n\n\\hat{f^{\\mathrm{new}}} is a random variable since it is a function of the observed y. We can compute a confidence interval for f^{\\mathrm{new}} with function predict(interval = \"confidence\"), since \\hat{f^{\\mathrm{new}}}= x^{\\mathrm{new}} \\hat{\\beta}, \n\\hat{f^{\\mathrm{new}}} \\sim \\mathcal{N} (f^{\\mathrm{new}} , {\\rm Var}(\\hat{f^{\\mathrm{new}}} ) )\n\nwhere \\begin{aligned}\n{\\rm Var}(\\hat{f^{\\mathrm{new}}} ) &=  {x^{\\mathrm{new}}} {\\rm Var}(\\hat{\\beta}) {x^{\\mathrm{new}}}^\\prime \\\\\n&= \\sigma^2 x^{\\mathrm{new}}(X^\\prime X)^{-1}{x^{\\mathrm{new}}}^\\prime\n\\end{aligned}\n\\widehat{{\\rm Var}(\\hat{f^{\\mathrm{new}}})}, an estimate of {\\rm Var}(\\hat{f^{\\mathrm{new}}} ) is obtained using \\hat\\sigma^2 instead of \\sigma^2.\nThen, \n\\begin{aligned}\n\\left({{\\rm Var}(\\hat{f^{\\mathrm{new}}})} \\right)^{-1/2}(\\hat{f^{\\mathrm{new}}} - f^{\\mathrm{new}}) &\\sim \\mathcal{N}(0,  {\\rm Id}_{n^{\\mathrm{new}}} )  \\\\\n\\left(\\widehat{{\\rm Var}(\\hat{f^{\\mathrm{new}}})} \\right)^{-1/2}(\\hat{f^{\\mathrm{new}}} - f^{\\mathrm{new}}) &\\sim t_{n^{\\mathrm{new}},n-p}\n\\end{aligned}\n where t_{n^{\\mathrm{new}},n-p} is the multivariate t distribution with n-p degrees of freedom (the components of this n^{\\mathrm{new}}-vector are independent and follow a t distribution with n-p df).\nConsider now a vector of new measured values y^{\\mathrm{new}}. We can again use the predict(interval = \"prediction\") function for computing a prediction interval for y^{\\mathrm{new}}. By definition of the model, \ny^{\\mathrm{new}} \\sim \\mathcal{N} (f^{\\mathrm{new}} , \\sigma^2 \\, {\\rm Id}_{n^{\\mathrm{new}}} )\n Then, if we want to compute a prediction interval for y^{\\mathrm{new}}, we must take into account the variability of y^{\\mathrm{new}} around f^{\\mathrm{new}}, but also the uncertainty on f^{\\mathrm{new}} since it is unknown:\ny^{\\mathrm{new}} = \\hat{f^{\\mathrm{new}}} + (f^{\\mathrm{new}}-\\hat{f^{\\mathrm{new}}}) + \\varepsilon^{\\mathrm{new}}  Thus, \ny^{\\mathrm{new}} - \\hat{f^{\\mathrm{new}}} \\sim \\mathcal{N}(0, {\\rm Var}(\\hat{f^{\\mathrm{new}}}) + \\sigma^2 \\, {\\rm Id}_{n^{\\mathrm{new}}} )\n Then, \n\\begin{aligned}\n\\left(x^{\\mathrm{new}} {\\rm Var}(\\hat{\\beta}) x^{\\mathrm{new}} + {\\sigma}^2 {\\rm Id}_{n^{\\mathrm{new}}} \\right)^{-1/2}(y^{\\mathrm{new}} - \\hat{f^{\\mathrm{new}}}) & \\sim \\mathcal{N}(0,  {\\rm Id}_{n^{\\mathrm{new}}} ) \\\\\n\\left(x^{\\mathrm{new}} \\widehat{{\\rm Var}(\\hat{\\beta})} x^{\\mathrm{new}} + \\hat{\\sigma}^2 {\\rm Id}_{n^{\\mathrm{new}}} \\right)^{-1/2}(y^{\\mathrm{new}} - \\hat{f^{\\mathrm{new}}}) & \\sim t_{n^{\\mathrm{new}},n-p} \n\\end{aligned}"
  },
  {
    "objectID": "docs/regression/map566-lab-polynomial-regression.html#preliminary",
    "href": "docs/regression/map566-lab-polynomial-regression.html#preliminary",
    "title": "Polynomial regression model: exercices",
    "section": "1 Preliminary",
    "text": "Only functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. You could also try the package broom that standardizes the output of built-in R functions for statistical modelling\n\nlibrary(tidyverse)\nlibrary(ggfortify) # extend some ggplot2 features\nlibrary(broom)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/regression/map566-lab-polynomial-regression.html#introduction",
    "href": "docs/regression/map566-lab-polynomial-regression.html#introduction",
    "title": "Polynomial regression model: exercices",
    "section": "2 Introduction",
    "text": "The file ratWeight.csv consists of rat weights measured over 14 weeks during a subchronic toxicity study related to the question of genetically modified (GM) corn.\nWe will only consider the weight of rat B38625.\n\nrat_weight <- read_csv(\"../../data/ratWeight.csv\") %>% filter(id == 'B38625')\nrat_weight %>% kableExtra::kbl() %>% kableExtra::kable_styling()\n\n\n \n  \n    id \n    week \n    weight \n    regime \n    gender \n    dosage \n  \n \n\n  \n    B38625 \n    1 \n    219.8 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    2 \n    268.0 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    3 \n    320.9 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    4 \n    358.6 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    5 \n    386.4 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    6 \n    418.8 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    7 \n    442.5 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    8 \n    472.5 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    9 \n    484.0 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    10 \n    508.4 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    11 \n    521.7 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    12 \n    520.5 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    13 \n    530.5 \n    Control \n    Male \n    33% \n  \n  \n    B38625 \n    14 \n    530.8 \n    Control \n    Male \n    33% \n  \n\n\n\n\n\nBased on this data, our objective is to build a regression model of the form\ny_j = f(x_j) + e_j \\quad ; \\quad 1 \\leq j \\leq n\nWe will restrict ourselves to polynomial regression, by considering functions of the form\n\n\\begin{aligned}\nf(x) &= f(x ; c_0, c_1, c_2, \\ldots, c_d) \\\\\n&= c_0 + c_1 x + c_2 x^2 + \\ldots + c_d x^d\n\\end{aligned}"
  },
  {
    "objectID": "docs/regression/map566-lab-polynomial-regression.html#questions",
    "href": "docs/regression/map566-lab-polynomial-regression.html#questions",
    "title": "Polynomial regression model: exercices",
    "section": "3 Questions",
    "text": "Plot the data\nFit several polynomials (degree 0, 1, 2, 3, 6, …) to this data using the lm function.\nLook at some diagnostic plots to eliminate miss-specified models.\nCompare the predictive performance of the models that have been retained.\nUse statistical tests and information criteria to compare the models and select “the best one”."
  },
  {
    "objectID": "docs/regression/map566-lab-nonlinear-regression.html#preliminary",
    "href": "docs/regression/map566-lab-nonlinear-regression.html#preliminary",
    "title": "Nonlinear regression: exercices",
    "section": "1 Preliminary",
    "text": "The usual libraries:\n\nlibrary(tidyverse)\nlibrary(ggfortify) # extend some ggplot2 features\nlibrary(broom)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/regression/map566-lab-nonlinear-regression.html#introduction",
    "href": "docs/regression/map566-lab-nonlinear-regression.html#introduction",
    "title": "Nonlinear regression: exercices",
    "section": "2 Introduction",
    "text": "We consider the same data file ratWeight.csv with rat weights measured over 14 weeks during a subchronic toxicity study related to the question of genetically modified (GM) corn."
  },
  {
    "objectID": "docs/regression/map566-lab-nonlinear-regression.html#questions",
    "href": "docs/regression/map566-lab-nonlinear-regression.html#questions",
    "title": "Nonlinear regression: exercices",
    "section": "3 Questions",
    "text": "Load the ratWeight.csv data file and plot the weight of the females of the control group\nSelect the ID B38837 and fit a polynomial model to the growth curve of this female rat.\nFit a Gompertz model f_1(t) = A e^{-b e^{-k\\, t}} to this data.\n\nHint: use for initial values: A = 200, b = 1, k = 0.1.\n\nFit the two following growth models:\n\n\nAsymptotic regression model:\n\nf_2(t)  = A \\left( 1 - b\\, e^{-k\\, t} \\right)\n\nLogistic curve: f_3(t)  = \\frac{A}{1 + e^{-\\gamma( t-\\tau)}}\n\n\nPropose two other parameterizations of the asymptotic regression model which involves\n\n\nthe weight at birth w_0 (when t=0), the limit weight w_\\infty (when t\\to \\infty) and k\nthe weight at birth, the weight at the end of the study w_{14} and the ratio r=(w_{14}-w_{7})/(w_7 - w_0)\n\nCan we compare these models?\n\nWe will now use model f_{2a}. Check that the estimate of \\beta=(w_0, w_\\infty, k) obtained with the nls function is the least squares estimate.\nCheck that this estimate is also the least squares estimate of the linearized model. Then, how are computed the standard errors of \\hat\\beta?\nCompute 90% confidence intervals for the model parameters using several approaches (profile likelihood, linearization, parametric bootstrap)\nCompute a 90% confidence interval for the predicted weight and a 90% prediction interval for the measured weight using the delta method."
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Setup instructions",
    "section": "",
    "text": "R and RStudio are separate downloads and installations\n\nR is the underlying statistical computing environment\nRStudio is a graphical integrated development environment (IDE)\n\n\n0.1 Installing R\nGo to the CRAN webpage, select your OS and follow the instructions.\n\n\n0.2 Installing RStudio Desktop\nGo to the download page. Select, download and install the file corresponding to your OS.\n\n\n0.3 Installing R packages\nLaunch Rstudio and execute the following commands in the console (at least these R packages will be needed during MAP566)\n\ninstall.packages(\"tidyverse\")\n\n\nOn Windows\n\nYou may need Rtools (dedicated page) and git (dedicated page)\n\nOn MacOS\n\nYou may need XCode: visit the dedicated page, download the Mandatory tools and install them on your computer\n\nOn Linux\n\nIf installation of a package fails in Rstudio, just READ THE MESSAGES: you may be asked to install some missing system libraries with, e.g.,\n\nsudo apt-get install lib-missing"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Welcome",
    "section": "Course description",
    "text": "The objective of this course is to show students how statistics is used in practice to answer a specific question, by introducing a series of important model-based approaches.\nThe students will learn to select and use appropriate statistical methodologies and acquire solid and practical skills by working-out examples on real-world data sets from various areas including medicine, genomics, ecology, and others.\nAll analyses will be conducted with the R software, possibly with interfacing to Python. No strong knwoledge neither of R or Python programming is required (only basic scripting).\n\n\n\n\n\n\nImportant remark\n\n\n\nMuch of the material used in this course is due to Marc Lavielle, who was the first to set up the Statistics in Actions course. We only have made some adjustments to it."
  },
  {
    "objectID": "index.html#schedule-tentative",
    "href": "index.html#schedule-tentative",
    "title": "Welcome",
    "section": "Schedule (tentative)",
    "text": "Teachers : Julien Chiquet (lecture + 1 PC), Geneviève Robin (2 PC)\nCourse Evaluation: 2 individual homework assignements + a final exam/project\nCourse Language: French with all material in English\nNumerus closus: 80 students\n\nStatistical tests (x1)\n\nTwo-populations comparison\nPower analysis\nMultiple Testing\n\nRegression models (x2)\n\nLinear and Non Linear Regression models\nNonlinear regression models\nInference Diagnostic, Model comparison\n\nMixed effects models (x2)\n\nLinear mixed effects models\nNonlinear mixed effects models\nEM algorithm\n\nMixture models and model-based clustering (x2)\n\nGaussian mixture models for data clustering\nStochastic Block Models for graph clustering\n(Variational) EM algorithm\n\nModel-based Dimension Reduction (x2)\n\nMultivariate Gaussian model\nProbabilistic Gaussian PCA\nGeneralized mixed effect models"
  }
]