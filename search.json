[
  {
    "objectID": "docs/tests/map566-lab-tests.html#preliminary",
    "href": "docs/tests/map566-lab-tests.html#preliminary",
    "title": "Hypothesis Tesing",
    "section": "Preliminary",
    "text": "Preliminary\nOnly functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation:\n\nlibrary(tidyverse)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#gene-expression",
    "href": "docs/tests/map566-lab-tests.html#gene-expression",
    "title": "Hypothesis Tesing",
    "section": "1 Gene expression",
    "text": "1 Gene expression\nThe dataset geHT.csv consists of gene expression measurements for ten genes under control and treatment conditions, with four replicates each.\n\ngene_data  <- readr::read_csv(\"../../data/geHT.csv\")\ncontrols   <- gene_data %>% dplyr::select(starts_with(\"c\")) %>% unlist()\ntreatments <- gene_data %>% dplyr::select(starts_with(\"t\")) %>% unlist()\n\n\nPlot the data using boxplots to compare control versus treatment\nTest the hypothesis that the mean of the control expression values is 2000.\n\nHint: Use the t.test function to perform the test (> ?t.test for more information about this function)\n\nTest that there is no difference overall between the treatments and controls for any of the genes (test that the whole experiment didn’t work or there are no differentially expressed genes)\n\nHint: Compare the results ignoring/knowing that the data are paired data.\n\nTest if the variances for the gene expression are the same under treatment or control conditions\n\nHint: Perform a F test to compare the two variances using the var.test function (> ?var.test for more information about this function)"
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#smoking-no-smoking",
    "href": "docs/tests/map566-lab-tests.html#smoking-no-smoking",
    "title": "Hypothesis Tesing",
    "section": "2 Smoking, no smoking",
    "text": "2 Smoking, no smoking\n\nThere are 88 smokers among a group of 300 people of a same population. Test that the proportion of smokers in this population is less than or equal to 0.25, greater than or equal to 0.25, equal to 0.25. Show that we can use an exact test, or a test relying on an approximation.\n\nHint: What is the probability distribution of the number X_1 of smokers in this group of n_1=300 people ? Look at the binom.test and prop.test functions.\n\nThere are 90 smokers in another group of 400 people, coming from another population. Can we conclude that the proportion of smokers are different in these two populations?\n\nHint: look at the fisher.test and prop.test functions.\n\n## help in building thecontingency table\nsmokers     <- c(88, 90)\nnon.smokers <- c(300-88, 400-90)\nsmoke <- matrix(c(smokers,non.smokers), nrow = 2)\nsmoke\n\n     [,1] [,2]\n[1,]   88  212\n[2,]   90  310"
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#efficacy-of-the-bnt162b2-mrna-covid-19-vaccine",
    "href": "docs/tests/map566-lab-tests.html#efficacy-of-the-bnt162b2-mrna-covid-19-vaccine",
    "title": "Hypothesis Tesing",
    "section": "3 Efficacy of the BNT162b2 mRNA Covid-19 Vaccine",
    "text": "3 Efficacy of the BNT162b2 mRNA Covid-19 Vaccine\nIn an ongoing multinational, placebo-controlled, observer-blinded, pivotal efficacy trial, a total of 43,548 participants underwent randomization, of whom 43,448 received injections: 21,720 with BNT162b2 and 21,728 with placebo. There were 8 cases of Covid-19 with onset at least 7 days after the second dose among participants assigned to receive BNT162b2 and 162 cases among those assigned to placebo.\nBased on this result, Pfizer concludes that BNT162b2 was 95% effective in preventing Covid-19.\n\nHow was this value obtained?\n\nHint: The idea is to compare the observed number of BNT162b2 participants who were really infected with the expected number of BNT162b2 participants who would have been infected if the vaccine was not more effective than the placebo.\nLet p_0 and p_1 be the probabilities to be infected in the placebo and BNT162b2 groups. How can p_0 and p_1 be estimated? How can the expected number of BNT162b2 participants be estimated under the null hypothesis (p_0=p_1)?\n\nDerive a 95% confidence interval for the vaccine efficacy VE using a normal approximation for \\log(\\hat{p}_1/\\hat{p}_0) where \\hat{p}_0 and \\hat{p}_1 are the empirical proportions of cases of Covid-19 in the two groups,\n\nHint: What are the asymptotic distributions of \\hat{p}_0 and \\hat{p}_1? Using the delta-method, compute the asymptotic distribution for \\log(\\hat{p}_0) and \\log(\\hat{p}_1) and \\log(\\hat{p}_1/\\hat{p}_0). Derive a confidence interval for \\log(p_1/p0) and then for VE.\n\nDerive a 95% confidence interval for the vaccine efficacy approximating a confidence interval for {p}_1/{p}_0 by a prediction interval for \\hat{p}_1/\\hat{p}_0 and using a Monte-Carlo simulation for estimating this prediction interval.\n\nHint:\n\nK <- 1000000\nx0 <- rbinom(K, n0, hat.p0)\nx1 <- rbinom(K, n1, hat.p1)\n## just complete to estimate the target quantities"
  },
  {
    "objectID": "docs/tests/map566-lab-tests.html#identification-of-genes",
    "href": "docs/tests/map566-lab-tests.html#identification-of-genes",
    "title": "Hypothesis Tesing",
    "section": "4 Identification of genes",
    "text": "4 Identification of genes\nBreast cancer is the most common malignant disease in Western women. In these patients, it is not the primary tumour, but its metastases at distant sites that are the main cause of death.\nPrognostic markers are needed to identify patients who are at the highest risk for developing metastases, which might enable oncologists to begin tailoring treatment strategies to individual patients. Gene-expression signatures of primary breast tumours might be one way to identify the patients who are most likely to develop metastatic cancer.\nThe datafile geneMFS.csv contains the expression level of 11 genes and the metastasis-free survival (the period until metastasis is detected) for 527 patients.\n\ncancer <- read_csv(\"../../data/geneMFS.csv\") \n\nThe objective of this study is to identify which genes may be good or poor prognosis for the development of metastasis.\n\nGraphically compare the distribution of the gene expressions in the groups of patients with early metastasis (MFS <1000) and late metastasis (MFS>1000).\n\nHint: Consider using scale_y_log10()\n\nCompare the gene expression levels in these two groups using a parametric test.\n\nHint: Perform a t-test for comparing the log-expression of each gene. Use the p.adjust function to apply the Bonferroni correction and the Benjamini-Hochberg correction to these p-values.\n\nCompare these results with those obtained using a non parametric test.\n\nHint: Perform Wilcoxon rank sum tests (using the wilcox.test function) instead of t tests."
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#introduction",
    "href": "docs/tests/map566-lecture-multiple.html#introduction",
    "title": "Multiple Testing",
    "section": "Introduction",
    "text": "Introduction\n\nlibrary(tidyverse)\nlibrary(gridExtra)\ntheme_set(theme_bw())\n\nWhen we perform a large number of statistical tests, some will have p-values less than 0.05 purely by chance, even if all the null hypotheses are really true.\nMore precisely, if we do a large number m of statistical tests, and for m_{0\\cdot} of them the null hypothesis is actually true, we would expect about 5% of the m_{0\\cdot} tests to be significant at the 0.05 level, just due to chance: these significant results are false discoveries (or false positives). On the other hand, if some alternative hypothesis are true, we can miss some of them: these non significant results are false negatives.\n \\begin{array}{c|c|c|c}\n& \\text{Null hypothesis true} & \\text{Alternative hypothesis true}  & \\\\ \\hline\n\\text{Test non significant} & m_{00} & m_{10} & m_{\\cdot 0} \\\\ \\hline\n\\text{Test significant} & m_{01} & m_{11} & m_{\\cdot 1} \\\\ \\hline\n& m_{0 \\cdot} & m_{1 \\cdot} & m\n\\end{array} \nIf important conclusions and decisions are based on these false positives, it is then important to control the family-wise error rate (FWER):\n\nthe family-wise error rate is the probability of making one or more false discoveries, or type I errors when performing multiple hypotheses tests\n\n FWER = \\mathbb{P}(m_{01}\\geq 1) \nWhen true positives are expected, it is possible to miss some of them. We then necessarily need to accept false positives if we want to limit the number of these false negatives. It is important in such situation to control the false discovery rate (FDR)\n\nThe false discovery rate (FDR) is the expected proportion of false discoveries among the discoveries\n\nFDR = \\mathbb{E}\\left(\\frac{m_{01}}{m_{01} + m_{11}}\\right) = \\mathbb{E}\\left(\\frac{m_{01}}{m_{\\cdot1} }\\right) Several procedures exist for controlling either the FWER or the FDR."
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#distribution-of-the-p-values",
    "href": "docs/tests/map566-lecture-multiple.html#distribution-of-the-p-values",
    "title": "Multiple Testing",
    "section": "1 Distribution of the p-values",
    "text": "1 Distribution of the p-values\n\n1.1 Introduction\nThe health effects of a Roundup-tolerant genetically modified maize, cultivated with or without Roundup, and Roundup alone, were studied during a 2 years study in rats.\nFor each sex, one control group had access to plain water and standard diet from the closest isogenic non-transgenic maize control; six groups were fed with 11, 22 and 33% of GM NK603 maize either treated or not with Roundup. The final three groups were fed with the control diet and had access to water supplemented with different concentrations of Roundup.\nA sample of 200 rats including 100 males and 100 females was randomized into 20 groups of 10 rats of the same sex. Within each group, rats received the same diet. For each sex, there are therefore nine experimental groups and one control group.\nThe file ratSurvival.csv reports the lifespan (in days) for each animal. Here, the experiment stopped after 720 days. Then, the reported survival time is 720 for those animals who were still alive at the end of the experiment.\nSee the opinion of the Haut Conseil des Biotechnologies for more information about this study.\nHere is a summary of the data,\n\nsurvival <- read_csv(\"../../data/ratSurvival.csv\") %>% mutate_if(is.character, factor) \nsurvival %>% rmarkdown::paged_table()\n\n\n  \n\n\nlevels(survival$regimen)\n\n [1] \"control\"     \"NK603-11%\"   \"NK603-11%+R\" \"NK603-22%\"   \"NK603-22%+R\"\n [6] \"NK603-33%\"   \"NK603-33%+R\" \"RoundUp A\"   \"RoundUp B\"   \"RoundUp C\"  \n\n\n\n\n1.2 Single comparison between 2 groups\nOne objective of this study is the comparison of the survival time between the control group and the experimental groups.\nConsider for instance the control group of females\n\ntime_control <- survival %>% \n  filter(regimen == \"control\", gender == \"female\") %>% pull(time)\ntime_control\n\n [1] 540 645 720 720 720 720 720 720 720 720\n\n\nOnly 2 rats of this group died before the end of the experiment. On the other hand, 7 females of the group fed with 22% of maize NK693 died during the experiment.\n\ntime_test <- survival %>% \n  filter(regimen == \"NK603-22%\", gender == \"female\") %>% pull(time)\ntime_test\n\n [1] 290 475 480 510 550 555 650 720 720 720\n\n\nA negative effect of the diet on the survival means that the rats of the experimental group tend to die before those of the control group. Then, we would like to test\n\\begin{aligned}\n& H_0: \\ \"\\text{the NK603 22\\% diet has no effect on the survival of female rats }\"\\\\\n\\text{versus } & H_1: \\ \"\\text{the NK603 22\\% diet leads to decreased survival time for female rats}\"\\\\ \\end{aligned}\nIn terms of survival functions, that means that, under H_1, the probability to be alive at a given time t is lower for a rat of the experimental group than for a rat of the control group. We then would like to test\n\\begin{aligned}\n& H_0: \\ ``\\mathbb{P}(T_{\\rm test}>t) = \\mathbb{P}(T_{\\rm control}>t), \\text{ for any } t>0\" \\\\\n\\text{versus } & H_1: \\ ``\\mathbb{P}(T_{\\rm test}>t) < \\mathbb{P}(T_{\\rm control}>t), \\text{ for any } t>0\" \\end{aligned}\nBecause of the (right) censoring process, we cannot just compare the mean survival times using a t-test. On the other hand, we can use the Wilcoxon-Mann-Whitney test which precisely aims to compare the ranks of the survival times in both groups.\n\nwilcox.test(time_test, time_control, alternative=\"less\")\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  time_test and time_control\nW = 22, p-value = 0.01144\nalternative hypothesis: true location shift is less than 0\n\n\nHere, the p-value should lead us to reject the null hypothesis and conclude that 22% of the GM maize in the diet has a negative effect on the survival."
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#a-single-comparison-among-many-others",
    "href": "docs/tests/map566-lecture-multiple.html#a-single-comparison-among-many-others",
    "title": "Multiple Testing",
    "section": "2 A single comparison… among many others",
    "text": "2 A single comparison… among many others\nShould we really accept this conclusion as it stands? No, because we don’t know the whole story… Remember that there are 9 experimental groups for each sex. Then, 18 comparisons with the control groups are performed.\n\ndo_all_comparisons <- function(data) {\n  map(levels(data$gender), \n    function(g) {\n    control <- filter(data, regimen == \"control\" & gender==g) %>% pull(time)\n    regimes <- setdiff(levels(data$regimen), \"control\")\n    map(regimes, function(regime) {\n      test <- filter(data, gender == g & regimen == regime) %>% pull(time)\n      wt  <- wilcox.test(test, control, alternative = \"less\")\n      data.frame(\n        gender    = g,\n        regime    = regime,\n        statistic = unname(wt$statistic),\n        p.value   = wt$p.value\n      )\n    }) %>% bind_rows()\n  }) %>% bind_rows() %>% arrange(p.value)\n}\nall_comparisons <- do_all_comparisons(survival)\nall_comparisons %>% rmarkdown::paged_table(options = list(rows.print = 18))\n\n\n  \n\n\n\nLet us plot the ordered p-values:\n\n\nShow the code\nall_comparisons %>% \n  ggplot() + geom_point(aes(x = 1:18, color = regime, y = p.value, shape = gender), size=4) + \n  scale_y_log10() + xlab(\"regime\") + ylab(\"p-value\") + scale_color_viridis_d() + theme_bw()\n\n\n\n\n\nIf we then decide to only report the largest observed differences, associated to the smallest p-values, how can we conclude that these differences are statistically significant?\n\n2.1 Permutation test\n\n\n\n\n\n\nPermutation test\n\n\n\nA permutation test (also called a randomization test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points. If the labels are exchangeable under the null hypothesis, then the resulting tests yield exact significance levels. Prediction intervals can also be derived.\n\n\nIn our example, imagine that the null hypothesis is true. We can then randomly exchange the labels (i.e. the regimen) and perform the 18 comparisons between the experimental groups and the control groups.\n\npermuted_data <- survival %>% \n  split(.$gender) %>% \n  map(mutate, regimen = sample(regimen)) %>% \n  bind_rows()\ndo_all_comparisons(permuted_data) %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nThe test statistics and the p-values now really behave how they are supposed to behave under the null hypothesis. Let us plot this:\n\n\nShow the code\ndo_all_comparisons(permuted_data) %>% \n  ggplot() + geom_point(aes(x = 1:18, color = regime, y = p.value, shape = gender), size=4) + \n  scale_y_log10() + xlab(\"regime\") + ylab(\"p-value\") + scale_color_viridis_d() + theme_bw()\n\n\n\n\n\nIs this really different from the plot we obtained before, with the original data? To answer this question we repeat the same experiment using many different permutations. We will be able to estimate the m distributions of the m test statistics as well as the m distributions of the m p-values under the null hypothesis.\n\nn_replicates <- 1000\nres <- parallel::mclapply(1:n_replicates, function(i) {\n  \n  permuted_data <- survival %>% \n    split(.$gender) %>% \n    map(mutate, regimen = sample(regimen)) %>% \n    bind_rows()\n\n  do_all_comparisons(permuted_data) %>% \n    select(statistic, p.value) %>% mutate(simu = i)\n}, mc.cores = parallel::detectCores()) %>% bind_rows()\n\nWe can estimate, for instance, prediction intervals of level 90% for the m=18 ordered p-values\n\nquantiles_pvalues <- \n  split(res, res$simu) %>% \n  map(pull, p.value) %>% \n  map(sort) %>% bind_rows() %>% \n  apply(1, quantile, c(0.05, 0.5, 0.95)) %>% t() %>% as_tibble() %>%\n  setNames(c(\"low\",\"median\",\"up\")) %>% mutate(rank = 1:n())\n\nand plot them, with the original p-values\n\n\nShow the code\nquantiles_pvalues %>% ggplot() +\n  geom_errorbar(aes(x = rank, ymin = low, ymax = up), width=0.2, size=1.5, colour=\"grey50\") +\n  scale_y_log10() + xlab(\"regimen\") + ylab(\"p-value\") + scale_x_continuous(breaks=NULL)  +  \n  geom_point(data = all_comparisons, aes(x = 1:18, color = regime, y = p.value, shape=gender), size=4) \n\n\n\n\n\nHere, all the p-values, including the smallest ones, belong to the 90% prediction intervals: all the observed p-values behave individually how they are expected to behave under the null hypothesis.\nIn particular, when 18 comparisons are performed, it’s not unlikely under the null hypothesis to obtain a smallest p-value less than or equal to the observed one (0.011).\nThe probability of such event can easily be estimated by Monte Carlo simulation. Let p_{(1),\\ell} be the smallest p-value obtained from the \\ell-th replicate of the Monte Carlo. Then,\n\n\\mathbb{P}\\left(p_{(1)} \\leq p_{(1)}^{\\mathrm{obs}}\\right) \\approx \\frac{1}{L} \\sum_{\\ell=1}^{L} \\mathbf{1}_{\\left\\{p_{(1),\\ell} \\leq p_{(1)}^{\\mathrm{obs}}\\right\\}}\n\n\nstat_rank1 <- split(res, res$simu) %>% map_dbl(function(x) sort(x$statistic)[1])\nmean(stat_rank1 < all_comparisons$statistic[1])\n\n[1] 0.153"
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#controlling-the-family-wise-error-rate",
    "href": "docs/tests/map566-lecture-multiple.html#controlling-the-family-wise-error-rate",
    "title": "Multiple Testing",
    "section": "3 Controlling the Family Wise Error Rate",
    "text": "3 Controlling the Family Wise Error Rate\n\n3.1 The Bonferroni correction\nImagine that we perform m comparisons and that all the m null hypotheses are true, i.e. m=m_{0\\cdot}.. If we use the same significance level \\alpha_m for the m tests, how should we choose \\alpha_m in order to control the family-wise error rate (FWER)?\n \\begin{aligned}\n{\\rm FWER} &= \\mathbb{P}(m_{01}\\geq 1) \\\\\n&= 1 - \\mathbb{P}(m_{01}= 0)  \\\\\n&= 1 - (1-\\alpha_m)^m\n\\end{aligned} \nThen, if we set FWER=\\alpha, the significance level for each individual test should be \\begin{aligned}\n\\alpha_m &=  1 - (1-\\alpha)^{\\frac{1}{m}} \\quad \\quad (\\text{Sidak correction})\\\\\n& \\simeq  \\frac{\\alpha}{m} \\quad \\quad (\\text{Bonferroni correction})\n\\end{aligned}\nLet p_k be the p-value of the k-th test. Using the Bonferroni correction, the k-th test is significant if   p_k \\leq \\alpha_m \\quad  \\Longleftrightarrow \\quad   m \\, p_k \\leq \\alpha  We can then either compare the original p-value p_k to the corrected significance level \\alpha/m, or compare the adjusted p-value p_k^{\\rm (bonferroni)} =\\min(1, m \\, p_k) to the critical value \\alpha.\n\nm <- nrow(all_comparisons)\nall_comparisons$p.value_bonferonni <- pmin(1, all_comparisons$p.value * m)\nall_comparisons %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nUsing the Bonferroni correction, none of the 18 comparisons is significant.\nRemark: the function p.adjust proposes several adjustements of the p-values for multiple comparisons, including the Bonferroni adjustment:\n\np.adjust(all_comparisons$p.value, method = \"bonferroni\")\n\n [1] 0.2058803 0.3837141 0.5121818 1.0000000 1.0000000 1.0000000 1.0000000\n [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n[15] 1.0000000 1.0000000 1.0000000 1.0000000\n\n\nThe Bonferroni correction is appropriate when a single false positive in a set of tests would be a problem. It is mainly useful when there are a fairly small number of multiple comparisons and very few of them might be significant. The main drawback of the Bonferroni correction is its lack of power: it may lead to a very high rate of false negatives."
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#controlling-the-false-discovery-rate",
    "href": "docs/tests/map566-lecture-multiple.html#controlling-the-false-discovery-rate",
    "title": "Multiple Testing",
    "section": "4 Controlling the False Discovery Rate",
    "text": "4 Controlling the False Discovery Rate\n\n4.1 Detecting associations\n(Example from Handbook of Biological Statistics)\nGarcı́a-Arenzana et al. (2014) tested associations of 25 dietary variables with mammographic density, an important risk factor for breast cancer, in Spanish women. They found the following results\n\ndata <- read_csv(\"../../data/dietary.csv\") %>% mutate_if(is.character, factor) \ndata %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nWe can see that five of the variables show a significant p-value (<0.05). However, because Garcı́a-Arenzana et al. (2014) tested 25 dietary variables, we would expect one or two variables to show a significant result purely by chance, even if diet had no real effect on mammographic density.\nApplying the Bonferroni correction, we divide \\alpha=0.05 by the number of tests (m=25) to get the Bonferroni critical value, so a test would have to have p<0.002 to be significant. Under that criterion, only the test for total calories is significant.\n\n\n4.2 The Benjamini-Hochberg procedure\n\n\n\n\n\n\nControlling FDR\n\n\n\nAn alternative approach is to control the false discovery rate, i.e the expected proportion of “discoveries” (significant results) that are actually false positives. FDR control offers a way to increase power while maintaining some principled bound on error.\nImagine for instance that we compare expression levels for 20,000 genes between liver tumors and normal liver cells. We are going to do additional experiments on any genes that show a significant difference between the normal and tumor cells. Then, because we don’t want to miss genes of interest, we are willing to accept up to 25% of the genes with significant results being false positives. We’ll find out they’re false positives when we do the followup experiments. In this case, we would set the false discovery rate to 25%.\n\n\nThe Benjamini and Hochberg (1995) procedure (BH) controls the FDR… and it is simple to use!\nIndeed, for a given \\alpha and a given sequence of ordered p-values P_{(1)}, P_{(2)}, , P_{(m)}, it consists in computing the m adjusted p-values defined as\nP_{(i)}^{\\rm BH} = \\min\\left( P_{(i)}\\frac{m}{i} \\ , \\ P_{(i+1)}^{\\rm BH} \\right)\n\ndata$p.bh <- p.adjust(data$`p-value`, method = \"BH\")\ndata %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nThen, the discoveries, i.e. the significant tests, are those with an adjusted p-value less than \\alpha.\nIt can be shown that this procedure guarantees that for independent tests, and for any alternative hypothesis,\n\\begin{aligned}\n{\\rm FDR} &= \\mathbb{E}\\left(\\frac{m_{01}}{m_{01} + m_{11}}\\right) \\\\\n&\\leq \\frac{m_{0\\cdot}}{m} \\alpha \\\\\n&\\leq \\alpha\n\\end{aligned}\nwhere m_{0\\cdot} is the (unknown) total number of true null hypotheses, and where the first inequality is an equality with continuous p-value distributions.\nIn our example, the first five tests would be significant with \\alpha=0.25, which means that we expect no more than 25% of these 5 tests to be false discoveries.\nRemark 1: The BH procedure is equivalent to consider as significant the non adjusted p-values smaller than a threshold P_{\\rm BH} defined as\n P_{\\rm BH} = \\max_i \\left\\{ P_{(i)}: \\ \\ P_{(i)} \\leq \\alpha \\frac{i}{m}  \\right\\} \nIn other words, the largest p-value that has P_{(i)}<(i/m)\\alpha is significant, and all of the p-values smaller than it are also significant, even the ones that aren’t less than their Benjamini-Hochberg critical value \\alpha \\times i/m\n\nalpha <- 0.25\nm <- nrow(data)\ndata$critical.value <- (1:m)/m*alpha\ndata\n\n# A tibble: 25 x 4\n   dietary           `p-value`  p.bh critical.value\n   <fct>                 <dbl> <dbl>          <dbl>\n 1 Total calories        0.001 0.025           0.01\n 2 Olive oil             0.008 0.1             0.02\n 3 Whole milk            0.039 0.21            0.03\n 4 White meat            0.041 0.21            0.04\n 5 Proteins              0.042 0.21            0.05\n 6 Nuts                  0.061 0.254           0.06\n 7 Cereals and pasta     0.074 0.264           0.07\n 8 White fish            0.205 0.491           0.08\n 9 Butter                0.212 0.491           0.09\n10 Vegetables            0.216 0.491           0.1 \n# … with 15 more rows\n\n\nIf we plot this, we clearly exhibit two regimen in the behaviour of the p-values distribution:\n\n\nShow the code\npl1 <- ggplot(data) + geom_line(aes(x=1:m,y=`p-value`), colour=\"blue\") +\ngeom_line(aes(x=1:m,y=critical.value), colour=\"red\") + xlab(\"(i)\") + theme_bw()\ngrid.arrange(pl1, pl1 + xlim(c(1,8)) + ylim(c(0,0.21)) + geom_vline(xintercept=5.5))\n\n\n\n\n\nThe largest p-value with P_{(i)}<(i/m)\\alpha is proteins, where the individual p-value (0.042) is less than the (i/m)\\alpha value of 0.050. Thus the first five tests would be significant.\nRemark 2: The FDR is not bounded by \\alpha, but by (m_{0\\cdot}/m) \\alpha. We could increase the global power of the tests and get a FDR equal to the desired level \\alpha, either by defining the critical values as (i/m_{0\\cdot})\\alpha, or by multiplying the adjusted p-values by m_{0\\cdot}/m.\nUnfortunately, m_{0\\cdot} is unknown… but it can be estimated, as the number of non significant tests for instance.\n\nm0.est <- sum(data$p.bh>alpha)\ndata$crit.valc <- round(data$critical.value*m/m0.est,4)\ndata$p.bhc <- round(data$p.bh*m0.est/m,4)\nhead(data,10)\n\n# A tibble: 10 x 6\n   dietary           `p-value`  p.bh critical.value crit.valc p.bhc\n   <fct>                 <dbl> <dbl>          <dbl>     <dbl> <dbl>\n 1 Total calories        0.001 0.025           0.01    0.0125 0.02 \n 2 Olive oil             0.008 0.1             0.02    0.025  0.08 \n 3 Whole milk            0.039 0.21            0.03    0.0375 0.168\n 4 White meat            0.041 0.21            0.04    0.05   0.168\n 5 Proteins              0.042 0.21            0.05    0.0625 0.168\n 6 Nuts                  0.061 0.254           0.06    0.075  0.203\n 7 Cereals and pasta     0.074 0.264           0.07    0.0875 0.211\n 8 White fish            0.205 0.491           0.08    0.1    0.393\n 9 Butter                0.212 0.491           0.09    0.112  0.393\n10 Vegetables            0.216 0.491           0.1     0.125  0.393\n\n\nWe would consider the 7 first p-values as significant using this new correction."
  },
  {
    "objectID": "docs/tests/map566-lecture-multiple.html#references",
    "href": "docs/tests/map566-lecture-multiple.html#references",
    "title": "Multiple Testing",
    "section": "5 References",
    "text": "5 References\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nGarcı́a-Arenzana, Nicolás, Eva Marı́a Navarrete-Muñoz, Virginia Lope, Pilar Moreo, Carmen Vidal, Soledad Laso-Pablos, Nieves Ascunce, et al. 2014. “Calorie Intake, Olive Oil Consumption and Mammographic Density Among Spanish Women.” International Journal of Cancer 134 (8): 1916–25. https://breast-cancer-research.biomedcentral.com/articles/10.1186/bcr2102."
  },
  {
    "objectID": "docs/tests/map566-lecture-single.html#one-sample-t-test",
    "href": "docs/tests/map566-lecture-single.html#one-sample-t-test",
    "title": "Statistical Tests",
    "section": "2.1 One sample t-test",
    "text": "2.1 One sample t-test\nBefore considering the problem of comparing two groups, let us start looking at the weight of the male rats only:\n\n\nShow the code\nrat_weight %>% filter(gender == \"Male\") %>% \n  ggplot() + aes(x = regime, y = weight) +\n  geom_violin(aes(fill = regime)) + geom_jitter(alpha = 0.5) +  ylab(\"weight (g)\")\n\n\n\n\n\nLet x_1, x_2, x_n the weights of the n male rats. We will assume that the x_i’s are independent and normally distributed with mean \\mu and variance \\sigma^2:\n x_i \\sim^{\\mathrm{iid}} \\mathcal{N}(\\mu \\ , \\ \\sigma^2)\n\n2.1.1 One sided test\nWe want to test\nH_0: \\ ``\\mu \\leq \\mu_0\" \\quad \\text{versus} \\quad H_1: \\ ``\\mu > \\mu_0\" \nFunction t.test can be used for performing this test:\n\nx <- rat_weight %>% filter(gender == \"Male\") %>% pull(\"weight\")\nmu0 <- 500\nt.test(x, alternative=\"greater\", mu=mu0)\n\n\n    One Sample t-test\n\ndata:  x\nt = 1.2708, df = 77, p-value = 0.1038\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 498.0706      Inf\nsample estimates:\nmean of x \n 506.2218 \n\n\nLet us see what these outputs are and how they are computed.\nLet \\bar{x} = n^{-1}\\sum_{i=1}^n x_i be the empirical mean of the data.  \\bar{x} \\sim \\mathcal{N}(\\mu \\ , \\ \\frac{\\sigma^2}{n}) Then,  \\begin{aligned}\n\\frac{\\sqrt{n}(\\bar{x} - \\mu)}{\\sigma} \\ &   \\sim \\ \\mathcal{N}(0 \\ , \\ 1) \\\\\n\\frac{\\sqrt{n}(\\bar{x} - \\mu)}{s} \\ &   \\sim  \\ t_{n-1}\n\\end{aligned} \nwhere s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2  is the empirical variance of the x_i’s.\nThe statistic used for the test should be a function of the data whose distribution under H_0 is known, and whose expected behavior under H_1 allows one to define a rejection region (or critical region) for the null hypothesis.\nHere, the test statistic is \nT_{\\rm stat} = \\frac{(\\bar{x} - \\mu_0)}{s/\\sqrt{n}}   which follows a t-distribution with n-1 degrees of freedom when \\mu=\\mu_0.\n\\bar{x} is expected to be less than or equal to \\mu_0 under the null hypothesis, and greater than \\mu_0 under the alternative hypothesis, Hence, T_{\\rm stat} is expected to be less than or equal to 0 under H_0 and greater than 0 under H_1. We then reject the null hypothesis H_0 if T_{\\rm stat} is greater than some threshold q.\nSuch decision rule may lead to two kinds of error:\n\nThe type I error is the incorrect rejection of null hypothesis when it is true,\nThe type II error is the failure to reject the null hypothesis when it is false.\n\nThe type I error rate or significance level is therefore the probability of rejecting the null hypothesis given that it is true.\nIn our case, for a given significance level \\alpha, we will reject H_0 if T_{\\rm stat} > qt_{1-\\alpha,n-1}, where qt_{1-\\alpha,n-1} is the quantile of order 1-\\alpha for a t-distribution with n-1 degrees of freedom.\nIndeed, by definition,\n \\begin{aligned}\n\\mathbb{P}(\\text{reject } H_0 \\ | \\ H_0 \\ \\text{true}) &= \\mathbb{P}(T_{\\rm stat} > qt_{1-\\alpha,n-1} \\ | \\ \\mu \\leq \\mu_0) \\\\\n& \\leq \\mathbb{P}(T_{\\rm stat} > qt_{1-\\alpha,n-1} \\ | \\ \\mu = \\mu_0) \\\\\n& \\leq \\mathbb{P}(t_{n-1} > qt_{1-\\alpha,n-1}) \\\\\n& \\leq \\alpha\n\\end{aligned} \n\nalpha <- 0.05\nx_mean <- mean(x)\nx_sd <- sd(x)\nn <- length(x)\ndf <- n - 1\nt.stat <- sqrt(n)*(x_mean-mu0)/x_sd\nc(t.stat,qt(1-alpha, df))\n\n[1] 1.270806 1.664885\n\n\nWe therefore don’t reject H_0 in our example since T_{\\rm stat} < qt_{1-\\alpha,n-1}.\nWe can equivalently compute the significance level for which the test becomes significant. This value is called the p-value: \\begin{aligned}\np_{\\rm value} & = \\max\\mathbb{P}_{H_0}(T_{\\rm stat} > T_{\\rm stat}^{\\rm obs}) \\\\\n& = \\mathbb{P}(T_{\\rm stat} > T_{\\rm stat}^{\\rm obs}  \\ | \\ \\mu=\\mu_0) \\\\\n&= 1 - \\mathbb{P}(t_{n-1} \\leq T_{\\rm stat}^{\\rm obs})\n\\end{aligned}\nNow, T_{\\rm stat} > qt_{1-\\alpha,n-1} under H_0 if and only if \\mathbb{P}(t_{n-1} \\leq T_{\\rm stat}^{\\rm obs}) \\geq 1-\\alpha. Then, the test is significant at the level \\alpha if and only if p_{\\rm value}\\leq \\alpha.\n\np.value <- 1 - pt(t.stat,df) \nprint(p.value)\n\n[1] 0.1038119\n\n\n\n\n\nHere, we would reject H_0 for any significance level \\alpha \\geq 0.104.\nImportant: The fact that the test is not significant at the level \\alpha does not allow us to conclude that H_0 is true, i.e. that \\mu is less than or equal to 500. We can only say that the data does not allow us to conclude that \\mu>500.\nImagine now that we want to test if \\mu \\geq 515 for instance. The alternative here is H_1: \\ ``\\mu < 515\".\n\nmu0 <- 515\nt.test(x, alternative = \"less\", mu = mu0)\n\n\n    One Sample t-test\n\ndata:  x\nt = -1.793, df = 77, p-value = 0.03845\nalternative hypothesis: true mean is less than 515\n95 percent confidence interval:\n    -Inf 514.373\nsample estimates:\nmean of x \n 506.2218 \n\n\nMore generally, we may want to test H_0: \\ ``\\mu \\geq \\mu_0\" \\quad \\text{versus} \\quad H_1: \\ ``\\mu < \\mu_0\"  We still use the statistic T_{\\rm stat} = \\sqrt{n}(\\bar{x}-\\mu_0)/s for this test, but the rejection region is now the area that lies to the left of the critical value qt_{\\alpha,n-1} since\n \\begin{aligned}\n\\mathbb{P}(\\text{reject } H_0 \\ | \\ H_0 \\ \\text{true}) &= \\mathbb{P}(T_{\\rm stat} < qt_{\\alpha,n-1} \\ | \\ \\mu \\geq \\mu_0) \\\\\n& \\leq \\mathbb{P}(T_{\\rm stat} < qt_{\\alpha,n-1} \\ | \\ \\mu = \\mu_0) \\\\\n& \\leq \\alpha\n\\end{aligned} \n\nt.stat <- sqrt(n)*(x_mean-mu0)/x_sd\np.value <- pt(t.stat,df)\nc(t.stat, df, p.value)\n\n[1] -1.79295428 77.00000000  0.03845364\n\n\nHere, the p-value is less than \\alpha=0.05: we then reject the null hypothesis at the 5\\% level and conclude that \\mu < 515.\n\n\n2.1.2 Two sided test\nA two sided test (or two tailed test) can be used to test if \\mu=500 for instance\n\nmu0 = 500\nt.test(x, alternative = \"two.sided\", mu  =mu0)\n\n\n    One Sample t-test\n\ndata:  x\nt = 1.2708, df = 77, p-value = 0.2076\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 496.4727 515.9709\nsample estimates:\nmean of x \n 506.2218 \n\n\nMore generally, we can test H_0: \\ ``\\mu = \\mu_0\" \\quad \\text{versus} \\quad H_1: \\ ``\\mu \\neq \\mu_0\"  The test also uses the statistic T_{\\rm stat} = \\sqrt{n}(\\bar{x}-\\mu_0)/s, but the rejection region has now two parts: we reject H_0 if |T_{\\rm stat}| > qt_{1-\\alpha/2}. Indeed,\n \\begin{aligned}\n\\mathbb{P}(\\text{reject } H_0 \\ | \\ H_0 \\ \\text{true}) &= \\mathbb{P}(|T_{\\rm stat}| > qt_{1 -\\frac{\\alpha}{2},n-1} \\ | \\ \\mu = \\mu_0) \\\\\n& = \\mathbb{P}(T_{\\rm stat} < qt_{\\frac{\\alpha}{2},n-1} \\ | \\ \\mu = \\mu_0) +\n\\mathbb{P}(T_{\\rm stat} > qt_{1-\\frac{\\alpha}{2},n-1} \\ | \\ \\mu = \\mu_0)\\\\\n&= \\mathbb{P}(t_{n-1} \\leq qt_{\\frac{\\alpha}{2},n-1}) +  \\mathbb{P}(t_{n-1} \\geq qt_{1-\\frac{\\alpha}{2},n-1}) \\\\\n&= \\frac{\\alpha}{2} + \\frac{\\alpha}{2} \\\\\n& = \\alpha\n\\end{aligned} \nThe p-value of the test is now \\begin{aligned}\np_{\\rm value} & = \\mathbb{P}_{H_0}(|T_{\\rm stat}| > |T_{\\rm stat}^{\\rm obs}|) \\\\\n& = \\mathbb{P}_{H_0}(T_{\\rm stat} < -|T_{\\rm stat}^{\\rm obs}|)  + \\mathbb{P}_{H_0}(T_{\\rm stat} > |T_{\\rm stat}^{\\rm obs}|)\\\\\n&= \\mathbb{P}(t_{n-1} \\leq -|T_{\\rm stat}^{\\rm obs}|) +  \\mathbb{P}(t_{n-1} \\geq |T_{\\rm stat}^{\\rm obs}|) \\\\\n&= 2 \\,\\mathbb{P}(t_{n-1} \\leq -|T_{\\rm stat}^{\\rm obs}|)\n\\end{aligned}\n\nt.stat <- sqrt(n)*(x_mean-mu0)/x_sd\np.value <- 2*pt(-abs(t.stat),df) \nc(t.stat, df, p.value)\n\n[1]  1.2708058 77.0000000  0.2076238\n\n\n\n\n\nHere, p_{\\rm value}= 0.208. Then, for any significance level less than 0.208, we cannot reject the hypothesis that \\mu = 500.\n\n\n2.1.3 Confidence interval for the mean\nWe have just seen that the data doesn’t allow us to reject the hypothesis that \\mu = 500. But we would come to the same conclusion with other values of \\mu_0. In particular, we will never reject the hypothesis that \\mu = \\bar{x}:\n\nt.test(x, mu=x_mean, conf.level = 1 - alpha)$p.value\n\n[1] 1\n\n\nFor a given significance level (\\alpha = 0.05 for instance), we will not reject the null hypothesis for values of \\mu_0 close enough to \\bar{x}.\n\npv.510 <- t.test(x, mu = 510, conf.level = 1-alpha)$p.value\npv.497 <- t.test(x, mu = 497, conf.level = 1-alpha)$p.value\nc(pv.510, pv.497)\n\n[1] 0.44265350 0.06340045\n\n\nOn the other hand, we will reject H_0 for values of \\mu_0 far enough from \\bar{x}:\n\npv.520 <- t.test(x, mu = 520, conf.level = 1-alpha)$p.value\npv.490 <- t.test(x, mu = 490, conf.level = 1-alpha)$p.value\nc(pv.520, pv.490)\n\n[1] 0.006204188 0.001406681\n\n\nThere exist two values of \\mu_0 for which the decision is borderline\n\npv1 <- t.test(x, mu = 496.47, conf.level = 1-alpha)$p.value\npv2 <- t.test(x, mu = 515.97, conf.level = 1-alpha)$p.value\nc(pv1,pv2)\n\n[1] 0.04993761 0.05001986\n\n\nIn fact, for a given \\alpha, these two values \\mu_{\\alpha,{\\rm lower}} and \\mu_{\\alpha,{\\rm upper}} define a confidence interval for \\mu: We are ``confident’’ at the level 1-\\alpha that any value between \\mu_{\\alpha,{\\rm lower}} and \\mu_{\\alpha,{\\rm upper}} is a possible value for \\mu.\n\n\nShow the code\nmu     <- seq(490, 520, by = 0.25)\nt_stat <- (x_mean - mu) / x_sd * sqrt(n)\npval   <- pmin(pt(-t_stat, df) + (1 - pt(t_stat, df)),\n               pt(t_stat, df) + (1 - pt(-t_stat, df)))\nCI     <- x_mean + x_sd/sqrt(n) * qt(c(alpha/2, 1-alpha/2), df)\ndata.frame(mu = mu, p.value = pval) %>% \n  ggplot() + geom_line(aes(x = mu, y = p.value)) + \n  geom_vline(xintercept = x_mean, colour=\"red\", linetype=2)+\n  geom_hline(yintercept = alpha, colour=\"green\", linetype=2)+\n  geom_vline(xintercept = CI, colour=\"red\") +\n  scale_x_continuous(breaks = round(c(490,500,510,520,CI,x_mean),2)) \n\n\n\n\n\nBy construction,\n\\begin{aligned}\n1-\\alpha &=  \\mathbb{P}\\left(qt_{\\frac{\\alpha}{2},n-1} < \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} < qt_{1-\\frac{\\alpha}{2},n-1} \\right) \\\\\n&= \\mathbb{P}\\left(\\bar{x} +\\frac{s}{\\sqrt{n}}qt_{\\frac{\\alpha}{2},n-1} < \\mu < \\bar{x} +\\frac{s}{\\sqrt{n}}qt_{1-\\frac{\\alpha}{2},n-1} \\right)\n\\end{aligned}\nThe confidence interval of level 1-\\alpha for \\mu is therefore the interval {\\rm CI}_{1-\\alpha} = \\left[\\bar{x} +\\frac{s}{\\sqrt{n}}qt_{\\frac{\\alpha}{2},n-1} \\ \\ , \\ \\\n\\bar{x} +\\frac{s}{\\sqrt{n}}qt_{1-\\frac{\\alpha}{2},n-1}\\right] \n\n(CI <- x_mean + x_sd / sqrt(n) * qt(c(alpha/2, 1-alpha/2), df))\n\n[1] 496.4727 515.9709\n\n\n\nRemark. The fact that \\mathbb{P}( \\mu \\in {\\rm CI}_{1-\\alpha}) = 1- \\alpha does not mean that \\mu is a random variable! It is the bounds of the confidence interval that are random because they are function of the data.\nA confidence interval of level 1-\\alpha should be interpreted like this: imagine that we repeat the same experiment many times, with the same experimental conditions, and that we build a confidence interval for \\mu for each of these replicate. Then, the true mean \\mu will lie in the confidence interval (1-\\alpha)100\\% of the times.\nLet us check this property with a Monte Carlo simulation.\n\nn_replicate <- 1e5\nn <- 100; mu <- 500; sd <- 40\nR <- replicate(n_replicate, {\n  x  <- rnorm(n, mu, sd)\n  ci <- mean(x) + sd(x)/sqrt(n)*qt(c(alpha/2, 1-alpha/2), n-1)\n  (mu > ci[1] & mu < ci[2])\n})\nmean(R)\n\n[1] 0.94987\n\n\n\n\nRemark. The decision rule to reject or not the null hypothesis can be derived from the confidence interval. Indeed, the confidence interval plays the role of an acceptance region: we reject H_0 if \\mu_0 does not belong to {\\rm CI}_{1-\\alpha}.\nIn the case of a one sided test, the output of t.test called confidence interval is indeed an acceptance region for \\mu, but not a “confidence interval” (we cannot seriously consider that \\mu can take any value above 500 for instance )\n\nrbind(\nc(x_mean + x_sd/sqrt(n)*qt(alpha,df) , Inf),\nc(-Inf, x_mean + x_sd/sqrt(n)*qt(1-alpha,df)))\n\n         [,1]     [,2]\n[1,] 499.0229      Inf\n[2,]     -Inf 513.4207"
  },
  {
    "objectID": "docs/tests/map566-lecture-single.html#two-samples-t-test",
    "href": "docs/tests/map566-lecture-single.html#two-samples-t-test",
    "title": "Statistical Tests",
    "section": "2.2 Two samples t-test",
    "text": "2.2 Two samples t-test\n\n2.2.1 What should we test?\nLet us now compare the weights of the male and female rats.\n\nrat_weight %>% \n  ggplot() + aes(x = gender, y = weight) +\n  geom_violin(aes(fill = gender)) + geom_jitter(alpha = 0.5) +  ylab(\"weight (g)\")\n\n\n\n\nLooking at the data is more than enough for concluding that the mean weight of the males is (much) larger than the mean weight of the females Computing a p-value here is of little interest \n\nrat_weight %>% group_by(gender) %>% summarize(average_weight = mean(weight)) %>% \n  rmarkdown::paged_table()\n\n\n  \n\n\nx <- rat_weight %>% filter(gender == \"Male\") %>% pull(\"weight\")\ny <- rat_weight %>% filter(gender == \"Female\") %>% pull(\"weight\")\nt.test(x, y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 40.35, df = 117.08, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 212.4535 234.3851\nsample estimates:\nmean of x mean of y \n 506.2218  282.8025 \n\n\nLet us see now what happens if we compare the control and GMO groups for the male rats.\n\nrat_weight %>% filter(gender == \"Male\") %>% \n  ggplot() + aes(x = regime, y = weight) +\n  geom_boxplot(aes(fill = regime)) + geom_jitter(alpha = 0.5) +  ylab(\"weight (g)\")\n\n\n\nx <- rat_weight %>% filter(gender == \"Male\" & regime == \"Control\") %>% pull(\"weight\")\ny <- rat_weight %>% filter(gender == \"Male\" & regime == \"GMO\") %>% pull(\"weight\")\n\nWe observe a difference between the two empirical means (the mean weight after 14 weeks is greater in the control group), but we cannot say how significant this difference is by simply looking at the data. Performing a statistical test is now necessary.\nLet x_{1}, x_{2}, \\ldots, x_{n_x} be the weights of the n_x male rats of the control group and y_{1}, y_{2}, \\ldots, y_{n_x} the weights of the n_y male rats of the GMO group. We will assume normal distributions for both (x_{i}) and (y_{i}):\n x_{i} \\sim^{\\mathrm{iid}} \\mathcal{N}(\\mu_x \\ , \\ \\sigma^2_x) \\quad ; \\quad y_{i} \\sim^{\\mathrm{iid}} \\mathcal{N}(\\mu_y \\ , \\ \\sigma^2_y)\nWe want to test\nH_0: \\ ``\\mu_x = \\mu_y\" \\quad \\text{versus} \\quad H_1: \\ ``\\mu_x \\neq \\mu_y\" \n\n\n2.2.2 Assuming equal variances\nWe can use the function t.test assuming first equal variances (\\sigma^2_x=\\sigma_y^2)\n\nalpha <- 0.05\nt.test(x, y, conf.level = 1-alpha, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  x and y\nt = 1.5426, df = 76, p-value = 0.1271\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.358031 34.301621\nsample estimates:\nmean of x mean of y \n 513.7077  498.7359 \n\n\nThe test statistic is\nT_{\\rm stat} = \\frac{\\bar{x} - \\bar{y}}{s_p \\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}}   where s_p^2 is the pooled variance:\ns_p^2 = \\frac{1}{n_x+n_y-2} \\left(\\sum_{i=1}^{n_x} (x_{i}-\\bar{x})^2 + \\sum_{i=1}^{n_y} (y_{i}-\\bar{y})^2 \\right)  \nUnder the null hypothesis, T_{\\rm stat} follows a t-distribution with n_x+n_y-2 degree of freedom. The p-value is therefore\n\\begin{aligned}\np_{\\rm value} & = \\mathbb{P}_{H_0}(|T_{\\rm stat}| > |T_{\\rm stat}^{\\rm obs}|) \\\\\n&= \\mathbb{P}(t_{n_x+n_y-2} \\leq -T_{\\rm stat}^{\\rm obs}) + 1 - \\mathbb{P}(t_{n_x+n_y-2} \\leq T_{\\rm stat}^{\\rm obs})\n\\end{aligned}\n\nnx <- length(x)\nny <- length(y)\nx_mean <- mean(x)\ny_mean <- mean(y)\nx_sc <- sum((x-x_mean)^2)\ny_sc <- sum((y-y_mean)^2)\nxy_sd <- sqrt((x_sc+y_sc)/(nx+ny-2))\nt.stat <- (x_mean-y_mean)/xy_sd/sqrt(1/nx+1/ny)\ndf <- nx + ny -2\np.value <- pt(-t.stat, df) + (1- pt(t.stat, df))\nc(t.stat, df, p.value)\n\n[1]  1.5426375 76.0000000  0.1270726\n\n\nThe confidence interval for the mean difference \\mu_x-\\mu_y is computed as {\\rm CI}_{1-\\alpha} = [\\bar{x} - \\bar{y} +s_p \\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}qt_{\\frac{\\alpha}{2},n_x+n_y-2} \\ \\ , \\ \\\n\\bar{x} - \\bar{y} +s_p \\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}qt_{1-\\frac{\\alpha}{2},n_x+n_y-2} ] \n\nx_mean - y_mean  + xy_sd*sqrt(1/nx+1/ny)*qt(c(alpha/2,1-alpha/2), df)\n\n[1] -4.358031 34.301621\n\n\n\n\n2.2.3 Assuming different variances\nAssuming equal variances for the two groups may be disputable.\n\nrat_weight %>% filter(gender == \"Male\") %>% \n  group_by(regime) %>% \n  summarise(mean = mean(weight), sd = sd(weight))\n\n# A tibble: 2 x 3\n  regime   mean    sd\n  <chr>   <dbl> <dbl>\n1 Control  514.  43.2\n2 GMO      499.  42.5\n\n\nWe can then use the t.test function with different variances (which is the default)\n\nt.test(x, y, conf.level = 1-alpha)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 1.5426, df = 75.976, p-value = 0.1271\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.358129 34.301719\nsample estimates:\nmean of x mean of y \n 513.7077  498.7359 \n\n\n\n\n\nThe Welch (or Satterthwaite) approximation to the degrees of freedom is used instead of n_x+n_y-2= 76:\n\\mathrm{df}_W = \\frac{(c_x + c_y)^2}{{c_x^2}/{(n_x-1)} + {c_y^2}/{(n_y-1)}} where c_x = \\sum (x_{i}-\\bar{x})^2/(n_x(n_x-1)) and c_y = \\sum (y_{i}-\\bar{y})^2/(n_y(n_y-1)).\nFurthermore, unlike in Student’s t-test with equal variances, the denominator is not based on a pooled variance estimate:\nT_{\\rm stat} = \\frac{\\bar{x} - \\bar{y}}{ \\sqrt{{s_x^2}/{n_x}+{s_y^2}/{n_y}}}   where s_x^2 and s_y^2 are the empirical variances of (x_i) and (y_i):  s_x^2 = \\frac{1}{n_x-1}\\sum_{i=1}^{n_x} (x_{i}-\\bar{x})^2  \\quad ; \\quad\ns_y^2 = \\frac{1}{n_y-1}\\sum_{i=1}^{n_y} (y_{i}-\\bar{y})^2\n\nsbar.xy <- sqrt(var(x)/nx+var(y)/ny)\nt.stat <- (x_mean-y_mean)/sbar.xy\ncx <- x_sc/(nx-1)/nx\ncy <- y_sc/(ny-1)/ny\ndfw <- (cx + cy)^2 / (cx^2/(nx-1) + cy^2/(ny-1))\np.value <- pt(-t.stat,dfw) + (1- pt(t.stat,dfw))\nc(t.stat, dfw, p.value)\n\n[1]  1.5426375 75.9760868  0.1270739\n\n\nThe confidence interval for \\mu_x-\\mu_y is now computed as\n{\\rm CI}_{1-\\alpha} = [\\bar{x} - \\bar{y} +\\sqrt{\\frac{s_x^2}{n_x}+\\frac{s_y^2}{n_y}} \\ qt_{\\frac{\\alpha}{2},\\mathrm{df}_W} \\ \\ , \\ \\\n\\bar{x} - \\bar{y} +\\sqrt{\\frac{s_x^2}{n_x}+\\frac{s_y^2}{n_y}} \\ qt_{1-\\frac{\\alpha}{2},\\mathrm{df}_W} ] \n\nx_mean-y_mean  + sbar.xy*qt(c(alpha/2,1-alpha/2),dfw)\n\n[1] -4.358129 34.301719"
  },
  {
    "objectID": "docs/tests/map566-lecture-single.html#power-of-a-t-test",
    "href": "docs/tests/map566-lecture-single.html#power-of-a-t-test",
    "title": "Statistical Tests",
    "section": "4.1 Power of a t-test",
    "text": "4.1 Power of a t-test\nUntil now, we have demonstrated that the experimental data does not highlight any significant difference in weight between the control group and the GMO group.\nOf course, that does not mean that there is no difference between the two groups. Indeed, absence of evidence is not evidence of absence. In fact, no experimental study would be able to demonstrate the absence of effect of the diet on the weight.\nNow, the appropriate question is rather to evaluate what the experimental study can detect. If feeding a population of rats with GMOs has a signicant biological effect on the weight, can we ensure with a reasonable level of confidence that our statistical test will reject the null hypothesis and conclude that there is indeed a difference in weight between the two groups?\nA power analysis allows us to determine the sample size required to detect an effect of a given size with a given degree of confidence. Conversely, it allows us to determine the probability of detecting an effect of a given size with a given level of confidence, under sample size constraints.\nFor a given \\delta \\in {\\mathbb R}, let \\beta(\\delta) be the type II error rate, i.e. the probability to fail rejecting H_0 when \\mu_x-\\mu_y = \\delta, with \\delta\\neq 0.\nThe power of the test is the probability to reject the null hypothesis when it is false. It is also a function of \\delta =\\mu_x-\\mu_y defined as\n \\begin{aligned}\n\\eta(\\delta) &= 1 - \\beta(\\delta) \\\\\n&= \\mathbb{P}(\\text{reject } H_0 \\ | \\ \\mu_x-\\mu_y=\\delta )\n\\end{aligned} \nRemember that, for a two sided test, we reject the null hypothesis when |T_{\\rm stat}| > qt_{1-\\alpha/2, \\mathrm{df}}, where \\mathrm{df} is the appropriate degree of freedom.\nOn the other hand, {(\\bar{x} - \\bar{y} - \\delta)}/{s_{xy}}, where s_{xy} = \\sqrt{{s_x^2}/{n_x}+{s_y^2}/{n_y}}, follows a t-distribution with \\mathrm{df} degrees of freedom. Thus,\n \\begin{aligned}\n\\eta(\\delta) &= 1 - \\mathbb{P}\\left(qt_{\\frac{\\alpha}{2},\\mathrm{df}} < T_{\\rm stat} < qt_{1 -\\frac{\\alpha}{2},\\mathrm{df}} \\ | \\ \\mu_x-\\mu_y=\\delta\\right) \\\\\n& = 1- \\mathbb{P}\\left(qt_{\\frac{\\alpha}{2},\\mathrm{df}} < \\frac{\\bar{x} - \\bar{y}}{s_{xy}} < qt_{1-\\frac{\\alpha}{2},\\mathrm{df}} \\ | \\ \\mu_x-\\mu_y=\\delta\\right) \\\\\n&= 1- \\mathbb{P}\\left(qt_{\\frac{\\alpha}{2},\\mathrm{df}} - \\frac{\\delta}{s_{xy}} < \\frac{\\bar{x} - \\bar{y} - \\delta}{s_{xy}} < qt_{1-\\frac{\\alpha}{2},\\mathrm{df}} - \\frac{\\delta}{s_{xy}} \\ | \\ \\mu_x-\\mu_y=\\delta\\right) \\\\\n&= 1 - Ft_{\\mathrm{df}}\\left(qt_{1-\\frac{\\alpha}{2},\\mathrm{df}} - \\frac{\\delta}{s_{xy}}\\right) + Ft_{\\mathrm{df}}\\left(qt_{\\frac{\\alpha}{2},\\mathrm{df}} - \\frac{\\delta}{s_{xy}}\\right)\n\\end{aligned} \nAs an example, let us compute the probability to detect a difference in weight of 10g with two groups of 80 rats each and assuming that the standard deviation is 30g in each group.\n\nalpha=0.05\nnx_new <- ny_new <- 80\ndelta_mu <- 10\nx_sd <- 30\ndf <- nx_new + ny_new-2\ndt <- delta_mu/x_sd/sqrt(1/nx_new+1/ny_new)\n1-pt(qt(1-alpha/2,df)-dt,df) + pt(qt(alpha/2,df)-dt,df) \n\n[1] 0.5528906\n\n\nThe function pwr.t.test allows to compute this power:\n\npwr.t.test(n = nx_new, d = delta_mu/x_sd, type = \"two.sample\",\n           alternative=\"two.sided\", sig.level = alpha)\n\n\n     Two-sample t test power calculation \n\n              n = 80\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.5538758\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nLet us perform a Monte Carlo simulation, to check this result and better understand what it means. Imagine that the “true” difference in weight is \\delta=10g. Then, if could repeat the same experiment a (very) large number of times, we would reject the null hypothesis in 55\\% of cases.\n\nn_replicate <- 1e5\nmu_x <- 500\nmu_y <- mu_x + delta_mu\nR <- replicate(n_replicate, {\n  x_new <- rnorm(nx_new, mu_x, x_sd)\n  y_new <- rnorm(ny_new, mu_y, x_sd)\n  t.test(x_new, y_new, alternative=\"two.sided\")$p.value < alpha\n})\nmean(R)\n\n[1] 0.55364\n\n\nWe may consider this probability as too small. If our objective is a power of 80% at least, with the same significance level, we need to increase the sample size.\n\npwr.t.test(power = 0.8, d = delta_mu/x_sd, sig.level=alpha) \n\n\n     Two-sample t test power calculation \n\n              n = 142.2462\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\nIndeed, we see that n\\geq 143 animals per group are required in order to reach a power of 80%.\n\nnx_new <- ny_new <- ceiling(pwr.t.test(power=0.8, d = delta_mu/x_sd, sig.level=alpha)$n)\ndf <- nx_new + ny_new - 2\ndt <- delta_mu/x_sd/sqrt(1/nx_new + 1/ny_new)\n1-pt(qt(1-alpha/2,df)-dt,df) + pt(qt(alpha/2,df)-dt,df)\n\n[1] 0.8020466\n\n\nAn alternative for increasing the power consists in increasing the type I error rate\n\npwr.t.test(power=0.8, d=delta_mu/x_sd, n = 80, sig.level = NULL) \n\n\n     Two-sample t test power calculation \n\n              n = 80\n              d = 0.3333333\n      sig.level = 0.2067337\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIf we accept a significance level of about 20%, then we will be less demanding for rejecting H_0: we will reject the null hypothesis when |T_{\\rm stat}|>qt_{0.9,158}= 1.29, instead of |T_{\\rm stat}|>qt_{0.975,158}= 1.98. This strategy will therefore increase the power, but also the type I error rate."
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#regression-models",
    "href": "docs/regression/map566-lecture-regression-background.html#regression-models",
    "title": "Linear Regression: Quick Recap",
    "section": "Regression models",
    "text": "Regression models\nA regression model relates a response variable y to a set of explanatory variables x. Assuming that we have access to n set of values (x_j, y_j), 1 \\leq j \\leq n), of these variable, the regression model is assumed to take the form y_j = f(x_j,\\beta) + \\varepsilon_j \\quad ; \\quad 1\\leq j \\leq n\nwhere f is a structural model which depends on a p-vector of parameters \\beta. We will assume that the residuals (\\varepsilon_j) are independent random variables with mean 0 and variance \\sigma^2: \n\\mathbb{E}(\\varepsilon_j) = 0 \\quad ; \\quad  \\mathbb{E}(\\varepsilon^2_j) = \\sigma^2 \\quad ; \\quad \\mathbb{E}(\\varepsilon_j \\varepsilon_k) = 0  \\ (j \\neq k)"
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#ordinary-least-squares",
    "href": "docs/regression/map566-lecture-regression-background.html#ordinary-least-squares",
    "title": "Linear Regression: Quick Recap",
    "section": "1 Ordinary least squares",
    "text": "1 Ordinary least squares\n\n1.1 Least squares estimator\nA method for choosing automatically the “best parameters” \\beta consists in minimizing the sum of squared errors of prediction, i.e. the residual sum of squares (RSS) :\n\nRSS(\\beta) = \\sum_{j=1}^n (y_j - f(x_j))^2 = \\|y - X\\beta \\|^2\n\nThen, \n\\hat{\\beta} = \\arg\\min_{\\beta} \\|y - X\\beta \\|^2 = (X^\\prime X)^{-1}X^\\prime y\n\n\n\n1.2 Estimation of the residual error variance\nAn unbiased estimate of \\sigma^2 is \n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\|y - X\\hat{\\beta} \\|^2\n Indeed,\n\n\\begin{aligned}\n\\|y - X\\hat{\\beta} \\|^2 &= \\| y - X(X^\\prime X)^{-1}X^\\prime y\\|^2   \\\\\n&= \\| \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon\\|^2 \\\\\n&= \\varepsilon^\\prime \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right)^\\prime \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon \\\\\n&= \\varepsilon^\\prime \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon \\\\\n&= {\\rm trace} \\left\\{ \\varepsilon^\\prime \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon \\right\\} \\\\\n&= {\\rm trace} \\left\\{  \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\varepsilon \\varepsilon^\\prime \\right\\}\n\\end{aligned}\n Then,\n\n\\begin{aligned}\n\\mathbb{E}{\\|y - X\\hat{\\beta} \\|^2} &= {\\rm trace} \\left(  \\left({\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\mathbb{E}{\\varepsilon \\varepsilon^\\prime} \\right) \\\\\n&= \\sigma^2 {\\rm trace} \\left(  {\\rm I}_n - X(X^\\prime X)^{-1}X^\\prime \\right) \\\\\n&= \\sigma^2 \\left( {\\rm trace} \\left( {\\rm I}_n \\right)  - {\\rm trace} \\left(X(X^\\prime X)^{-1}X^\\prime \\right) \\right) \\\\\n&= \\sigma^2 \\left( n  - {\\rm trace} \\left((X^\\prime X)^{-1}X^\\prime X \\right) \\right) \\\\\n&= \\sigma^2 \\left( n  - {\\rm trace} \\left({\\rm I}_d \\right) \\right) \\\\\n&= \\sigma^2 ( n - p)\n\\end{aligned}\n\nThe standard deviation of the residual errors, called residual standard error in R, is the square root of this estimated variance\n\n\n1.3 The standard errors of the estimates\nWe can remark that \n\\begin{aligned}\n\\hat{\\beta} &=  (X^\\prime X)^{-1}X^\\prime y \\\\\n&= \\beta + (X^\\prime X)^{-1}X^\\prime \\varepsilon\n\\end{aligned}\n\nThen, since \\mathbb{E}{e}=0,  \\mathbb{E}(\\hat{\\beta}) = \\beta\nand \n\\begin{aligned}\n\\mathbb{V}{\\hat\\beta} &=  \\mathbb{V}{(X^\\prime X)^{-1}X^\\prime \\varepsilon} \\\\\n&= (X^\\prime X)^{-1}X^\\prime \\mathbb{V}{\\varepsilon} X (X^\\prime X)^{-1} \\\\\n&= \\sigma^2 (X^\\prime X)^{-1}\n\\end{aligned}\n\nWe can therefore use this formula to compute the variance covariance matrix of \\hat\\beta. Then, the standard error of each component of \\hat\\beta is defined as the square root of the diagonal elements of the variance-covariance matrix V=\\mathbb{V}{\\hat\\beta}:\n{\\rm se}(\\hat\\beta_k) = \\sqrt{V_{k k}}"
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#maximum-likelihood-approach",
    "href": "docs/regression/map566-lecture-regression-background.html#maximum-likelihood-approach",
    "title": "Linear Regression: Quick Recap",
    "section": "2 Maximum likelihood approach",
    "text": "2 Maximum likelihood approach\nIf we assume that (\\varepsilon_j, 1 \\leq j \\leq n) is a sequence of independent and normally distributed random variables with mean 0 and variance 1: \n\\varepsilon_j \\sim^{\\mathrm{iid}} \\mathcal{N}(0, 1),\n then the y_j are also independent and normally distributed:\n y_j \\sim \\mathcal{N}(f(x_j, \\beta),\\sigma^2). The vector y=(y_1,y_2,\\ldots,y_n) is therefore a Gaussian vector which probability density function (pdf) depends on a vector of parameters \\theta=(\\beta,\\sigma^2):\n\\begin{aligned}\n\\mathbb{P}(y ; \\theta) = \\prod_{j=1}^n \\mathbb{P}(y_j; \\theta) & = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\text{exp}\\left(-\\frac{1}{2\\sigma^2}(y_j - f(x_j, \\beta))^2 \\right) \\\\\n& =  (2\\pi \\sigma^2)^{-\\frac{n}{2}} \\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j, \\beta))^2\\right).\n\\end{aligned}\n\n2.1 Likelihood\nFor a given vector of observations y, the likelihood \\ell is the function of the parameter \\theta=(\\beta,\\sigma^2) defined as:\n\n\\ell(\\theta) = \\mathbb{P}(y ; \\theta)\n The log-likelihood is therefore \n\\log\\ell(\\theta) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2\n\n\n\n2.2 Maximum likelihood estimator\nAssume that \\theta takes its values in a subset \\Theta of \\mathbb{R}^p. Then, the Maximum Likelihood (ML) estimator of \\theta is a function of y that maximizes the likelihood function:\n\n\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta}\\ell(\\theta) = \\arg\\max_{\\theta \\in \\Theta}\\log\\ell(\\theta)\n\nMaximization of the log-likelihood can be performed in two steps:\n\n\\beta, the parameter of the structural model is estimated by minimizing the residual sum of squares:\n\n\\begin{aligned}\n\\hat{\\beta} &= \\arg\\min_{\\beta} \\left\\{\nn\\log(2\\pi) + n\\log(\\sigma^2) + \\frac{1}{\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2\n\\right\\} \\\\\n&= \\arg\\min_{\\beta}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2\n\\end{aligned}\nWe see that, for this model, the Maximum Likelihood estimator \\hat{\\beta} is also the Least Squares estimator of \\beta.\n\n\\sigma^2, the variance of the residual errors \\varepsilon_j is estimated in a second step:\n\n\\begin{aligned}\n\\hat{\\sigma}^2 &= \\arg\\min_{\\sigma^2 \\in \\mathbb{R}^+} \\left\\{\nn\\log(2\\pi) + n\\log(\\sigma^2) + \\frac{1}{\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j,\\hat{\\beta}))^2\n\\right\\} \\\\\n&= \\frac{1}{n}\\sum_{j=1}^n(y_j - f(x_j,\\hat{\\beta}))^2\n\\end{aligned}\nFinally, the log-likelihood computed with \\hat{\\theta}=(\\hat{\\beta},\\hat{\\sigma}^2) reduces to \n\\log\\ell(\\hat{\\theta}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{1}{n}\\sum_{j=1}^n(y_j - f(x_j,\\hat{\\beta}))^2\\right) -\\frac{n}{2}\n\n\n\n2.3 The Fisher Information matrix\n\n\n2.4 Some general definitions\nThe partial derivative of the log-likelihood with respect to \\theta is called the score. Under general regularity conditions, the expected value of the score is 0. Indeed, it is easy to show that\n\\mathbb{E}\\left(\\frac{\\partial}{\\partial \\theta} \\log\\mathbb{P}(y;\\theta^\\star)\\right) = 0.\nwhere \\theta^\\star is the ``true’’ unknown value of \\theta such that the observations y where generated with model \\mathbb{P}(\\cdot;\\theta^\\star).\nThe variance of the score is called the Fisher information matrix (FIM): \nI_n(\\theta^\\star) = \\mathbb{E}{\\left(\\frac{\\partial}{\\partial \\theta} \\log\\mathbb{P}(y;\\theta^\\star)\\right)\\left(\\frac{\\partial}{\\partial \\theta} \\log\\mathbb{P}(y;\\theta^\\star)\\right)^\\prime}.\n\nFurthermore, it can be shown that if \\log\\ell is twice differentiable with respect to \\theta,\n\n\\begin{aligned}\nI_n(\\theta^\\star) &= -  \\mathbb{E}\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^\\prime} \\log\\mathbb{P}(y;\\theta^\\star)\\right) \\\\\n&= - \\sum_{j=1}^n \\mathbb{E}\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^\\prime} \\log\\mathbb{P}(y_j;\\theta^\\star)\\right)\n\\end{aligned}\n\n\n\n2.5 The central limit theorem\nThe following central limit theorem (CLT) holds under certain regularity conditions: \nI_n(\\theta^\\star)^{\\frac{1}{2}}(\\hat{\\theta}-\\theta^\\star) \\xrightarrow{n\\to \\infty} {\\mathcal N}(0,{\\rm Id}_n) .\n This theorem shows that under relevant hypotheses, the estimator \\hat{\\theta} is consistent and converges to \\theta^\\star at rate \\sqrt{n} since I_n=\\mathcal{O}(n).\nThe normalizing term I_n(\\theta^\\star)^{-1} is unknown since it depends on the unknown parameter \\theta^\\star. We can use instead the observed Fisher information: \n\\begin{aligned}\nI_{y}(\\hat{\\theta}) &= - \\frac{\\partial^2}{\\partial \\theta^2} \\log\\ell(\\hat{\\theta}) \\\\\n&=-\\sum_{i=1}^n \\frac{\\partial^2}{\\partial \\theta^2} \\log \\mathbb{P}(y_i ; \\hat{\\theta}).\n\\end{aligned}\n We can then approximate the distribution of \\hat{\\theta} by a normal distribution with mean \\theta^\\star and variance-covariance matrix I_y(\\hat{\\theta})^{-1}: \n\\hat{\\theta} \\approx {\\mathcal N}(\\theta^\\star , I_y(\\hat{\\theta})^{-1}) .\n The square roots of the diagonal elements of I_y(\\hat{\\theta})^{-1} are called the standard errors (s.e.) of the elements of \\hat{\\theta}.\n\n\n2.6 The FIM for a regression model\nWe have seen that, for a regression model, \n\\begin{aligned}\n\\log\\ell(\\theta) &= \\log\\ell(\\beta,\\sigma^2) \\\\\n&= -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2\n\\end{aligned}\n By definition,\n\nI_n(\\theta) =  \\left( \\begin{array}{cc}\n-\\mathbb{E}{\\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\log\\ell(\\beta,\\sigma^2)} &\n-\\mathbb{E}{\\frac{\\partial^2}{\\partial \\beta \\partial \\sigma^2} \\log\\ell(\\beta,\\sigma^2)} \\\\\n-\\mathbb{E}{\\frac{\\partial^2}{\\partial \\sigma^2 \\partial \\beta^\\prime } \\log\\ell(\\beta,\\sigma^2)} &  \n-\\mathbb{E}{\\frac{\\partial^2}{\\partial \\sigma^{2^2} } \\log\\ell(\\beta,\\sigma^2)}\n\\end{array} \\right)\n\nThen, \n\\begin{aligned}\n\\mathbb{E}{\\frac{\\partial^2}{\\partial \\beta \\partial \\sigma^2} \\log\\ell(\\beta,\\sigma^2)} &=\n-\\frac{1}{\\sigma^4} \\times\\frac{\\partial}{\\partial \\beta}f(x_j,\\beta) \\times\\mathbb{E}{y_j - f(x_j,\\beta)} \\\\\n&= 0\n\\end{aligned}\n\nand the FIM reduces to\n\nI_n(\\theta) =  \\left( \\begin{array}{cc}\n-  \\mathbb{E}{\\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\log\\ell(\\beta,\\sigma^2)} & 0 \\\\\n0 &  -\\mathbb{E}{\\frac{\\partial^2}{\\partial \\sigma^{2^2} } \\log\\ell(\\beta,\\sigma^2)}\n\\end{array} \\right)\n\nBecause of the bloc structure of I_n(\\theta^\\star), the variance-covariance of \\hat{\\beta} can be estimated by I^{-1}_y(\\hat{\\beta}) where \n\\begin{aligned}\nI_y(\\hat{\\beta}) &= - \\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\log\\ell(\\hat{\\beta},\\hat{\\sigma}^2) \\\\\n&= \\frac{1}{2\\hat\\sigma^2}\\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\left(\\sum_{j=1}^n(y_j - f(x_j,\\hat\\beta))^2 \\right) \\\\\n&= \\frac{1}{\\hat\\sigma^2} \\sum_{j=1}^n \\left(\n\\left(\\frac{\\partial}{\\partial \\beta}f(x_j,\\hat\\beta)\\right)\\left(\\frac{\\partial}{\\partial \\beta}f(x_j,\\hat\\beta)\\right)^\\prime - \\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime}f(x_j,\\hat\\beta)y_j\n\\right)\n\\end{aligned}\n Remark: In the case of a linear model y=X\\beta+e, we find that I_y(\\hat{\\beta}) = (X^\\prime X)/\\hat\\sigma^2.\nThe variance of \\hat{\\sigma}^2 is estimated by I^{-1}_y(\\hat{\\sigma}^2) where \n\\begin{aligned}\nI_y(\\hat{\\sigma}^2) &= -\\frac{\\partial^2}{\\partial \\sigma^{2^2} } \\log\\ell(\\hat{\\beta},\\hat{\\sigma}^2) \\\\\n&= -\\frac{n}{2\\hat\\sigma^4} + \\frac{1}{\\hat\\sigma^6}\\sum_{j=1}^n(y_j - f(x_j,\\hat\\beta))^2 \\\\\n&= \\frac{n}{2\\hat\\sigma^4}\n\\end{aligned}\n\nThen, {\\rm se}(\\hat{\\sigma}^2) = \\hat{\\sigma}^2/\\sqrt{n/2}."
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#statistical-inference-and-diagnostics",
    "href": "docs/regression/map566-lecture-regression-background.html#statistical-inference-and-diagnostics",
    "title": "Linear Regression: Quick Recap",
    "section": "3 Statistical inference and diagnostics",
    "text": "3 Statistical inference and diagnostics\nSuppose that residuals (\\varepsilon_j) are independent and normally distributed with mean 0 and variance \\sigma^2: \n\\varepsilon_j \\sim^{\\mathrm{iid}} \\mathcal{N}(0 \\ , \\ \\sigma^2).\n\n\n3.1 Statistical tests for the model parameters\nIn this case, \\hat{\\beta} is also normally distributed:\n\n\\hat{\\beta} \\sim \\mathcal{N}(\\beta \\ , \\ \\sigma^2 (X^\\prime X)^{-1})\n and, for k=1, 2, \\ldots , p, t_k = \\frac{\\hat{\\beta}_k - \\beta_k}{{\\rm se}(\\hat{\\beta}_k)} follows a t-distribution with n-d degrees of freedom.\nFor each component \\beta_k of \\beta, we can then perform a t-test (known as the Wald test) to test \nH_{k,0} : ``\\beta_k = 0\"  \\quad \\text{versus} \\quad H_{k,1}: ``\\beta_k \\neq 0\" \nIndeed, under the null hypothesis H_{k,0}, t_{{\\rm stat}, k} = {\\hat{\\beta}_k}/{{\\rm se}(\\hat{\\beta}_k)} follows a t-distribution with n-d degrees of freedom.\nThe p-value for this test is therefore\n\np_k= \\mathbb{P}{|t_{n-d}| \\geq |t_{{\\rm stat}, k}^{\\rm obs}| } = 2(1 - \\mathbb{P}{t_{n-d} \\leq |t_{{\\rm stat}, k}^{\\rm obs}| } )\n\n\n\n3.2 Confidence interval for the model parameters\nUsing the fact that t_k follows a t-distribution with n-p degrees of freedom, we can build a confidence interval for \\beta_k of level 1-\\alpha:\n{\\rm CI}_{1-\\alpha}(\\beta_k) = [\\hat{\\beta}_k + qt_{\\alpha/2, n-d}\\ {\\rm se}(\\hat{\\beta}_k) \\ , \\ \\hat{\\beta}_k + qt_{1-\\alpha/2, n-d}\\ {\\rm se}(\\hat{\\beta}_k)] where qt_{\\alpha/2, n-d} and qt_{1-\\alpha/2, n-d} are the quantiles of order \\alpha/2 and 1-\\alpha/2 for a t-distribution with n-p df.\nIndeed, we can easily check that \\mathbb{P}{{\\rm CI}_{1-\\alpha}(\\beta_k) \\ni \\beta_k} = 1-\\alpha.\nThe function confint computes such confidence intervals for \\beta (default level = 95%))\n\n\n3.3 F-test of the overall significance\nA F-test is also performed to test if at least one of the coefficients \\beta_1, \\ldots , \\beta_{p} is non zero:\n\n\\begin{aligned}\nH_0 &:  \\quad (\\beta_1, \\beta_2, \\cdots ,\\beta_p) = (0, 0, \\cdots, 0) \\\\\nH_1 &:  \\quad (\\beta_1, \\beta_2, \\cdots ,\\beta_p) \\neq (0, 0, \\cdots, 0)\n\\end{aligned}\n\nThe test statistic for testing H_0 against H_1 is \n\\begin{aligned}\nF_{\\rm stat} &=  \\frac{\\|X\\hat{\\beta} - \\bar{y}\\|^2/(p-1)}{\\|y - X\\hat{\\beta} \\|^2/(n-p)}\n\\end{aligned}\n\nLet us show that, under the null hypothesis, the test statistic F_{\\rm stat} has a F distribution with (p-1,n-p) degrees of freedom:\nBy construction, \\|y - X\\hat{\\beta} \\|^2/\\sigma^2 has a \\chi^2 distribution with n-d df. On the other hand, under H_0, y_j=\\varepsilon_j \\sim^{\\mathrm{iid}} \\mathcal{N}(0,\\sigma^2) and \\|y - \\bar{y}\\|^2/\\sigma^2 has a \\chi^2 distribution with n-1 df.\nUsing the fact that \n\\|y - \\bar{y}\\|^2 = \\|X\\hat{\\beta} - \\bar{y}\\|^2 + \\|y - X\\hat{\\beta} \\|^2\n We deduce that, under H_0, \\|X\\hat{\\beta} - \\bar{y}\\|^2/\\sigma^2 has a \\chi^2 distribution with (n-1) - (n-p) = p-1 df which leads to the conclusion since (\\chi^2(\\nu_1)/\\nu1)/(\\chi^2(\\nu_2)/\\nu2) = F(\\nu_1,\\nu_2).\nThe p-value of the F-test is therefore \\text{p-value(F-test)} = \\mathbb{P}(F_{p-1,n-p} > F_{\\rm stat})=1- \\mathbb{P}(F_{p-1,n-p} \\leq F_{\\rm stat})\nRemark: t-test and F-test are equivalent for linear models with only one predictor. In the case of polynomial regression of degree d = 1, both tests can be used equally for testing if \\beta_1=0. Indeed, \n\\begin{aligned}\nF_{\\rm stat} &=  \\frac{\\|\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y}\\|^2}{\\|y - \\hat{\\beta}_0 - \\hat{\\beta}_1 x\\|^2/(n-2)} \\\\[1.5ex]\n&=  \\frac{\\hat{\\beta}_1^2 \\|x - \\bar{x}\\|^2}{\\hat{\\sigma}^2} = \\frac{\\hat{\\beta}_1^2}{se^2(\\hat{\\beta}_1)} = t_{\\rm stat}^2\n\\end{aligned}\n\nFurthermore, if t_{\\rm stat} has a t distribution with n-2 df, then t_{\\rm stat}^2 has a F distribution with (1,n-2) df. Both p-values are therefore equal.\n\n\n3.4 Coefficient of determination\nThe multiple R-squared R^2 is the proportion of variation in the response variable that has been explained by the model. Using the fact that \n\\|y - \\bar{y}\\|^2 = \\|X\\hat{\\beta} - \\bar{y}\\|^2 + \\|y - X\\hat{\\beta} \\|^2\n\n\nR^2 =  \\frac{\\|X\\hat{\\beta} - \\bar{y}\\|^2}{\\|y - \\bar{y}\\|^2} = 1 - \\frac{\\|y - X\\hat{\\beta} \\|^2}{\\|y - \\bar{y}\\|^2}\n\nBy construction, adding more predictors to the model, i.e. increasing the degree of the polynome, is always going to increase the R-squared value. Adjusted R-squared penalizes this effect by normalizing each term by the associated degree of freedom.\n\n\\begin{aligned}\nR^2_{\\rm adj} &=  1 - \\frac{\\|y - X\\hat{\\beta} \\|^2/(n-p)}{\\|y - \\bar{y}\\|^2/(n-1)}\n\\end{aligned}\n\nThe R-squared is a purely descriptive statistics. The adjusted R-squared should be preferably used to compare the explanatory power of models built from the same dataset.\n\n\n3.5 Some diagnostic plots\nSeveral diagnostic plots are available. The first one plots the estimated residuals y_j - \\hat{f}(x_j) versus the predicted (or fitted) values with a smooth line.\nThe second diagnostic plot is a normal QQ plot. The QQ plot is obtained by plotting the standardized residuals (i.e. the residuals divided by their standard deviation) versus the theoretical quantiles of order 1/(n+1), 2/(n+1), \\ldots, n/(n+1). If the residuals are normally distributed, then the points should be randomly distributed around the line y=x."
  },
  {
    "objectID": "docs/regression/map566-lecture-regression-background.html#model-comparison",
    "href": "docs/regression/map566-lecture-regression-background.html#model-comparison",
    "title": "Linear Regression: Quick Recap",
    "section": "4 Model comparison",
    "text": "4 Model comparison\nAgain, we assume in this part that the residual errors are independent and identically distributed (i.i.d.), with a normal distribution, mean 0 and variance \\sigma^2.\n\n4.1 ANOVA\nConsider two nested linear models with, respectively, p_1 and p_2 coefficients. Let \\hat{y}_1 and \\hat{y}_2 be the respective predicted values under. Cochran Theorem states that\n \\|y - \\hat{y}_1 \\|^2 = \\|\\hat{y}_2 - \\hat{y}_1\\|^2 + \\|y - \\hat{y}_2 \\|^2 \nThen, the statistics used for the test is\n\nF_{\\rm stat} =  \\frac{\\text{explained variance}}{\\text{unexplained variance}} = \\frac{\\|\\hat{y}_2 - \\hat{y}_1\\|^2/(p_2 - p_1)}{\\|y - \\hat{y}_2 \\|^2/(n-p_2)}\n\nUnder the null, the test statistics F_{\\rm stat} follows a F distribution with (p_2-p_1 , n-p_2) degrees of freedom.\nThis ANOVA test can be performed by means of the anova function.\n\n\n4.2 Likelihood ratio test\nWhen two models are nested, we can compare them by performing a likelihood ratio test (LRT).\nLet \\log\\ell_1 and \\log\\ell_2 be the log-likelihood functions of models \\mathcal{M}_1 and \\mathcal{M}_2. Then, for large n, the distribution of the test statistics \n\\begin{aligned}\nLRT_{\\rm stat} &= 2(\\log\\ell_2(\\hat\\theta_2) - \\log\\ell_1(\\hat\\theta_1) \\\\\n&= n \\log \\left( \\frac{\\sum_{j=1}^n(y_j - f_1(x_j,\\hat{\\beta_1}))^2}{\\sum_{j=1}^n(y_j - f_2(x_j,\\hat{\\beta_2}))^2} \\right)\n\\end{aligned}\n can be approximated by a \\chi^2 distribution with p_2-p_1=d_2-d_1 df.\n\n\n4.3 Deviance\nThe deviance for a given regression model and a given set of observations y, is a measure of goodness of fit defined, in R, as:\n\nD = \\sum_{j=1}^n(y_j - f(x_j,\\hat{\\beta}))^2\n\n\n\n4.4 Information criteria\nFunctions AIC and BIC compute the Akaike information criterion and Bayesian information criterion. AIC and BIC are penalized versions of the log-likelihood defined by:\n\n\\begin{aligned}\nAIC &= -2\\log\\ell(\\hat{\\theta}) + 2P \\\\\nBIC &= -2\\log\\ell(\\hat{\\theta}) + \\log(n)P\n\\end{aligned}\n where P is the number of parameters of the model, i.e. the length of \\theta.\nOn one hand, -2\\log\\ell(\\hat{\\theta}) decreases when P increases. On the other hand, the penalization term (2P or \\log(n)P) increases with P. The objective of these criteria is to propose a model with an optimal compromise between the goodness of fit (measured by the log-likelihood) and the complexity of the model (measured by the number of parameters P).\n\n\n\n4.5 Confidence and prediction intervals\nFor given values x^{\\mathrm{new}} of the explanatory variable, we can use the fitted model for estimating the predicted response f^{\\mathrm{new}}=f(x^{\\mathrm{new}}). This estimation is defined as\n\n\\hat{f^{\\mathrm{new}}} = f(x^{\\mathrm{new}} ,\\hat{\\beta})\n\n\\hat{f^{\\mathrm{new}}} is a random variable since it is a function of the observed y. We can compute a confidence interval for f^{\\mathrm{new}} with function predict(interval = \"confidence\"), since \\hat{f^{\\mathrm{new}}}= x^{\\mathrm{new}} \\hat{\\beta}, \n\\hat{f^{\\mathrm{new}}} \\sim \\mathcal{N} (f^{\\mathrm{new}} , {\\rm Var}(\\hat{f^{\\mathrm{new}}} ) )\n\nwhere \\begin{aligned}\n{\\rm Var}(\\hat{f^{\\mathrm{new}}} ) &=  {x^{\\mathrm{new}}} {\\rm Var}(\\hat{\\beta}) {x^{\\mathrm{new}}}^\\prime \\\\\n&= \\sigma^2 x^{\\mathrm{new}}(X^\\prime X)^{-1}{x^{\\mathrm{new}}}^\\prime\n\\end{aligned}\n\\widehat{{\\rm Var}(\\hat{f^{\\mathrm{new}}})}, an estimate of {\\rm Var}(\\hat{f^{\\mathrm{new}}} ) is obtained using \\hat\\sigma^2 instead of \\sigma^2.\nThen, \n\\begin{aligned}\n\\left({{\\rm Var}(\\hat{f^{\\mathrm{new}}})} \\right)^{-1/2}(\\hat{f^{\\mathrm{new}}} - f^{\\mathrm{new}}) &\\sim \\mathcal{N}(0,  {\\rm Id}_{n^{\\mathrm{new}}} )  \\\\\n\\left(\\widehat{{\\rm Var}(\\hat{f^{\\mathrm{new}}})} \\right)^{-1/2}(\\hat{f^{\\mathrm{new}}} - f^{\\mathrm{new}}) &\\sim t_{n^{\\mathrm{new}},n-p}\n\\end{aligned}\n where t_{n^{\\mathrm{new}},n-p} is the multivariate t distribution with n-p degrees of freedom (the components of this n^{\\mathrm{new}}-vector are independent and follow a t distribution with n-p df).\nConsider now a vector of new measured values y^{\\mathrm{new}}. We can again use the predict(interval = \"prediction\") function for computing a prediction interval for y^{\\mathrm{new}}. By definition of the model, \ny^{\\mathrm{new}} \\sim \\mathcal{N} (f^{\\mathrm{new}} , \\sigma^2 \\, {\\rm Id}_{n^{\\mathrm{new}}} )\n Then, if we want to compute a prediction interval for y^{\\mathrm{new}}, we must take into account the variability of y^{\\mathrm{new}} around f^{\\mathrm{new}}, but also the uncertainty on f^{\\mathrm{new}} since it is unknown:\ny^{\\mathrm{new}} = \\hat{f^{\\mathrm{new}}} + (f^{\\mathrm{new}}-\\hat{f^{\\mathrm{new}}}) + \\varepsilon^{\\mathrm{new}}  Thus, \ny^{\\mathrm{new}} - \\hat{f^{\\mathrm{new}}} \\sim \\mathcal{N}(0, {\\rm Var}(\\hat{f^{\\mathrm{new}}}) + \\sigma^2 \\, {\\rm Id}_{n^{\\mathrm{new}}} )\n Then, \n\\begin{aligned}\n\\left(x^{\\mathrm{new}} {\\rm Var}(\\hat{\\beta}) x^{\\mathrm{new}} + {\\sigma}^2 {\\rm Id}_{n^{\\mathrm{new}}} \\right)^{-1/2}(y^{\\mathrm{new}} - \\hat{f^{\\mathrm{new}}}) & \\sim \\mathcal{N}(0,  {\\rm Id}_{n^{\\mathrm{new}}} ) \\\\\n\\left(x^{\\mathrm{new}} \\widehat{{\\rm Var}(\\hat{\\beta})} x^{\\mathrm{new}} + \\hat{\\sigma}^2 {\\rm Id}_{n^{\\mathrm{new}}} \\right)^{-1/2}(y^{\\mathrm{new}} - \\hat{f^{\\mathrm{new}}}) & \\sim t_{n^{\\mathrm{new}},n-p}\n\\end{aligned}"
  },
  {
    "objectID": "docs/regression/map566-lab-polynomial-regression.html#preliminary",
    "href": "docs/regression/map566-lab-polynomial-regression.html#preliminary",
    "title": "Polynomial Regression",
    "section": "1 Preliminary",
    "text": "1 Preliminary\nOnly functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation.\n\nlibrary(tidyverse)\nlibrary(ggfortify) # extend some ggplot2 features\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/regression/map566-lab-polynomial-regression.html#introduction",
    "href": "docs/regression/map566-lab-polynomial-regression.html#introduction",
    "title": "Polynomial Regression",
    "section": "2 Introduction",
    "text": "2 Introduction\nThe file ratWeight.csv consists of rat weights measured over 14 weeks during a subchronic toxicity study related to the question of genetically modified (GM) corn.\nWe will only consider the weight of rat B38625.\n\nrat_weight <- read_csv(\"../../data/ratWeight.csv\") %>% filter(id == 'B38625')\nrat_weight %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nBased on this data, our objective is to build a regression model of the form\ny_j = f(x_j) + e_j \\quad ; \\quad 1 \\leq j \\leq n\nWe will restrict ourselves to polynomial regression, by considering functions of the form\n\n\\begin{aligned}\nf(x) &= f(x ; c_0, c_1, c_2, \\ldots, c_d) \\\\\n&= c_0 + c_1 x + c_2 x^2 + \\ldots + c_d x^d\n\\end{aligned}"
  },
  {
    "objectID": "docs/regression/map566-lab-polynomial-regression.html#questions",
    "href": "docs/regression/map566-lab-polynomial-regression.html#questions",
    "title": "Polynomial Regression",
    "section": "3 Questions",
    "text": "3 Questions\n\nPlot the data\nFit several polynomials (degree 0, 1, 2, 3, 6, …) to this data using the lm function.\nLook at some diagnostic plots to eliminate miss-specified models.\nCompare the predictive performance of the models that have been retained.\nUse statistical tests and information criteria to compare the models and select “the best one”."
  },
  {
    "objectID": "docs/regression/map566-lecture-nonlinear-regression.html#preliminary",
    "href": "docs/regression/map566-lecture-nonlinear-regression.html#preliminary",
    "title": "Nonlinear Regression",
    "section": "Preliminary",
    "text": "Preliminary\nOnly functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. You could also try the package broom that standardizes the output of built-in R functions for statistical modelling\n\nlibrary(tidyverse)\nlibrary(parallel)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/regression/map566-lecture-nonlinear-regression.html#introduction",
    "href": "docs/regression/map566-lecture-nonlinear-regression.html#introduction",
    "title": "Nonlinear Regression",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe faithful data (provided by the R base package datasets) consist of the waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA.\nLet us see how these data look like.\n\n\nShow the code\nfaithful_plot <- \n  faithful %>% \n  ggplot() + aes(x = eruptions, y = waiting) + \n  geom_point(size=2, colour=\"#993399\") +\n  ylab(\"Waiting time to next eruption (mn)\") + xlab(\"duration of the eruption (mn)\")\nfaithful_plot\n\n\n\n\n\nWe aim to fit a model to this data that describes the relationship between duration and waiting time.\nIf we try to fit a fit a polynomial model, we can check that a polynomial of degree 4 is considered as the “best polynomial model”.\n\npoly4 <- lm(waiting ~poly(eruptions, 4), data = faithful)\n\n\nfaithful_plot <- \n  faithful_plot + \n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 4), se = FALSE, colour=\"#339900\")\nfaithful_plot\n\n\n\n\nEven if this model is the “best” polynomial model, we may have serious doubts on the capabilities of the model to predict waiting times for durations outside of the observed range of durations.\nFurthermore, parameters of the model, i.e. the polynomials’ coefficients, have no obvious physical interpretation. Using a polynomial model here, we are therefore not seeking to build a structural model f that approximates a physical phenomenon, but merely seeking to rely the variability in the observations to the explanatory variables x, x^2, …\nWe therefore need to consider other types of models, i) that do not necessarily assume linear relationships between the response variable and the explanatory variables, ii) whose parameters have some physical interpretation.\nA logistic function (or logistic curve) is a common “S” shape (sigmoid curve), with equation:\n\nf_1(x) = \\frac{A}{1+{\\rm exp}(-\\gamma(x-\\tau))}\n\n\n\nHere, A is the limiting value (when x \\to \\infty), \\gamma measure the steepness of the curve and \\tau is the x-value of the sigmoid’s midpoint.\nThis model is a nonlinear model in the sense that the regression function f_1 is a nonlinear function of the parameters.We can fit this model to our data using the nls function.\n\nnlm1 <- nls(waiting ~  A / ( 1 + exp(- gamma * (eruptions -tau))), faithful, start = c(A=70, gamma=2, tau=1))\nsummary(nlm1)\n\n\nFormula: waiting ~ A/(1 + exp(-gamma * (eruptions - tau)))\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)    \nA      93.1097     4.5080  20.654  < 2e-16 ***\ngamma   0.6394     0.1022   6.254 1.57e-09 ***\ntau     1.4623     0.1092  13.391  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.763 on 269 degrees of freedom\n\nNumber of iterations to convergence: 9 \nAchieved convergence tolerance: 6.242e-06\n\n\nWe will see in the next sections what these results are and how they are computed.\nAn extension of the logistic function assumes a minimum waiting time S between eruptions:\n\nf_2(x) =  S + \\frac{A-S}{1+{\\rm exp}(-\\gamma(x-\\tau))}\n\n\n\nWe can again use nls to fit this nonlinear model to the data:\n\nnlm2 <- nls(waiting ~ (A-S) / ( 1 + exp(-gamma * (eruptions - tau)) ) + S, faithful, start = c(A=90, gamma=2, tau=2, S=50))\nsummary(nlm2)\n\n\nFormula: waiting ~ (A - S)/(1 + exp(-gamma * (eruptions - tau))) + S\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)    \nA      82.4659     0.9973  82.689  < 2e-16 ***\ngamma   2.2539     0.4355   5.175 4.47e-07 ***\ntau     3.0553     0.1107  27.610  < 2e-16 ***\nS      51.3221     1.8303  28.040  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.622 on 268 degrees of freedom\n\nNumber of iterations to convergence: 9 \nAchieved convergence tolerance: 6.287e-06\n\n\nWe can now compute and plot the waiting times predicted with these two fitted models\n\n\nShow the code\nfaithful_plot <- faithful_plot + \n  geom_smooth(\n    method  = \"nls\",  se = FALSE, color = \"#E69F00\",\n    formula = y ~ A / ( 1 + exp(- gamma * (x-tau))), \n    method.args = list(start = c(A=70, gamma=2, tau=1))) +\n  geom_smooth(\n    method  = \"nls\",  se = FALSE, color = \"#56B4E9\", \n    formula = y ~ (A-S) / ( 1 + exp(-gamma * (x-tau)) ) + S, \n    method.args = list(start=c(A=90, gamma=2, tau=2, S=50))) \nfaithful_plot\n\n\n\n\n\nWe will see in the next sections\n\nhow to fit a nonlinear model to the data. We will use the model f_1 and show how to retrieve the results of nlm1.\nhow to evaluate the capability of the model to describe the observed data,\nhow to compare possible models,\nhow to compute confidence intervals and prediction intervals for predicted values."
  },
  {
    "objectID": "docs/regression/map566-lecture-nonlinear-regression.html#fitting-a-nonlinear-model",
    "href": "docs/regression/map566-lecture-nonlinear-regression.html#fitting-a-nonlinear-model",
    "title": "Nonlinear Regression",
    "section": "2 Fitting a nonlinear model",
    "text": "2 Fitting a nonlinear model\n\n2.1 Estimation of the parameters of the model\n\n2.1.1 Least squares estimation\nIn the model\ny_j = f(x_j,\\beta) + \\varepsilon_j \\quad ; \\quad 1\\leq j \\leq n The least squares (LS) estimator of \\beta minimizes the residual sum of squares (RSS)\n\n\\hat{\\beta} = \\arg\\min_{\\beta}\\sum_{j=1}^n (y_j - f(x_j,\\beta))^2\n\nHere, there is no closed-form expression for \\hat{\\beta}. An optimization procedure is then used for computing \\hat{\\beta}.\nWe define the first model {\\cal M}_1 as\ny_j = f_1(x_j,\\beta) + \\varepsilon_j \\quad ; \\quad 1\\leq j \\leq n\nwhere f_1 is the logistic function defined above and where \\beta = (A,\\gamma,\\tau). In R,\n\nf_1  <- function(beta, x) {\n  A <- beta[1]; gamma <- beta[2]; tau <- beta[3]\n  A / ( 1 + exp(- gamma * (x-tau)))\n}\n\nLet us check that function nls computes the nonlinear least-squares estimates of the parameters of the (nonlinear) model, that is, solve the above optimization problem in \\beta. We first create a function that computes the residual sum of squares for a given vector of parameters \\beta, which will be the objective (or cost) function from the optimization point of view:\n\nrss_1 <- function(beta, x, y) sum( (y - f_1(beta, x) )^2 ) \n\nThen the LS estimate of \\beta can be computed using nlm (nonlinear minimization) which minimizes the residuals sum of squares using a Newton-type algorithm.\n\noptim_nlm1 <- nlm(rss_1, c(A = 90, gamma = 2, tau = 2), faithful$eruptions, faithful$waiting)\nbeta_hat <- setNames(optim_nlm1$estimate, names(coef(nlm1)))\nbeta_hat\n\n         A      gamma        tau \n93.1100965  0.6393835  1.4622674 \n\n\nAssume now that the residual errors are random variables with mean 0 and variance \\sigma^2\n\\mathbb{E}[\\varepsilon_j] = 0 \\quad, \\quad \\mathbb{E}[\\varepsilon_j^2] = \\sigma^2 \\quad, \\quad 1 \\leq j \\leq n\nThen, following the approach for linear models, if \\beta is a vector of length p, there are n-p degrees of freedom,the residual error variance is defined as\n\n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{j=1}^n \\left( y_j - f(x_j,\\hat{\\beta})\\right)^2\n\nand the so-called residual standard error is\n\n\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}\n\n\nn  <- nrow(faithful)\np  <- length(beta_hat)\ndf <- (n - p)\nsigma_hat <- sqrt(rss_1(beta_hat, faithful$eruptions, faithful$waiting)/df)\nsigma_hat\n\n[1] 5.762887\n\n\n\n\n2.1.2 Maximum likelihood estimation\nLet \\varepsilon_j=\\sigma \\varepsilon_j where (\\varepsilon_j) is a sequence of independent and normally distributed random variables with mean 0 and variance 1\n\n\\varepsilon_j \\sim^{\\mathrm{iid}}  \\mathcal{N}(0, 1).\n\nWe can then rewrite the model as follows:\ny_j = f(x_j,\\beta) + \\sigma \\varepsilon_j \\quad ; \\quad 1\\leq j \\leq n\nThe maximum likelihood (ML) estimator of \\beta coincides with the least squares estimator\n\\begin{aligned}\n\\hat{\\beta} &=  \\arg\\min_{\\beta}\\sum_{j=1}^n \\left(y_j - f(x_j,\\beta)\\right)^2\n\\end{aligned}\nand the ML estimators of \\sigma^2 and \\sigma are\n\n\\hat{\\sigma}^2_{\\rm ml} = \\frac{1}{n} \\sum_{j=1}^n \\left(y_j - f(x_j,\\hat{\\beta})\\right)^2, \\qquad \\hat{\\sigma}_{\\rm ml} = \\sqrt{\\hat{\\sigma}^2_{\\rm ml}}\n\n\nsigma_hat_ML <- sqrt(rss_1(beta_hat, faithful$eruptions, faithful$waiting) / n)\nsigma_hat_ML\n\n[1] 5.731018\n\n\n\n\n\n2.2 Standard errors of the parameter estimates\nSeveral methods exist for estimating the standard errors of the parameter estimates. In particular, the nls function uses a linear approximation of the model, but a likelihood approach or a parametric bootstrap may also provide estimates of these s.e.\n\n2.2.1 Linearization approach\nAn nls object has methods for several generic functions, including vcov which computes the variance-covariance matrix of the estimated parameters \\hat{\\beta}.\n\nvcov(nlm1)\n\n               A        gamma          tau\nA     20.3217803 -0.451414983  0.432032790\ngamma -0.4514150  0.010452942 -0.008769223\ntau    0.4320328 -0.008769223  0.011923452\n\n\nThe standard errors of the estimates are then the square roots of the diagonal elements of this matrix\n\nsqrt(diag(vcov(nlm1)))\n\n        A     gamma       tau \n4.5079685 0.1022396 0.1091946 \n\n\nThe nls function linearizes the model for computing this variance-covariance matrix. Indeed, for any \\beta ``close’’ to \\hat{\\beta},\n\nf(x_j , \\beta) \\simeq f(x_j , \\hat{\\beta}) + \\nabla f(x_j , \\hat{\\beta})(\\beta - \\hat{\\beta})\n\nwhere \\nabla f(x_j , {\\beta}) is the gradient of f(x_j , {\\beta}), i.e. the row vector of the first derivatives of f(x_j ,{\\beta}) with respect to the d components of \\beta. Setting z_j=y_j-f(x_j , \\hat{\\beta}) + \\nabla f(x_j , \\hat{\\beta}) \\hat{\\beta} and g_j=\\nabla f(x_j , \\hat{\\beta})), the original model can be approximated by the linear model\n\nz_j = g_j \\ \\beta + \\varepsilon_j.\n\nWriting this model in the matrix form z=G\\, \\beta + \\varepsilon where g_j is the jth row of matrix G, we can check that the LS estimator of \\beta for this model is the LS estimator of the original model \\hat{\\beta}\n\nProposition 1 (Equivalence of the two LS estimate) The LS estimator of the linearized model is equivalent to the LS estimator of the original model:\n\n\\begin{aligned}\n\\hat{\\beta} &=  \\arg\\min_{\\beta}\\sum_{j=1}^n(y_j - f(x_j,\\beta))^2 \\\\\n& =  \\arg\\min{\\beta}\\sum_{j=1}^n(z_j - g_j\\beta)^2 \\\\\n\\end{aligned}\n\n\n\nLet \\tilde{\\beta} be the LS estimator of the linearized model. Then,\n\\begin{aligned}\n\\tilde{\\beta} &=  \\arg\\min{\\beta} \\, \\left\\|z - G\\beta \\right\\|^2 \\\\\n&= (G^\\prime G)^{-1}G^\\prime z \\\\\n&= (G^\\prime G)^{-1}G^\\prime (y - f(x,\\hat\\beta) + G\\hat\\beta) \\\\\n&= \\hat\\beta + (G^\\prime G)^{-1}\\nabla f(x,\\hat\\beta) (y - f(x,\\hat\\beta) ) \\\\\n\\end{aligned}\nBy definition, \\hat\\beta minimizes U(\\beta) = \\| y -f(x,\\beta) \\|^2. Then,\n\n\\nabla U(\\hat\\beta) = -2\\nabla f(x,\\hat\\beta) (y - f(x,\\hat\\beta) )= 0\n\nThus, \\tilde\\beta=\\hat\\beta \\Box.\n\nLet us check this property numerically:\n\nhx <- deriv(\n  expr    = y ~ A / ( 1 + exp(- gamma * (x - tau))), \n  namevec = c(\"A\", \"gamma\", \"tau\"), \n  function.arg = function(A, gamma, tau, x) { }\n) \nfr <- hx(beta_hat[1], beta_hat[2], beta_hat[3], faithful$eruptions)\nG  <- attr(fr, \"gradient\")\nz  <- faithful$waiting - f_1(beta_hat, faithful$eruptions) + G %*% beta_hat\nsolve(crossprod(G)) %*% crossprod(G,z)\n\n           [,1]\nA     93.110121\ngamma  0.639383\ntau    1.462268\n\n\nSince \\hat{\\beta} =(G^\\prime G)^{-1}G^\\prime z, the variance-covariance of \\hat\\beta can be approximated by\n\\mathbb{V}_{\\rm lin}(\\hat\\beta) = \\hat\\sigma^2 (G^\\prime G)^{-1} \n\nV_lin <- sigma_hat^2*solve(t(G)%*%G)\nV_lin\n\n               A        gamma          tau\nA     20.3230488 -0.451427057  0.432076372\ngamma -0.4514271  0.010452837 -0.008769862\ntau    0.4320764 -0.008769862  0.011924743\n\n\nand we can derive standard errors ({\\rm se}_{\\rm lin}(\\hat{\\beta}_k), 1 \\leq k \\leq p) for the parameter estimates,\n\nse_lin <- sqrt(diag(V_lin))\nse_lin\n\n        A     gamma       tau \n4.5081092 0.1022391 0.1092005 \n\n\n\n\n2.2.2 Maximum likelihood approach\nLet I_y(\\hat{\\beta}) be the observed Fisher information matrix at \\hat{\\beta}:\n\n\\begin{aligned}\nI_y({\\hat\\beta}) &= - \\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\log \\ell(\\hat{\\beta},\\hat{\\sigma}^2) \\\\\n&= \\frac{1}{2\\hat\\sigma^2}\\frac{\\partial^2}{\\partial \\beta \\partial \\beta^\\prime} \\left(\\sum_{j=1}^n(y_j - f(x_j,\\hat\\beta))^2 \\right).\n\\end{aligned}\n\nThen, the Central Limit Theorem states that the variance of \\hat{\\beta} can be approximated by the inverse of I_y(\\hat{\\beta}):\n\\mathbb{V}_{\\rm ml}(\\hat{\\beta}) = I_y(\\hat{\\beta})^{-1}. \nFunction nlm can return the Hessian of the function rss_1 to minimize, i.e. the matrix of the second derivatives \\partial^2/\\partial \\beta \\partial \\beta^\\prime \\sum_{j=1}^n(y_j - f(x_j,\\hat\\beta))^2\n\noptim_nlm1 <- nlm(rss_1, c(90, 2, 2), faithful$eruptions, faithful$waiting, hessian = \"true\")\nH <- optim_nlm1$hessian\nH\n\n           [,1]      [,2]        [,3]\n[1,]   324.8939   10848.5   -3794.477\n[2,] 10848.4973  379318.3 -114782.339\n[3,] -3794.4768 -114782.3   58845.489\n\n\nWe then derive the FIM and the variance \\mathbb{V}_{\\rm ml}(\\hat{\\beta}): ::: {.cell hash=‘map566-lecture-nonlinear-regression_cache/html/unnamed-chunk-32_9960d090e32a447dba7855f3df6146f8’}\nV_ml <- solve(H/(2*sigma_hat_ML^2))\nV_ml\n\n           [,1]         [,2]         [,3]\n[1,] 17.4344838 -0.386666087  0.369991073\n[2,] -0.3866661  0.008998208 -0.007381367\n[3,]  0.3699911 -0.007381367  0.010576191\n\n:::\nand\n\nse_ml <- sqrt(diag(V_ml))\nse_ml\n\n[1] 4.17546210 0.09485888 0.10284061\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeside, using the fact that \\mathbb{V}(\\chi^2(k))=2k, we can show that \\mathbb{V}(\\hat{\\sigma}^2) \\approx 2\\sigma^4/n. Then the standard error of \\hat{\\sigma} is approximately \\hat{\\sigma}/\\sqrt{2n}.\n\nsigma_hat/sqrt(2*n)\n\n[1] 0.2470817\n\n\n\n\n2.2.3 Parametric bootstrap\nIf we were able to repeat the same experiment under the same conditions, we would observe y^{(1)} = (y^{(1)}_j, 1\\leq j \\leq n) and we would compute \\beta^{(1)}, an estimate of \\beta. Then, if we could repeat this experiment L times, we would get L estimates \\beta^{(1)}, \\beta^{(2)}, , \\beta^{(L)} of \\beta. This sequence of estimates (\\beta^{(\\ell)}, 1 \\leq \\ell \\leq L) would be a sample of random variables distributed as \\hat{\\beta} and could therefore be used for estimating this distribution.\nWhen such replicates are not available, parametric bootstrap (or Monte-Carlo simulation) is a way to mimic the repetition of an experiment.\nFor \\ell=1,2,\\ldots, L, we generate “observations” (y^{(\\ell)}_j, 1\\leq j \\leq n) with the model of interest, the original explanatory variables (x_j, 1\\leq j\\leq n) and using the estimated parameters \\hat\\theta=(\\hat\\beta,\\hat{\\sigma}^2):\n\ny^{(\\ell)}_j = f(x_j,\\hat{\\beta}) + \\hat{\\sigma} \\varepsilon^{(\\ell)}_j \\quad ; \\quad 1\\leq j \\leq n\n\nwhere \\varepsilon^{(\\ell)}_j \\sim^{\\mathrm{iid}} \\mathcal{N}(0,1). We also compute the LS /ML estimate of \\beta:\n\\hat\\beta^{(\\ell)} = \\arg\\min_{\\beta}\\sum_{j=1}^n(y^{(\\ell)}_j - f(x_j,\\beta))^2\nThe variance-covariance matrix of \\hat\\beta is then estimated by the empirical variance-covariance matrix of (\\hat\\beta^{(\\ell)}):\n \\mathbb{V}_{\\rm mc}(\\hat{\\beta}) = \\frac{1}{L-1}\\sum_{\\ell=1}^L ( \\hat\\beta^{(\\ell)} - \\bar{\\beta} )( \\hat\\beta^{(\\ell)} - \\bar{ \\beta} )^\\prime\n\nwhere \\bar{ \\beta} = 1/L \\sum_{\\ell=1}^L \\hat\\beta^{(\\ell)}.\n\nL <- 1000\ny_hat_ref <- predict(nlm1)\nbeta_hat <- coef(nlm1)\nx <- faithful$eruptions\nbetas_boot <- parallel::mclapply(1:L, function(b) {\n  y_b <- y_hat_ref + sigma_hat * rnorm(length(y_hat_ref))\n  coef(suppressWarnings(nls(y_b ~  f_1(beta, x), start = list(beta = beta_hat))))\n}) %>% unlist() %>% matrix(ncol = L) %>% t()\nV_mc <- cov(betas_boot)\nV_mc\n\n           [,1]        [,2]        [,3]\n[1,] 31.8509888 -0.54377839  0.90228021\n[2,] -0.5437784  0.01107508 -0.01310615\n[3,]  0.9022802 -0.01310615  0.02981784\n\n\n\nse_mc <- sqrt(diag(V_mc))\nse_mc\n\n[1] 5.6436680 0.1052382 0.1726784\n\n\n\nRemark. It would be equivalent to directly compute the empirical standard deviation of each component of the sequence (\\hat\\beta^{(\\ell)}):\n\napply(betas_boot, 2, sd)\n\n[1] 5.6436680 0.1052382 0.1726784\n\n\n\nOne of the main advantages of this method is that it doesn’t make any assumption on \\hat\\beta, contrary to the maximum likelihood estimator which asymptotic distribution is known to be normal, with a known asymptotic variance. Then, this asymptotic distribution is used with a finite set of observations for approximating the distribution of the ML estimator, but without knowing how good this approximation is. On its part, the linearization approach makes use of an approximation of the structural model, without knowing how good this approximation is.\nIn this example, the ML estimator seems to underestimate the standard error of the estimates. On the other hand, results obtained with the linearization approach are very similar to those obtained by Monte Carlo simulation.\n\n\n\n2.3 Statistical tests for the model parameters\nThe summary of model nlm1 includes several informations about the model parameters:\n\nsummary(nlm1)$coefficient\n\n        Estimate Std. Error   t value     Pr(>|t|)\nA     93.1097358  4.5079685 20.654478 1.974771e-57\ngamma  0.6393916  0.1022396  6.253853 1.567065e-09\ntau    1.4622598  0.1091946 13.391324 1.110090e-31\n\n\nLet \\beta = (\\beta_k, 1\\leq k \\leq p) be the p-vector of parameters of the model. In the linearized model z=G\\beta+e,\nt_k = (\\hat{\\beta}_k - \\beta_k)/{\\rm se}(\\hat{\\beta}_k) follows a t-distribution with n-p degrees of freedom. We can then perform a t-test to test if \\beta_k=0.\nThe test statistics is t_{{\\rm stat}, k} = {\\hat{\\beta}_k}/{{\\rm se}(\\hat{\\beta}_k)} and the p-value for this test is\np_k = 2(1 - \\mathbb{P}(T_{n-d} \\leq |t_{{\\rm stat}, k}| ) \n\nt_stat  <- beta_hat/se_lin\np_value <- 2*(1 - pt(abs(t_stat), n-p))\ncbind(beta_hat, se_lin, t_stat, p_value) %>% round(4)\n\n      beta_hat se_lin  t_stat p_value\nA      93.1097 4.5081 20.6538       0\ngamma   0.6394 0.1022  6.2539       0\ntau     1.4623 0.1092 13.3906       0\n\n\n\n\n2.4 Confidence intervals for the model parameters\n\n2.4.1 Linearization approach\nUsing the linearized model z = G\\beta + \\varepsilon, we can compute a confidence interval for each component of \\beta as we do with any linear model:\n{\\rm CI}_{{\\rm lin}, 1-\\alpha}(\\beta_k) = [\\hat{\\beta}_k + qt_{\\alpha/2, n-p}\\ {\\rm se}_{\\rm lin}(\\hat{\\beta}_k) \\ , \\ \\hat{\\beta}_k + qt_{1-\\alpha/2, n-p}\\ {\\rm se}_{\\rm lin}(\\hat{\\beta}_k)]\nwhere qt_{p,\\nu} is the quantile of order p for a t-distribution with \\nu degree of freedom.\n\nlevel <- 0.95\nalpha <- 1 - level\nCI_linearized <- \n  cbind(\n  beta_hat + qt(alpha/2, n-p) * se_lin,\n  beta_hat + qt(1-alpha/2,n-p) * se_lin) %>% as.data.frame() %>%  \n  setNames(c(paste0((1-level)/2*100,\"%\"),paste0((1+level)/2*100,\"%\")))\nCI_linearized\n\n           2.5%       97.5%\nA     84.234071 101.9854003\ngamma  0.438101   0.8406823\ntau    1.247263   1.6772561\n\n\n\n\n2.4.2 Maximum likelihood approach\nWe can adopt the same approach with the ML estimate. Here, the standard errors ({\\rm se}_{\\rm ml}(\\hat{\\beta}_k), 1 \\leq k \\leq p) are derived from the asymptotic variance-covariance matrix of the parameter estimates V_{\\rm ml}(\\hat{\\beta}) .\n{\\rm CI}_{{\\rm ml},1-\\alpha}(\\beta_k) = [\\hat{\\beta}_k + qt_{\\alpha/2, n-p}\\ {\\rm se}_{\\rm ml}(\\hat{\\beta}_k) \\ , \\ \\hat{\\beta}_k + qt_{1-\\alpha/2, n-p}\\ {\\rm se}_{\\rm ml}(\\hat{\\beta}_k)]\n\nCI_ML <- \n  cbind(\n    beta_hat + qt(alpha/2,n-p) * se_ml,\n    beta_hat + qt(1-alpha/2,n-p) * se_ml) %>% as.data.frame() %>%  \n  setNames(c(paste0((1-level)/2*100,\"%\"),paste0((1+level)/2*100,\"%\")))\nCI_ML\n\n            2.5%       97.5%\nA     84.8889942 101.3304773\ngamma  0.4526314   0.8261519\ntau    1.2597849   1.6647346\n\n\n\n\n2.4.3 Parametric bootstrap\nThe sequence (\\hat{\\beta}^{(\\ell)}, 1 \\leq \\ell \\leq L) obtained by Monte Carlo simulation can be used for computing an empirical confidence interval:\n{\\rm CI}_{{\\rm mc},1-\\alpha}(\\beta_k) = [\\hat{\\beta}_{k,\\alpha/2} \\ , \\ \\hat{\\beta}_{k,1-\\alpha/2} ]   where, for any 0 < p < 1, \\hat{\\beta}_{k,p} is the empirical quantile of order p of (\\hat{\\beta}^{(\\ell)}_k, 1 \\leq \\ell \\leq L):\n\nCI_bootstrap <- \n  apply(as.matrix(betas_boot), 2, quantile, probs = c(alpha/2, 1-alpha/2)) %>%\n  t() %>% as.data.frame() %>% \n  setNames(c(paste0((1-level)/2*100,\"%\"),paste0((1+level)/2*100,\"%\")))\nCI_bootstrap\n\n       2.5%       97.5%\n1 86.564633 107.7469384\n2  0.445539   0.8522095\n3  1.325143   1.9079910\n\n\n\nRemark. These confidence intervals are slightly biased. We will use a linear model to explain where this bias comes from and show how to remove it.\nConsider the linear model y = X\\beta + \\sigma\\varepsilon. A confidence interval of level 1-\\alpha for \\beta_k is\n{\\rm CI}_{ 1-\\alpha}(\\beta_k) = [\\hat{\\beta}_k + qt_{\\alpha/2, n-d}\\ {\\rm se}_(\\hat{\\beta}_k) \\ , \\ \\hat{\\beta}_k + qt_{1-\\alpha/2, n-d}\\ {\\rm se}(\\hat{\\beta}_k)]\nwhere {\\rm se}(\\hat{\\beta}_k) = \\hat\\sigma^2 (X^\\prime X)^{-1}_{kk}.\nOn the other hand, for \\ell=1,2,\\ldots,L,\ny^{(\\ell)} = X\\hat{\\beta} + \\hat{\\sigma} \\varepsilon^{(\\ell)} \nand\n\\hat{\\beta}^{(\\ell)} = \\hat\\beta + \\hat{\\sigma}(X^\\prime X)^{-1}X^\\prime\\varepsilon^{(\\ell)}\nThus, conditionnally to the observations y, i.e. conditionnally to \\hat\\beta, \\hat{\\beta}_k^{(\\ell)} \\sim \\mathcal{N}(\\hat\\beta_k \\ , \\ {\\rm se}^2(\\hat{\\beta}_k)). Then, the empirical quantile \\hat{\\beta}_{k,p} is an estimator of the quantile of order p of a normal distribution with mean \\hat\\beta_k and variance {\\rm se}^2(\\hat{\\beta}_k). In other words, the confidence interval {\\rm CI}_{{\\rm mc},1-\\alpha}(\\beta_k) is an estimator of the interval [\\hat{\\beta}_k + q\\mathcal{N}_{\\alpha/2}\\ {\\rm se}_(\\hat{\\beta}_k) \\ , \\ \\hat{\\beta}_k + q\\mathcal{N}_{1-\\alpha/2}\\ {\\rm se}(\\hat{\\beta}_k)], where q\\mathcal{N}_p is the quantile of order p for a \\mathcal{N}(0,1) distribution.\nWe see that these quantiles for a normal distribution should be tranformed into quantiles for a t-ditribution with n-d df.\nAn unbiased confidence interval for \\beta_k is therefore\n{\\rm CI}^\\star_{{\\rm mc},1-\\alpha}(\\beta_k) = [\\hat{\\beta}_k + \\frac{qt_{\\alpha/2, n-d}}{q\\mathcal{N}_{\\alpha/2}}(\\hat{\\beta}_{k,\\alpha/2} - \\hat{\\beta}_k)\\ , \\  \\hat{\\beta}_k + \\frac{qt_{1-\\alpha/2, n-d}}{q\\mathcal{N}_{1-\\alpha/2}}(\\hat{\\beta}_{k,1-\\alpha/2} - \\hat{\\beta}_k)]  \nThe same correction can be used for nonlinear models:\n\nrq <- qt(1-alpha/2,df)/qnorm(1-alpha/2)\nbeta_hat + rq*(CI_bootstrap - beta_hat)\n\n        2.5%       97.5%\n1 86.5350526 107.8130908\n2  0.4446629   0.8531713\n3  1.3245230   1.9100055\n\n\n\n\n\n2.4.4 Profile likelihood\nFunction confint uses the profile likelihood method for computing confidence intervals for parameters in a fitted model.\n\nCI_profiled <- confint(nlm1, level = level)\nCI_profiled\n\n            2.5%       97.5%\nA     87.1321726 105.5760368\ngamma  0.4625255   0.8324095\ntau    1.3109469   1.8569351\n\n\nProfile likelihood confidence intervals are based on the log-likelihood function.\nImagine that we want to compute a confidence interval for \\beta_1. The profile likelihood of \\beta_1 is defined by\n\\ell_p(\\beta_1) = \\max_{\\beta_2, \\ldots, \\beta_d}\\ell(\\beta_1, \\beta_2, \\ldots, \\beta_d )\n\\ell_p(\\beta_1) does no longer depend on \\beta_2, \\ldots, \\beta_d since it has been profiled out.\nAs an example, let us compute and display the profile log-likelihood of A for model nlm1\n\nf_1A <- function(gamma,tau,x,A){A/(1+exp(-gamma*(x-tau)))}\nvalues_A <- seq(86, 110, by = 0.1)\nstart    <- list(gamma = beta_hat[2], tau = beta_hat[3])\nlogLik_A <- map(values_A, ~\n  nls(waiting ~  .x / ( 1 + exp(- gamma * (eruptions - tau))), faithful, start = start)\n) %>% map(logLik) %>% map_dbl(as.numeric)\n\n\n\nShow the code\ndata.frame(A = values_A, logLik = logLik_A) %>% \nggplot() + aes(x = A, y = logLik) + geom_line(color=\"blue\", size=1) + \n  geom_vline(xintercept = beta_hat[1], color=\"red\", linetype = \"longdash\") + \n  scale_x_continuous(breaks = c(seq(85,110, by = 5), round(beta_hat[[1]],2)),\"A\")\n\n\n\n\n\nConsider the test of H_0: \\beta_1 = \\beta_1^\\star against H_1: \\beta_1 \\neq \\beta_1^\\star. The likelihood ratio statistics is\nLR_{\\rm stat} = 2\\left(\\log(\\ell(\\hat\\beta)) - \\log(\\ell_p(\\beta_1^\\star))\\right)\nwhere \\hat\\beta is the value of \\beta that maximises the likelihood \\ell(\\beta) under H_1.\nUnder H_0, LR_{\\rm stat} follows a \\chi^2 distribution with 1 df. Then, the test is significant (i.e. we reject H_0), if LR_{\\rm stat}> q\\chi^2_{1,1-\\alpha} where q\\chi^2_{1,1-\\alpha} is the quantile of order 1-\\alpha for a \\chi^2 distribution with 1 df.\nA “profile likelihood confidence interval” of level 1-\\alpha for \\beta_1 consists of those values \\beta_1^\\star for which the test is not significant.\n\n\nqlevel <- qchisq(level,1)\n\nlp_A <- function(A, qlevel) {\n  nlm1_A <- nls(waiting ~  A / ( 1 + exp(- gamma * (eruptions - tau))), faithful, \n      start = list(gamma = beta_hat[2], tau = beta_hat[3]))\n  res <- as.numeric(2*(logLik(nlm1) - logLik(nlm1_A) ) - qlevel) \n  res\n}\n\nc1_A <- uniroot(lp_A, c(80,  beta_hat[1]), qlevel)$root\nc2_A <- uniroot(lp_A, c(beta_hat[1], 110), qlevel)$root\n\n\n\nShow the code\ndlogLik_A <- 2*(as.numeric(logLik(nlm1)) - logLik_A)\ndata.frame(A = values_A, dlogLik = dlogLik_A) %>% \n  ggplot() + geom_line(aes(values_A,dlogLik), size=1, color=\"blue\") + \n  geom_hline(yintercept=qlevel, color=\"red\", linetype = \"longdash\") +\n  geom_segment(aes(x = c1_A, xend = c1_A, y=-Inf, yend=qlevel),  color=\"red\", linetype = \"longdash\")  +\n  geom_segment(aes(x = c2_A, xend = c2_A, y=-Inf, yend=qlevel),  color=\"red\", linetype = \"longdash\")  +\n  scale_y_continuous(breaks = c(0,2,3,6,round(qlevel,2)),1) +\n  scale_x_continuous(breaks = c(seq(85,100,by=5),round(c1_A,1),round(c2_A,1), 110),\"A\") \n\n\n\n\n\nLet us now compute the profile likelihood confidence intervals for \\gamma and \\tau\n\nlp_gamma <- function(gamma) {\n  nlm1_gamma <- nls(waiting ~  A/(1 + exp(-gamma*(eruptions-tau))), faithful,\n                    start = list(A=beta_hat[1],tau=beta_hat[3]))\n  as.numeric(2*(logLik(nlm1) - logLik(nlm1_gamma) ) - qlevel)\n}\nc1_gamma <- uniroot(lp_gamma, lower = 0.40, upper = beta_hat[2])$root\nc2_gamma <- uniroot(lp_gamma, lower = beta_hat[2], upper=1)$root\n\nlp_tau <- function(tau) {\n  nlm1_tau <- nls(waiting ~ A/(1+exp(-gamma*(eruptions - tau))), faithful,\n                  start=list(A=beta_hat[1],gamma=beta_hat[2]))\n  as.numeric(2*(logLik(nlm1) - logLik(nlm1_tau) ) - qlevel)\n}\nc1_tau <- uniroot(lp_tau, lower=1.25, upper=beta_hat[3])$root\nc2_tau <- uniroot(lp_tau, lower=beta_hat[3], upper=2.25)$root\n\nCI_profiled_custom <- \n  rbind(A = c(c1_A , c2_A), gamma = c(c1_gamma, c2_gamma), tau = c(c1_tau, c2_tau)) %>% \n  as.data.frame() %>% \n  setNames(c(paste0((1-level)/2*100,\"%\"),paste0((1+level)/2*100,\"%\")))\nCI_profiled_custom\n\n            2.5%       97.5%\nA     87.1523081 105.4511191\ngamma  0.4636369   0.8310879\ntau    1.3118040   1.8520538\n\n\n\nRemark. The confint R function doesn’t use a \\chi^2 distribution with 1 df for the LRT statistics LR_{\\rm stat} (which is theoretically the right asymptotic distribution).\nOn the contrary, the square root of LR_{\\rm stat} is assumed to follow a half t-distribution with n-p df. Then, the null hypothesis H_0 is rejected when LR_{\\rm stat}> qt_{1-\\alpha/2,n-p}^2.\n\nqlevel <- qt(1-alpha/2,df)^2\nc1R_A <- uniroot(lp_A, c(80,  beta_hat[1]), qlevel)$root\nc2R_A <- uniroot(lp_A, c(beta_hat[1], 110), qlevel)$root\nc(c1R_A, c2R_A)\n\n[1]  87.13229 105.53713\n\n\nThe two tests - and then the two confidence intervals - are equivalent for large n since a t-distribution with n df converges to a \\mathcal{N}(0,1) when n goes to infinity. Then, for any 0 < p < 1,\n (qt_{p,n})^2 \\xrightarrow{n\\to \\infty} q\\chi^2_{p,1}"
  },
  {
    "objectID": "docs/regression/map566-lecture-nonlinear-regression.html#diagnostic-plots",
    "href": "docs/regression/map566-lecture-nonlinear-regression.html#diagnostic-plots",
    "title": "Nonlinear Regression",
    "section": "3 Diagnostic plots",
    "text": "3 Diagnostic plots\nLet us plot\n\nthe observed waiting times versus predicted waiting times,\nthe residuals versus eruption times,\nthe residuals versus predicted waiting times,\nthe distribution of the residuals\n\n\n\nShow the code\n## unfortunately, no plotting function is defined for 'nls' object\nresidual_nlm1 <- resid(nlm1)/sd(resid(nlm1))\npar(mfrow=c(2,2))\nplot(predict(nlm1), faithful$waiting)\nabline(a=0, b=1, lty=1, col=\"magenta\")\nplot(x, residual_nlm1)\nabline(a=0, b=0, lty=1, col=\"magenta\")\nplot(predict(nlm1), residual_nlm1)\nabline(a=0, b=0, lty=1, col=\"magenta\")\nboxplot(residual_nlm1)\nabline(a=0, b=0, lty=2, col=\"magenta\")\nabline(a=qnorm(0.25), b=0, lty=2, col=\"magenta\")\nabline(a=qnorm(0.75), b=0, lty=2, col=\"magenta\")\n\n\n\n\n\nOn one hand, observations and predictions look well randomly distributed around the line y=x. On the other hand, residual look well distributed around 0, with a constant variance. Furthermore, the distribution of the residuals appears to be symmetrical with quantiles close to those of a normal distribution\nThen, based on these graphs, we don’t have any good reason for rejecting model nlm1… which doesn’t mean we should stay with this model as our final model!"
  },
  {
    "objectID": "docs/regression/map566-lecture-nonlinear-regression.html#model-comparison",
    "href": "docs/regression/map566-lecture-nonlinear-regression.html#model-comparison",
    "title": "Nonlinear Regression",
    "section": "4 Model comparison",
    "text": "4 Model comparison\nWe can produce the same the diagnostic plots with model nlm2 and arrive at the same conclusion concerning this model.\n\n\nShow the code\nresidual_nlm2 <- resid(nlm2)/sd(resid(nlm2))\npar(mfrow=c(2,2))\nplot(predict(nlm2), faithful$waiting)\nabline(a=0, b=1, lty=1, col=\"magenta\")\nplot(x, residual_nlm2)\nabline(a=0, b=0, lty=1, col=\"magenta\")\nplot(predict(nlm2), residual_nlm2)\nabline(a=0, b=0, lty=1, col=\"magenta\")\nboxplot(residual_nlm2)\nabline(a=0, b=0, lty=2, col=\"magenta\")\nabline(a=qnorm(0.25), b=0, lty=2, col=\"magenta\")\nabline(a=qnorm(0.75), b=0, lty=2, col=\"magenta\")\n\n\n\n\n\nSince nlm1 and nlm2 are two possible model for fitting our data, we need some criteria for comparing them. The statistical tests and the information criteria used for comparing linear models can also be used for comparing nonlinear models.\nFirst, we can perform a ANOVA for testing model nlm1 against model nlm2 since these two models are nested (nlm1 correponds to nlm2 when S=0)\n\nanova(nlm1, nlm2)\n\nAnalysis of Variance Table\n\nModel 1: waiting ~ A/(1 + exp(-gamma * (eruptions - tau)))\nModel 2: waiting ~ (A - S)/(1 + exp(-gamma * (eruptions - tau))) + S\n  Res.Df Res.Sum Sq Df Sum Sq F value    Pr(>F)    \n1    269     8933.7                                \n2    268     8469.4  1  464.3  14.692 0.0001578 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet RSS_1 = \\|y- f_1(x,\\hat\\beta_1)\\|^2 and RSS_2 = \\|y- f_2(x,\\hat\\beta_2)\\|^2 be the residual sums of squares under, respectively, nlm1 and nlm2, and let d_1 and d_2 be lengths of vectors \\beta_1 and \\beta_2. Then,\n\nF_{\\rm stat} = \\frac{(RSS_1 - RSS_2)/(d_2-d_1)}{(RSS_2)/(n-d_2}\n\n\nRSS1 <- sum(resid(nlm1)^2)\nRSS2 <- sum(resid(nlm2)^2)\np1   <- length(coef(nlm1))\np2   <- length(coef(nlm2))\nF.stat <- ( (RSS1-RSS2)/(p2-p1) ) / ( RSS2/(n-p2) )\nc(RSS2, RSS1 - RSS2, F.stat, 1-pf(F.stat, p2-p1, n-p2))\n\n[1] 8.469424e+03 4.642987e+02 1.469192e+01 1.577976e-04\n\n\nRemark: since the model is nonlinear, we cannot decompose the residual sum of squares RSS_1 as we did with linear models. Indeed, here,\n\\|y- f_1(x,\\hat\\beta_1)\\|^2 \\neq \\|f_1(x,\\beta_1) - f_2(x,\\hat\\beta_2)\\|^2 + \\|y- f_2(x,\\hat\\beta_2)\\|^2\n\nc(RSS1-RSS2, sum((predict(nlm1)-predict(nlm2))^2))\n\n[1] 464.2987 520.3557\n\n\nAnother way to test nlm1 against nlm2 consists in testing if S=0 in model nlm2:\n\nsummary(nlm2)$coefficients\n\n       Estimate Std. Error   t value      Pr(>|t|)\nA     82.465891  0.9973074 82.688541 8.974188e-193\ngamma  2.253936  0.4355265  5.175198  4.469534e-07\ntau    3.055263  0.1106570 27.610217  2.420406e-80\nS     51.322076  1.8302946 28.040336  1.110849e-81\n\n\nEven if both tests clearly prefer model nlm2, we can remark that the t-test and the F-test are not equivalent since the models are nonlinear.\nInformation criteria such as AIC and BIC also prefer model nlm2:\n\nas.matrix(AIC(nlm1, nlm2))\n\n     df      AIC\nnlm1  4 1729.668\nnlm2  5 1717.152\n\nas.matrix(BIC(nlm1, nlm2))\n\n     df      BIC\nnlm1  4 1744.092\nnlm2  5 1735.181"
  },
  {
    "objectID": "docs/regression/map566-lecture-nonlinear-regression.html#confidence-intervals-and-prediction-intervals",
    "href": "docs/regression/map566-lecture-nonlinear-regression.html#confidence-intervals-and-prediction-intervals",
    "title": "Nonlinear Regression",
    "section": "5 Confidence intervals and prediction intervals",
    "text": "5 Confidence intervals and prediction intervals\nThere is no ready-made functions to calculate confidence intervals for predicted values and prediction intervals for new data. We will see how to do it by implementing two different methods.\n\n5.1 The delta-method\nFor a given value x_0 of the explanatory variable x, we can use the model f with the estimated parameter \\hat{\\beta} and predict the response as f(x_0,\\hat{\\beta}).\nSince \\hat{\\beta} is a random vector with variance-covariance matrix \\mathbb{V}(\\hat{\\beta}), f(x_0,\\hat{\\beta}) is also a random variable that can be approximated by a linear function of \\hat{\\beta}\n\nf(x_0 , \\beta) \\simeq f(x_0 , \\hat{\\beta}) + \\nabla f(x_0 , \\hat{\\beta})(\\beta - \\hat{\\beta})\n\nThen, the so-called delta-method consists in using this approximation for approximating the variance of f(x_0,\\hat{\\beta}) by\n\n\\mathbb{V}(f(x_0,\\hat{\\beta})) \\simeq \\nabla f(x_0 , \\hat{\\beta}) \\mathbb{V}(\\hat{\\beta}) \\nabla f(x_0 , \\hat{\\beta})^\\prime,\n\nand we can now use this approximation for computing a (1-\\alpha)100\\% confidence interval for each prediction f(x_0,\\beta):\n{\\rm CI}_{{\\rm lin}, 1-\\alpha}= [f(x_0,\\hat{\\beta}) + qt_{\\alpha/2, n-p}\\ {\\rm s.e.}(f(x_0,\\hat{\\beta})) \\ , \\ f(x_0,\\hat{\\beta}) + qt_{1-\\alpha/2, n-p}\\ {\\rm s.e.}(f(x_0,\\hat{\\beta}))]\nwhere {\\rm s.e.}(f(x_0,\\hat{\\beta})) is the standard error of f(x_0,\\hat{\\beta}) defined as\n\n{\\rm s.e.}(f(x_0,\\hat{\\beta})) = \\sqrt{\\nabla f(x_0 , \\hat{\\beta}) \\mathbb{V}(\\hat{\\beta}) \\nabla f(x_0 , \\hat{\\beta})^\\prime}\n\nWe can also compute a prediction interval for a future observation y_0 = f(x_0,\\beta) + \\varepsilon_0\nThe prediction for y_0 is\n\\hat{y}_0 = f(x_0 , \\hat{\\beta}).\nThen, the standard error for this prediction should take into account the uncertainty on f(x_0,\\beta) and the variability of the residual error e_0:\n\n{\\rm s.e.}(\\hat{y}_0) = \\sqrt{\\nabla f(x_0 , \\hat{\\beta}) \\mathbb{V}(\\hat{\\beta}) \\nabla f(x_0 , \\hat{\\beta})^\\prime + \\sigma^2}\n\nThen,\n{\\rm CI}_{{\\rm lin}, 1-\\alpha}(y_0) = [f(x_0,\\hat{\\beta}) + qt_{\\alpha/2, n-p}\\ {\\rm s.e.}(\\hat{y}_0) \\ , \\ f(x_0,\\hat{\\beta}) + qt_{1-\\alpha/2, n-p}\\ {\\rm s.e.}(\\hat{y}_0)]\nAs an example, let us compute the variance of f_2(x_0, \\hat{\\beta}_2) for x_0 = 1, 1.1, 1.2, \\ldots, 5.9, 6,\n\nf_prime <- deriv(y ~ (A-S)/(1+exp(-gamma*(x-tau))) + S, c(\"A\", \"gamma\", \"tau\", \"S\"), function(A, gamma, tau, S, x){} ) \nx_new <- seq(1, 6, by=0.1)\nbeta_hat <- coef(nlm2)\nf_new    <- f_prime(beta_hat[1], beta_hat[2], beta_hat[3], beta_hat[4], x_new)\ngrad_new <- attr(f_new, \"gradient\")\nGS <- rowSums((grad_new %*% vcov(nlm2)) * grad_new)\n\nWe can then derive a 95\\% confidence interval for each f_2(x_0,\\beta)\n\nalpha <- 0.05\ndelta_f <- sqrt(GS) * qt(1-alpha/2,  n - length(beta_hat))\ndf_delta <- data.frame(\n  x = x_new, \n  f = f_new, \n  lwr_conf = f_new - delta_f, \n  upr_conf = f_new + delta_f\n  )\n\nand for each y_0\n\ndelta_y    <- sqrt(GS + sigma(nlm2)^2)*qt(1-alpha/2,df)\ndf_delta[c(\"lwr_pred\",\"upr_pred\")] <- cbind(f_new - delta_y,f_new + delta_y)\n\nWe can now plot these two intervals together with the data:\n\n\nShow the code\nggplot(faithful)  + geom_point(aes(x = eruptions, y = waiting)) +\n   geom_ribbon(data=df_delta, aes(x=x_new, ymin=lwr_pred, ymax=upr_pred), alpha=0.1, fill=\"blue\") + \n  geom_ribbon(data=df_delta, aes(x=x_new, ymin=lwr_conf, ymax=upr_conf), alpha=0.2, fill=\"#339900\") +   geom_line(data=df_delta, aes(x=x_new, y=f_new), colour=\"#339900\", size=1)\n\n\n\n\n\n\n\n5.2 Parametric bootstrap\nA we have already seen, parametric bootstrap consists in simulating L replicates of the experiment, by drawing random observations with the fitted model, i.e. using the estimated parameters $ $. Then, for each replicate,\n\nan estimate \\hat{\\beta}^{(\\ell)} of the vector of parameters \\beta is computed,\na prediction f(x_0,\\hat{\\beta}^{(\\ell)}) is computed for each value of x_0,\na new observation y_0^{(\\ell)} is randomly drawn for each value of x_0.\n\nWe can then use the empirical quantiles of f(x_0,\\hat{\\beta}^{(\\ell)}, 1 \\leq \\ell \\leq L) and (y_0^{(\\ell)}, 1 \\leq \\ell \\leq L) to compute a confidence interval for f_0=f(x_0,\\beta) and a prediction interval for y_0.\nLet f_{0,p} and y_{0,p} be the empirical quantiles of order p of f(x_0,\\hat{\\beta}^{(\\ell)}, 1 \\leq \\ell \\leq L) and (y_0^{(\\ell)}, 1 \\leq \\ell \\leq L), respectively. Instead of using the empirical intervals\n\\begin{aligned}\n{\\rm CI}_{{\\rm mc},1-\\alpha}(f_0) &= [f_{0,\\alpha/2} \\ , \\ f_{0,1-\\alpha/2} ]  \\\\\n{\\rm CI}_{{\\rm mc},1-\\alpha}(y_0) &= [y_{0,\\alpha/2} \\ , \\ y_{0,1-\\alpha/2} ]  \n\\end{aligned}\nwe can define the confidence and prediction intervals using the correction previously introduced for obtaining unbiased intervals in the case of a linear model:\n\\begin{aligned}\n& {\\rm CI}^\\star_{{\\rm mc},1-\\alpha}(f_0) = \\\\\n& [f(x_0,\\hat{\\beta}) + \\frac{qt_{\\alpha/2, n-d}}{q\\mathcal{N}_{\\alpha/2}}(f_{0,\\alpha/2} - f(x_0,\\hat{\\beta}))\\ , \\  f(x_0,\\hat{\\beta}) + \\frac{qt_{1-\\alpha/2, n-d}}{q\\mathcal{N}_{1-\\alpha/2}}(f_{0,1-\\alpha/2} - f(x_0,\\hat{\\beta}))]  \\\\\n& {\\rm CI}^\\star_{{\\rm mc},1-\\alpha}(y_0) = \\\\\n& [f(x_0,\\hat{\\beta}) + \\frac{qt_{\\alpha/2, n-d}}{q\\mathcal{N}_{\\alpha/2}}(y_{0,\\alpha/2} - f(x_0,\\hat{\\beta}))\\ , \\  f(x_0,\\hat{\\beta}) + \\frac{qt_{1-\\alpha/2, n-d}}{q\\mathcal{N}_{1-\\alpha/2}}(y_{0,1-\\alpha/2} - f(x_0,\\hat{\\beta}))]  \n\\end{aligned}\nLet us compute these confidence and prediction interval bu boostraping for the f_2:\n\nf_hat <- function(beta, x){ beta[4] + (beta[1]-beta[4])/(1+exp(-beta[2]*(x-beta[3]))) }\nbeta_hat  <- coef(nlm2)\ny_hat_ref <- f_hat(beta_hat, x)\ndf_mc <- data.frame( x =x_new, f = f_new)\n\nres <- parallel::mclapply(1:1000, function(b) {\n  y_b <- y_hat_ref + sigma(nlm2) * rnorm(n)\n  nlm2_b <- nls(y_b ~ f_hat(beta, x), start = list(beta = beta_hat))\n  f_b <- predict(nlm2_b, newdata = df_mc)\n  list(f_hat = f_b, y_hat = f_b + rnorm(1, 0, sigma(nlm2)))\n})\n\ndf_mc[c(\"lwr_conf\",\"upr_conf\")] <- \n  map(res, \"f_hat\") %>% reduce(rbind) %>% \n  apply(2, quantile, c(alpha/2,1-alpha/2)) %>% t()\ndf_mc[c(\"lwr_pred\",\"upr_pred\")] <-\n  map(res, \"y_hat\") %>% reduce(rbind) %>% \n  apply(2, quantile, c(alpha/2,1-alpha/2)) %>% t()\n## removing bias\ndf_mc[,(3:6)] <- f_new + rq*(df_mc[,(3:6)] - f_new)\n\n\n\nShow the code\nggplot(faithful)  + geom_point(aes(x = eruptions, y = waiting)) +\n   geom_ribbon(data=df_mc, aes(x=x_new, ymin=lwr_pred, ymax=upr_pred), alpha=0.1, fill=\"blue\") + \n  geom_ribbon(data=df_mc, aes(x=x_new, ymin=lwr_conf, ymax=upr_conf), alpha=0.2, fill=\"#339900\") +   \n  geom_line(data=df_mc, aes(x=x_new, y=f_new), colour=\"#339900\", size=1)"
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#preliminary",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#preliminary",
    "title": "Polynomial Regression",
    "section": "Preliminary",
    "text": "Preliminary\nOnly functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation.\n\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(ggfortify)  # extend some ggplot2 features\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#the-cars-data-example",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#the-cars-data-example",
    "title": "Polynomial Regression",
    "section": "1 The ‘cars’ data example",
    "text": "1 The ‘cars’ data example\nThe data set cars gives the speed of cars and the distances taken to stop (Note that the data were recorded in the 1920s). The data frame consists of 50 observations (rows) and 2 variables (columns): speed (mph), stopping distance (ft)\n\ndata(cars)\ncars %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nScatter plots can help visualize any relationship between the explanatory variable speed (also called regression variable, or predictor) and the response variable dist.\n\n\nShow the code\ncars_plot <-\n  cars %>% \n  ggplot() +  aes(x = speed, y = dist) + \n  geom_point(size = 2, colour=\"#993399\") + xlab(\"Speed (mph)\") + ylab(\"Stopping distance (ft)\")  \ncars_plot\n\n\n\n\n\nBased on this data, our objective is to build a regression model of the form\ny_j = f(x_j) + \\varepsilon_j \\quad ; \\quad 1 \\leq j \\leq n\nwhere (x_j, 1 \\leq j \\leq n) and (y_j, 1 \\leq j \\leq n) represent, respectively, the n measured speeds and distances and where (\\varepsilon_j, 1 \\leq j \\leq n) is a sequence of residual errors. In other words, \\varepsilon_j represents the difference between the distance predicted by the model f(x_j) and the observed distance y_j.\nWe will restrict ourselves to polynomial regression, by considering functions of the form \n\\begin{aligned}\nf(x) &= f(x ; c_0, c_1, c_2, \\ldots, c_d) \\\\\n&= c_0 + c_1 x + c_2 x^2 + \\ldots + c_d x^d\n\\end{aligned}\n\nBuilding a polynomial regression model requires to perform the following tasks:\n\nFor a given degree d,\n\nestimate the parameters of the model,\nassess the validity of the model,\nevaluate the predictive performance of the model\n\nCompare the different possible models and select the best one(s)."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#fitting-polynomial-models",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#fitting-polynomial-models",
    "title": "Polynomial Regression",
    "section": "2 Fitting polynomial models",
    "text": "2 Fitting polynomial models\n\n2.1 Fitting a polynomial model as a linear model\nRemark that the polynomial regression model \ny_j = c_0 + c_1 x_j + c_2 x^2_j + \\ldots + c_{d} x^{d}_j + \\varepsilon_j\n can be written in a matrix form y = X\\beta + \\varepsilon  where \ny = \\left( \\begin{array}{c}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{array}\\right), \\quad\nX = \\left( \\begin{array}{cccc}\n1 & x_1 & \\cdots & x_1^{d} \\\\\n1 & x_2 & \\cdots & x_2^{d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & \\cdots & x_n^{d}\n\\end{array}\\right), \\quad\n\\beta = \\left( \\begin{array}{c}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_{d}\n\\end{array}\\right), \\quad\n\\varepsilon = \\left( \\begin{array}{c}\n\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\n\\end{array}\\right)\n\nThen, X\\beta is the vector of predicted values and varepsilon the vector of residual errors. Then, all what is said in the lecture about linear regression model can be applied.\n\n\n2.2 Fitting a polynomial of degree 0\nA very basic model considers that the observations (y_j) fluctuates around a constant value c_0:\ny_j = c_0 + \\varepsilon_j y_j = c_0 + \\varepsilon_j\n\nlm0 <- lm(dist ~ 1, data = cars)\nlm0\n\n\nCall:\nlm(formula = dist ~ 1, data = cars)\n\nCoefficients:\n(Intercept)  \n      42.98  \n\n\nThe coefficient c_0 is the empirical mean\n\nmean(cars$dist)\n\n[1] 42.98\n\n\nLooking how the model fits the data is more than enough for concluding that this model is miss-specified.\n\n\nShow the code\ncars_plot <- cars_plot + \n  geom_smooth(method = \"lm\", formula = y ~ 1, se = FALSE)\ncars_plot\n\n\n\n\n\nIndeed, a clear increasing trend is visible in the data while the model assumes that there is no trend. We therefore reject this first model.\n\n\n2.3 Fitting a polynomial of degree 1\n\n2.3.1 Numerical results\nThe scatter plot above rather suggests a linearly increasing relationship between the explanatory and response variables. Let us therefore assume now a linear trend which is mathematically represented by a polynomial of degree 1:\ny_j = c_0 + c_1 x_j + \\varepsilon_j \\quad ; \\quad 1 \\leq j \\leq n\nWe can then fit this model to our data using the function lm:\n\nlm1 <- lm(dist ~ speed, data = cars)\ncoef(lm1)\n\n(Intercept)       speed \n -17.579095    3.932409 \n\n\nThese coefficients are the intercept and the slope of the regression line, but more informative results about this model are available:\n\nsummary(lm1)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nThe slope c_1 is clearly statistically significant while the model explains about 65\\% of the variability of the data. The confidence interval for c_1 confirms that an increase of the speed leads to a significant increase of the stopping distance.\n\nconfint(lm1)\n\n                 2.5 %    97.5 %\n(Intercept) -31.167850 -3.990340\nspeed         3.096964  4.767853\n\n\nSee how these quantities are computed.\n\n\n2.3.2 Some diagnostic plots\nThe fact that the slope is significantly different from zero does not imply that this polynomial model of degree 1 correctly describes the data: at this stage, we can only conclude that a polynomial of degree 1 better explains the variability of the data than a constant model.\nDiagnostic plots are visual tools that allows one to ``see’’ if something is not right between a chosen model and the data it is hypothesized to describe.\nFirst, we can add the regression line to the plot of the data.\n\ncars_plot <- cars_plot + \n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, colour=\"#339900\")\ncars_plot\n\n\n\n\nThe regression line describes pretty well the global trend in the data: based on this graphic, there is no reason to reject the model.\nSeveral diagnostic plots are available for a lm object. The first two are a plot of residuals against fitted values and a normal QQ plot.\n\nautoplot(lm0, which = 1:2) ## autoplot comes from ggfortify\n\nWarning: `arrange_()` was deprecated in dplyr 0.7.0.\nPlease use `arrange()` instead.\nSee vignette('programming') for more help\n\n\n\n\n\nThe residual plot shows a slight (decreasing and increasing) trend which suggests that the residuals are not identically distributed around 0. Furthermore, the QQ plot shows that the extreme residual values are not the extreme values of a normal distribution. It may be therefore necessary to improve the regression model.\n\n\n2.3.3 The predictive performance of the model\nEven if the model it somewhat miss-specified, it may have good predictive properties.\nA common practice is to split the dataset into a 80:20 sample (training:test), then, build the model on the 80% sample and then use the model thus built to predict the reponse variable on test data.\nDoing it this way, we will have the model predicted values for the 20% data (test). We can then see how the model will perform with this ``new’’ data, by comparing these predicted values with the original ones. We can alo check the stability of the prediction given by the model, by comparing these predicted values with those obtained previouly, when the complete data were used for building the model.\nLet us first randomly define the training and test samples:\n\nn <- nrow(cars)\nis_train <- sample(c(TRUE, FALSE), n, prob = c(0.8, 0.2), replace = TRUE)\ncars_training <- filter(cars,  is_train)\ncars_test     <- filter(cars, !is_train)\n\n\npred_testing_set  <- \n  lm(dist ~ speed, data = cars_training) %>% \n  predict(cars_test)\n\n\n\nShow the code\ncars_pred_plot <-\n  cars_training %>% ggplot() +  aes(x = speed, y = dist) + \n  geom_point(size = 2, colour=\"#993399\") +  xlab(\"Speed mph)\") + ylab(\"Stopping distance (ft)\") + \n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  geom_point(data = data.frame(cars_test, testing = pred_testing_set),\n             mapping = aes(x = speed, y = dist), size = 2, shape = 3, colour=\"red\")\ncars_pred_plot\n\n\n\n\n\nOn one hand, it is reassuring to see that removing part of the data has a very little impact on the predictions. On the other hand, the predictive performance of the model remains limited because of the natural variability of the data. Indeed, this model built with the training sample only explains 43 % of the variabilility of the new test sample.\n\nR2_test <- cor(pred_testing_set, cars_test$dist)^2\nR2_test\n\n[1] 0.4283797\n\n\nLet us see now how the model behaves with extreme values by defining as test sample the data points with the 5 smallest and the 5 largest speeds.\n\ncars_training <- cars[c(6:45) ,]\ncars_test     <- cars[-c(6:45),]\npred_testing_set  <- \n  lm(dist ~ speed, data = cars_training) %>% \n  predict(cars_test)\n\n\n\nShow the code\ncars_pred_plot <-\n  cars_training %>% ggplot() +  aes(x = speed, y = dist) + \n  geom_point(size = 2, colour=\"#993399\") +  xlab(\"Speed mph)\") + ylab(\"Stopping distance (ft)\") + \n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  geom_point(data = data.frame(cars_test, testing = pred_testing_set),\n             mapping = aes(x = speed, y = dist), size = 2, shape = 3, colour=\"red\")\ncars_pred_plot\n\n\n\n\n\nWe see that the model now underestimate the stopping distance for the largest speeds. In conclusion, the model may be used with confidence for predicting the stopping distance for given speeds which are central values. Another model should probably be developed for predicting stopping distances for extreme speed values.\n\n\n2.3.4 Confidence interval and prediction interval\nImagine we aim to predict the stopping distance for a speed x. Using the estimated coefficients of our polynomial model, the prediction will be\n\\hat{f}(x) = \\hat{c}_0 + \\hat{c}_1x \nThis prediction is an estimation since it is based on estimated coefficients \\hat{c}_0 and \\hat{c}_1. Then, the uncertainty on the prediction should be quantified by providing a confidence interval for f(x).\nSince the model can be used for predicting the stopping distance for non extreme speeds, we will restrict ourselves to speeds between 6 and 23 mph.\n\nalpha <- 0.05\nnew_x <- data.frame( speed = (6:23))\nconf_inter <- \n  setNames(cbind(new_x,\n    predict(lm1, newdata = new_x, interval = \"confidence\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\nconf_inter %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nA prediction interval for a new measured distance y=f(x)+e can also be computed. This prediction interval takes into account both the uncertainty on the predicted distance f(x) and the variability of the measure, represented in the model by the residual error e.\n\npred_inter <-    setNames(cbind(new_x,\n    predict(lm1, newdata = new_x, interval = \"prediction\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\npred_inter %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nLet us plot these two intervals.\n\n\nShow the code\ncars_training %>% ggplot() +  aes(x = speed, y = dist) + geom_point() +\n  geom_ribbon(data = conf_inter, aes(ymin = lwr, ymax = upr), fill = \"red\" , alpha = 0.75) +\n  geom_ribbon(data = pred_inter, aes(ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.25)\n\n\n\n\n\nThese intervals are of very little interest and should not be used in practice. Indeed, we can see that both the confidence interval and the prediction interval contain negative values for small or moderate speeds, which is obviously unrealistic… We’ll see later how to transform the model and/or the data in order to take into account some constraints about the data.\nSee how these intervals are computed.\n\n\n\n2.4 Fitting a polynomial of degree 2\nWe can expect to better describe the extreme values by using a polynomial of higher degree. Let us therefore fit a polynomial of degree 2 to the data.\n\nlm2 <- lm(dist ~  speed + I(speed^2), data = cars)\nsummary(lm2)\n\n\nCall:\nlm(formula = dist ~ speed + I(speed^2), data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.720  -9.184  -3.188   4.628  45.152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  2.47014   14.81716   0.167    0.868\nspeed        0.91329    2.03422   0.449    0.656\nI(speed^2)   0.09996    0.06597   1.515    0.136\n\nResidual standard error: 15.18 on 47 degrees of freedom\nMultiple R-squared:  0.6673,    Adjusted R-squared:  0.6532 \nF-statistic: 47.14 on 2 and 47 DF,  p-value: 5.852e-12\n\n\nSurprisingly, the 3 t-tests seem to indicate that none of the coefficients is statistically significant. This result should be considered with caution.Indeed, it does not mean that the three coefficients can be replaced by 0 (i.e. removed) in the model.\nThe difficulty of interpretating these p-values is due to the strong correlations that exist between the estimates \\hat{c}_0, \\hat{c}_1 and \\hat{c}_2. It is recalled that the variance covariance matrix of the vector of estimates is \\sigma^2(X^\\prime X)^{-1}. We can then easily derive the correlation matrix from covariance matrix.\n\nX <- model.matrix(lm2)\nS <- solve(crossprod(X))\nR <- cov2cor(S)\nR\n\n            (Intercept)      speed I(speed^2)\n(Intercept)   1.0000000 -0.9605503  0.8929849\nspeed        -0.9605503  1.0000000 -0.9794765\nI(speed^2)    0.8929849 -0.9794765  1.0000000\n\n\nWe will see below how to use a sequence of orthogonal polynomials in order to get uncorrelated estimates.\nThe R-squared indicate that 66.7% of the variability of the data is explained by a polynomial of degree 2, instead of 65.1% with a polynomial of degree 1. On the other hand, the standard deviation of the residual error is 15.18 ft with this model instead of 15.38 ft with the previous one. These improvements are really too small to justify the use of a quadratic polynomial.\nNevertheless, plotting the data together with the fitted polynomial suggests that the extreme values are better predicted with a polynomial of degree 2 than with a straight line.\n\n\nShow the code\ncars_plot + \n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2, raw = TRUE), se = FALSE, colour=\"#339900\")\n\n\n\n\n\nThe predictive performance of the model need to be assesed in order to confirm this property of the model. We can again use the 80% most central data points to build the model and test it on the 20% remaining data points.\n\n\nShow the code\ncars_plot + \n  geom_smooth(data = cars_training, method = \"lm\", formula = y ~ poly(x, 2, raw = TRUE), se = FALSE, colour=\"#339900\")\n\n\n\n\n\nThe shape of the polynomial is totally different when it is built using the training data only. That means that the model is quite unstable, depending strongly on some data values. Then, repeating the same experiment (under the same experimental conditions) would probably lead to significantly different results.\nSuch lack of reproducibility of the predictions reinforces our idea that this model should be rejected.\n\n\n2.5 Fitting a polynomial without intercept\nThe stopping distance predicted by the model should be null when the speed is null. This constraint can easily be achieved by removing the intercept from the regression model: \nf(x) =  c_1 x + c_2 x^2 + \\ldots + c_d x^d\n Let us fit a regression model of degree 2 without intercept to our data.\n\nlm2_no_inter <- lm(dist ~ 0 + speed + I(speed^2), data = cars)\ncoef(lm2_no_inter)\n\n     speed I(speed^2) \n1.23902996 0.09013877 \n\n\nWe can check that the design matrix consists of the 2 columns (x_j) and (x_j^2).\n\nX <- model.matrix(lm2_no_inter)\nhead(X)\n\n  speed I(speed^2)\n1     4         16\n2     4         16\n3     7         49\n4     7         49\n5     8         64\n6     9         81\n\n\nWe can plot the data with the predicted response f(x), confidence interval for f(x) and prediction intervals for new observations y.\n\n\nShow the code\nalpha <- 0.05\nnew_x <- data.frame( speed = (6:23))\nfit <- data.frame(new_x, dist = predict(lm2_no_inter, newdata = new_x))\nconf_inter <- \n  setNames(cbind(new_x,\n    predict(lm2_no_inter, newdata = new_x, interval = \"confidence\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\npred_inter <- \n  setNames(cbind(new_x,\n    predict(lm2_no_inter, newdata = new_x, interval = \"prediction\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\ncars_training %>% ggplot() +  aes(x = speed, y = dist) + geom_point() +\n  geom_ribbon(data = conf_inter, aes(ymin = lwr, ymax = upr), fill = \"red\" , alpha = 0.75) +\n  geom_ribbon(data = pred_inter, aes(ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.25) +  \n  geom_line(data = fit, aes(x = speed, y = dist), colour=\"#339900\", size = 1)\n\n\n\n\n\nConfidence interval for f(x) now only contain positive values for any x>0 and is reduce to 0 when x=0.\nOn the other hand, prediction intervals for new observations still contain negative values. This is due to the fact that the residual error model is a constant error model y=f(x)+e, assuming that the variance of the error e does not depend on the predicted value f(x). This hypothesis doe not reflect the true phenomena and should be rejected. We will see that an appropriate alternative may consist in transforming the data.\n\n\n2.6 Using orthogonal polynomials\nInstead of defining each component of the regression model lm(dist ~ speed + I(speed^2) ), it is equivalent to define explicitly the regression model as a polynomial of degree 2\n\nlm2_poly <- lm(dist ~ poly(speed, degree = 2, raw = TRUE), data = cars)\ndesign_raw <- model.matrix(lm2_poly)\nhead(design_raw)\n\n  (Intercept) poly(speed, degree = 2, raw = TRUE)1\n1           1                                    4\n2           1                                    4\n3           1                                    7\n4           1                                    7\n5           1                                    8\n6           1                                    9\n  poly(speed, degree = 2, raw = TRUE)2\n1                                   16\n2                                   16\n3                                   49\n4                                   49\n5                                   64\n6                                   81\n\n\nResults obtained with both methods are absolutely identical ::: {.cell hash=‘map566-lecture-polynomial-regression_cache/html/unnamed-chunk-56_0b1251dbf8d2cb08456e77e556d3edcb’}\ncoef(lm2_poly)\n\n                         (Intercept) poly(speed, degree = 2, raw = TRUE)1 \n                           2.4701378                            0.9132876 \npoly(speed, degree = 2, raw = TRUE)2 \n                           0.0999593 \n\n:::\nWe have seen that this model leads to highly correlated estimators of the coefficients of the model which make it difficult the interpretation of the results. Indeed, let X be the design matrix for a polynomial model of degree d, \nX = \\left( \\begin{array}{cccc}\n1 & x_1 & \\cdots & x_1^{d} \\\\\n1 & x_2 & \\cdots & x_2^{d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & \\cdots & x_n^{d}\n\\end{array}\\right)\n Then, the columns of X are highly correlated since \n(X^\\prime X)_{kl} = \\sum_{j=1}^n x_j^{k+l-2}\n\nThe option raw = FALSE (which is the default) allows to use an orthogonal basis of polynomial.\n\n2.6.1 The Gram-Schmidt procedure\nOrthogonal polynomials can be obtained by applying the Gram-Schmidt orthogonalization process to the basis 1, x, x^2, , x^{d}:\n\n\\begin{aligned}\np_0(x) &= 1 \\\\\np_1(x) &= x - \\frac{\\langle x, p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(x) \\\\\np_2(x) &= x^2\n- \\frac{\\langle x^2 , p_1 \\rangle}{\\langle p_1, p_1 \\rangle} p_1(x)\n- \\frac{\\langle x^2 , p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(x)  \\\\\n&\\vdots \\\\\np_{d}(x) &= x^{d}\n- \\frac{\\langle x^{d} , p_{d-1} \\rangle}{\\langle p_{d-1}, p_{d-1} \\rangle} p_{d-1}(x)\n- \\cdots\n- \\frac{\\langle x^{d} , p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(x)\n\\end{aligned}\n\nLet \\tilde{X} = (p_0(x) , p_1(x), \\cdots , p_{d}(x)) be the new design matrix. Then, \n\\tilde{X}^\\prime \\tilde{X} = \\left( \\begin{array}{cccc}\nn & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots& \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{array} \\right)\n\n\n\n2.6.2 Orthogonal polynomial basis with degree 1\nLet us see the results obtained with a polynomial of degree 1\n\nlm1_ortho <- lm(dist ~ poly(speed, degree = 1), data = cars)\ndesign_ortho <- model.matrix(lm1_ortho)\nhead(design_ortho)\n\n  (Intercept) poly(speed, degree = 1)\n1           1              -0.3079956\n2           1              -0.3079956\n3           1              -0.2269442\n4           1              -0.2269442\n5           1              -0.1999270\n6           1              -0.1729098\n\ncrossprod(design_ortho)\n\n                         (Intercept) poly(speed, degree = 1)\n(Intercept)             5.000000e+01            1.665335e-16\npoly(speed, degree = 1) 1.665335e-16            1.000000e+00\n\n\nHere, the estimated intercept \\hat{c}_0 is the empirical mean \\bar{y}= NA\n\ncoef(lm1_ortho)\n\n            (Intercept) poly(speed, degree = 1) \n                42.9800                145.5523 \n\n\nAdding a term of degree 2 keeps the first two column of the design matrix unchanged\n\nlm2_ortho <- lm(dist ~ poly(speed, degree = 2), data = cars)\ndesign_ortho <- model.matrix(lm2_ortho)\nhead(design_ortho)\n\n  (Intercept) poly(speed, degree = 2)1 poly(speed, degree = 2)2\n1           1               -0.3079956               0.41625480\n2           1               -0.3079956               0.41625480\n3           1               -0.2269442               0.16583013\n4           1               -0.2269442               0.16583013\n5           1               -0.1999270               0.09974267\n6           1               -0.1729098               0.04234892\n\ncrossprod(design_ortho)\n\n                          (Intercept) poly(speed, degree = 2)1\n(Intercept)              5.000000e+01             1.665335e-16\npoly(speed, degree = 2)1 1.665335e-16             1.000000e+00\npoly(speed, degree = 2)2 1.665335e-16             1.301786e-16\n                         poly(speed, degree = 2)2\n(Intercept)                          1.665335e-16\npoly(speed, degree = 2)1             1.301786e-16\npoly(speed, degree = 2)2             1.000000e+00\n\n\nLet us look at the results obtained with this model:\n\nsummary(lm2_ortho)\n\n\nCall:\nlm(formula = dist ~ poly(speed, degree = 2), data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.720  -9.184  -3.188   4.628  45.152 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                42.980      2.146  20.026  < 2e-16 ***\npoly(speed, degree = 2)1  145.552     15.176   9.591 1.21e-12 ***\npoly(speed, degree = 2)2   22.996     15.176   1.515    0.136    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.18 on 47 degrees of freedom\nMultiple R-squared:  0.6673,    Adjusted R-squared:  0.6532 \nF-statistic: 47.14 on 2 and 47 DF,  p-value: 5.852e-12\n\n\nThese results give rise to several comments:\n\nthe first two estimated coefficients \\hat{c}_0 and \\hat{c}_1 are those obtained previously with a polynomial of degree 1\nthe p-values of the t-test can now properly be interpreted and confirm our previous analyis: the first two coefficients are statistically significant. The need of a quadratic term is much less obvious (p=0.136). Furthermore, confidence intervals can be derived for each coefficient\n\n\nconfint(lm2_ortho)\n\n                              2.5 %    97.5 %\n(Intercept)               38.662361  47.29764\npoly(speed, degree = 2)1 115.021940 176.08257\npoly(speed, degree = 2)2  -7.534552  53.52608\n\n\n\nThe estimated variance of the residuals \\hat{\\sigma}^2, the R-squared value and the adjusted R-squared value are identical when orthogonal or non orthogonal polynomial are used. Indeed, even if the parameterization is different, the two models are the same polynomials of degree 2 and therefore lead to the same predictions.\n\nSee how these quantities are computed."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#model-comparison",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#model-comparison",
    "title": "Polynomial Regression",
    "section": "3 Model comparison",
    "text": "3 Model comparison\n\n3.1 t-test\nSeveral quantitative criteria and several statistical tests are available for comparing models.\nFirst, we have seen that t-test are performed for each coefficient of the model\n\nsummary(lm1_ortho)$coefficients\n\n                        Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)              42.9800   2.175002 19.76090 1.061332e-24\npoly(speed, degree = 1) 145.5523  15.379587  9.46399 1.489836e-12\n\nsummary(lm2_ortho)$coefficients\n\n                          Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)               42.98000    2.14622 20.025902 1.188618e-24\npoly(speed, degree = 2)1 145.55226   15.17607  9.590906 1.211446e-12\npoly(speed, degree = 2)2  22.99576   15.17607  1.515265 1.364024e-01\n\n\nBased on these results, we can conclude that c_0 \\neq 0 and c_1 \\neq 0. On the other hand, the data does not allow us to conclude that c_2 \\neq 0.\n\n\n3.2 Analysis-of-variance (anova)\nBy construction, the F-statistics provided with the summary of a fitted model is the F-statistics of an anova for testing this fitted model against the model without explanatory variable lm0. This test is known as the overall F-test for regression.\n\nanova(lm0, lm1)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ 1\nModel 2: dist ~ speed\n  Res.Df   RSS Df Sum of Sq      F   Pr(>F)    \n1     49 32539                                 \n2     48 11354  1     21186 89.567 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm0, lm2)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ 1\nModel 2: dist ~ speed + I(speed^2)\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1     49 32539                                  \n2     47 10825  2     21714 47.141 5.852e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNot surprisingly, both lm1 and lm2 are clearly preferred to lm0. As already stated above the t-test and the F-test for testing lm0 against lm1 are equivalent.\nWe can also perform an ANOVA to test lm1 against lm2:\n\nanova(lm1, lm2)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ speed\nModel 2: dist ~ speed + I(speed^2)\n  Res.Df   RSS Df Sum of Sq     F Pr(>F)\n1     48 11354                          \n2     47 10825  1    528.81 2.296 0.1364\n\n\nlm1 is slightly preferred to lm2 for this criterion.\n\n\n3.3 Likelihood ratio test (LRT)\nThe likelihood ratio test can be used for testing 2 nested models. Since our 3 polynomial models are nested, we can use the LRT fo testing one of these model against another one.\nLog-likelihoods \\log\\ell_0(\\hat\\theta_0), \\log\\ell_1(\\hat\\theta_1) and \\log\\ell_2(\\hat\\theta_2) are available, where \\hat\\theta_k is the maximum likelihood estimate of the parameter of model lmk for k=0, 1, 2.\n\nlogLik(lm0)\nlogLik(lm1)\nlogLik(lm2)\n\n\n\n'log Lik.' -232.9012 (df=2)\n\n\n'log Lik.' -206.5784 (df=3)\n\n\n'log Lik.' -205.386 (df=4)\n\n\nFor testing lm0 against lm1, we compute the test statistic 2(\\log\\ell_1(\\hat\\theta_1) - \\log\\ell_0(\\hat\\theta_0) which has a \\chi^2 distribution with 1 degree of freedom under lm0. Then, the p-value of this test can easily be computed:\n\ndl <- 2*as.numeric(logLik(lm1) - logLik(lm0))\n1-pchisq(dl,1)\n\n[1] 3.995693e-13\n\n\nThe test statistic 2(\\log\\ell_2(\\hat\\theta_2) - \\log\\ell_1(\\hat\\theta_1) is used for testing lm1 against lm2:\n\ndl <- 2 * as.numeric(logLik(lm2) - logLik(lm1))\n1 - pchisq(dl,1)\n\n[1] 0.122521\n\n\nWe can see with this example that the ANOVA and the LRT give very similar results and lead to the same conclusion: the constant model can be rejected with high confidence (i.e. the stopping distance depends on the speed), while a model with both a linear and a quadratic component is not preferred to a model with a linear component only (i.e. we don’t reject the hypothesis that the stopping distance increase linearly with the speed).\n\n\n3.4 Information criteria\nInformation criteria such as the Akaike information criterion (AIC) and the Bayesian information criaterion (BIC) can also be used for comparing models which are not necessarily nested.\n\nAIC(lm0, lm1, lm2)\n\n    df      AIC\nlm0  2 469.8024\nlm1  3 419.1569\nlm2  4 418.7721\n\nBIC(lm0, lm1, lm2)\n\n    df      BIC\nlm0  2 473.6265\nlm1  3 424.8929\nlm2  4 426.4202\n\n\nModels with lowest AIC and/or BIC will be preferred. Here, both criteria agree for rejecting lm0 with high confidence. AIC has a very slight preference for lm2 while BIC penalizes a little bit more polynomial with higher degree and prefers lm1. Nevertheless, these differences are not large enough for selecting definitely any of these 2 models.\nSee how these quantities are computed."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#data-transformation",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#data-transformation",
    "title": "Polynomial Regression",
    "section": "4 Data transformation",
    "text": "4 Data transformation\nWhen fitting a linear regression model one assumes that there is a linear relationship between the response variable and each of the explanatory variables. However, in many situations there may instead be a non-linear relationship between the variables. This can sometimes be remedied by applying a suitable transformation to some (or all) of the variables, such as power transformations or logarithms.\nIn addition, transformations can be used to correct violations of model assumptions such as constant error variance and normality.\nApplying suitable transformations to the original data prior to performing regression may be sufficient to make linear regression models appropriate for the transformed data."
  },
  {
    "objectID": "docs/regression/map566-lecture-polynomial-regression.html#log-based-tranformations",
    "href": "docs/regression/map566-lecture-polynomial-regression.html#log-based-tranformations",
    "title": "Polynomial Regression",
    "section": "5 Log based tranformations",
    "text": "5 Log based tranformations\nLet’s plot the data using semi-log scales: we use a logarithmic scale for the x-axis and a linear scale for the y-axis in the first plot, a linear scale for the x-axis and a logarithmic scale for the y-axis in the second plot.\n\ngrid.arrange(\n  cars_plot + scale_x_log10(),\n  cars_plot + scale_y_log10(), nrow = 1)\n\n\n\n\nNone of thee two plots show any linear trend. Then, let us try with a log-log transformation.\n\ncars_plot + scale_x_log10() + scale_y_log10()\n\n\n\n\nLooks much better!\nWe can then try to fit the following model to the data:  \\log(y_j) = c_0 + c_1\\log(x_j) + \\varepsilon_j  Another representation of this model exhibits a power relationship between the explanatory and response variables  y_j = A \\, x_j^{c_1} \\, e^{\\varepsilon_j}  where A=e^{c_0}. This model ensures that the response y can only take positive values. Furthermore, the variability of the response increases with the explanatory variable. Indeed, for small \\varepsilon_j,  y_j \\approx A \\, x_j^{c_1}(1 + \\varepsilon_j)\n\nlm1_log <- lm(log(dist) ~ log(speed), data = cars)\nsummary(lm1_log)\n\n\nCall:\nlm(formula = log(dist) ~ log(speed), data = cars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.00215 -0.24578 -0.02898  0.20717  0.88289 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.7297     0.3758  -1.941   0.0581 .  \nlog(speed)    1.6024     0.1395  11.484 2.26e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4053 on 48 degrees of freedom\nMultiple R-squared:  0.7331,    Adjusted R-squared:  0.7276 \nF-statistic: 131.9 on 1 and 48 DF,  p-value: 2.259e-15\n\n\nThis model now explains 73% of the (transformed) data.\n\n5.1 Diagnostic plots\nLet us plot the data with the regression line:\n\ncars_plot_log <-\n  ggplot(cars) + aes(x = log(speed), y = log(dist)) + \n  geom_point(size = 2, colour=\"#993399\") \ncars_plot_log + geom_smooth(method = \"lm\", formula = y ~ x)\n\n\n\n\nThis plot, plus the two following diagnostic plots, show that the model fits quite well the data.\n\nautoplot(lm1_log, which = 1:2)\n\n\n\n\nIn the next plot, we compare the fits obtained when the model is built using the complete data (green) and when only the training sample is used (red).\n\n\nShow the code\ntraining <- c(6:45)\ncars_test     <- cars[-training, ]\ncars_training <- cars[ training , ]\ncars_plot_log +\n  geom_smooth(data = cars_training, \n    mapping = aes(x = log(speed), y = log(dist)),\n    method = \"lm\", formula = y ~ x, se = FALSE, color = \"red\") +\n  geom_smooth(data = cars,\n    mapping = aes(x = log(speed), y = log(dist)),\n    method = \"lm\", formula = y ~ x, se = FALSE)\n\n\n\n\n\nThese two models are very similar: the estimation is relatively insensible to extreme values.\n\n\n5.2 Confidence interval and prediction interval\nBased on the previous analysis, the model can be used for predicting stopping distances for a wide range of speeds. Let us compute and display the confidence interval for the regression line (of the model lm(log(dist) ~ log(speed))) and the prediction interval for new observed log-distances.\n\n\nShow the code\nalpha <- 0.05\nnew_x <- data.frame( speed = 3:30)\nfit <- data.frame(new_x, log_dist = predict(lm1_log, newdata = new_x))\nconf_inter <- \n  setNames(cbind(new_x,\n    predict(lm1_log, newdata = new_x, interval = \"confidence\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\npred_inter <- \n  setNames(cbind(new_x,\n    predict(lm1_log, newdata = new_x, interval = \"prediction\", level = 1 - alpha)),\n    c(\"speed\", \"dist\", \"lwr\", \"upr\"))\ncars_training %>% ggplot() +  aes(x = log(speed), y = log(dist)) + geom_point() +\n  geom_ribbon(data = conf_inter, aes(ymin = lwr, ymax = upr), fill = \"red\" , alpha = 0.75) +\n  geom_ribbon(data = pred_inter, aes(ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.25) +\n  geom_line(data = fit, aes(x = log(speed), y = log_dist), colour=\"#339900\", size = 1) +\n      xlab(\"speed (mph)\") + ylab(\"stopping distance (ft)\")  \n\n\n\n\n\nWe can derive from these intervals a confidence interval and a prediction interval for the non transformed data (i.e. using the original scale).\n\n\nShow the code\ncars_training %>% ggplot() +  aes(x = speed, y = dist) + geom_point() +\n  geom_ribbon(data = conf_inter, aes(ymin = exp(lwr), ymax = exp(upr)), fill = \"red\" , alpha = 0.75) +\n  geom_ribbon(data = pred_inter, aes(ymin = exp(lwr), ymax = exp(upr)), fill = \"blue\", alpha = 0.25) +\n  geom_line(data = fit, aes(x = speed, y = exp(log_dist)), colour=\"#339900\", size = 1) +\n      xlab(\"speed (mph)\") + ylab(\"stopping distance (ft)\")  \n\n\n\n\n\nThe general trend describes quite well how the stopping distance increases with the speed. Nevertheless, the residual error model seems to over-estimate the variability of the data for the greatest speeds.\n\n\n5.3 Model comparison\nWe will investigate now if the numerical criteria for model comparison confirm that this model based on a log-log transformation should be preferred to the polynomial models.\nLet us compute first the log-likelihood for this model.\n\nlogLik(lm1_log)\n\n'log Lik.' -24.76592 (df=3)\n\n\nWARNING: This value of the log-likelihood cannot be compared as it is to the log-likelihoods computed previously. Indeed, by definition, the log-likelihood for model lm1.log is the probability density function (pdf) of log(y).\n\n\\begin{aligned}\n\\log\\ell_{\\rm lm1.log} &= \\log\\ell(\\hat{\\theta}_{\\rm lm1.log}) \\\\\n&= \\mathbb{P} (\\log(y) ; \\hat{\\theta}_{\\rm lm1.log})\n\\end{aligned}\n where \\hat{\\theta}_{\\rm lm1.log} is the ML estimator of \\theta for model lm1.log.\nThen, the pdf of \\log(y) under this model cannot be directly compared to the pdf of y under another model. We need instead to compute the pdf of y under this model. For any \\theta and any 1 \\leq j \\leq n,\n\n\\begin{aligned}\n\\mathbb{P}(y_j ; \\theta) &= \\mathbb{P}(\\log(y_j) ; \\theta)\\times \\frac{1}{y_j}\n\\end{aligned}\n and  \\log (\\mathbb{P}(y ; \\theta)) = \\log (\\mathbb{P} (\\log(y) ; \\theta)) - \\sum_{j=1}^n \\log(y_j) \n\nlogLik(lm1_log) - sum(log(cars$dist))\n\n'log Lik.' -201.5613 (df=3)\n\n\nThe information criteria should also take into account this transformation\n\nAIC(lm1_log) + 2 * sum(log(cars$dist))\n\n[1] 409.1226\n\nBIC(lm1_log) + 2 * sum(log(cars$dist))\n\n[1] 414.8587\n\n\nBoth criteria prefer this model based on a log-log transformation than any polynomial model without any transformation.\nRemark: Since models lm1 and lm1_log have the same number of parameters, log-likelihoods without penalisation can be used for comparing these 2 models."
  },
  {
    "objectID": "docs/regression/map566-lab-nonlinear-regression.html#preliminary",
    "href": "docs/regression/map566-lab-nonlinear-regression.html#preliminary",
    "title": "Nonlinear Regression",
    "section": "Preliminary",
    "text": "Preliminary\nThe usual libraries:\n\nlibrary(tidyverse)\nlibrary(ggfortify) # extend some ggplot2 features\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/regression/map566-lab-nonlinear-regression.html#introduction",
    "href": "docs/regression/map566-lab-nonlinear-regression.html#introduction",
    "title": "Nonlinear Regression",
    "section": "1 Introduction",
    "text": "1 Introduction\nWe consider the same data file ratWeight.csv with rat weights measured over 14 weeks during a subchronic toxicity study related to the question of genetically modified (GM) corn."
  },
  {
    "objectID": "docs/regression/map566-lab-nonlinear-regression.html#questions",
    "href": "docs/regression/map566-lab-nonlinear-regression.html#questions",
    "title": "Nonlinear Regression",
    "section": "2 Questions",
    "text": "2 Questions\n\nLoad the ratWeight.csv data file and plot the weight of the females of the control group\nSelect the ID B38837 and fit a polynomial model to the growth curve of this female rat.\nFit a Gompertz model f_1(t) = A e^{-b e^{-k\\, t}} to this data.\n\nHint: use for initial values: A = 200, b = 1, k = 0.1.\n\nFit the two following growth models:\n\n\nAsymptotic regression model:\n\nf_2(t)  = A \\left( 1 - b\\, e^{-k\\, t} \\right)\n\nLogistic curve: f_3(t)  = \\frac{A}{1 + e^{-\\gamma( t-\\tau)}}\n\n\nPropose two other parameterizations of the asymptotic regression model which involves\n\n\nthe weight at birth w_0 (when t=0), the limit weight w_\\infty (when t\\to \\infty) and k\nthe weight at birth, the weight at the end of the study w_{14} and the ratio r=(w_{14}-w_{7})/(w_7 - w_0)\n\nCan we compare these models?\n\nWe will now use model f_{2a}. Check that the estimate of \\beta=(w_0, w_\\infty, k) obtained with the nls function is the least squares estimate.\nCheck that this estimate is also the least squares estimate of the linearized model. Then, how are computed the standard errors of \\hat\\beta?\nCompute 90% confidence intervals for the model parameters using several approaches (profile likelihood, linearization, parametric bootstrap)\nCompute a 90% confidence interval for the predicted weight and a 90% prediction interval for the measured weight using the delta method."
  },
  {
    "objectID": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#preliminary",
    "href": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#preliminary",
    "title": "Nonlinear Mixed Effects Models",
    "section": "Preliminary",
    "text": "Preliminary\nFunctions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. We also need the lme4 package which is the standard for fitting mixed-model. lattice is used for graphical representation of quantities such as random and fixed effects in the mixed models. saemix implements Stochastic EM algorithms to fit non-linear mixed effect models.\n\nlibrary(tidyverse)\nlibrary(saemix)\nlibrary(corrplot)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#introduction",
    "href": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#introduction",
    "title": "Nonlinear Mixed Effects Models",
    "section": "1 Introduction",
    "text": "1 Introduction\nSéralini, Cellier, and Vendomois (2007) published the paper “New Analysis of a Rat Feeding Study with a Genetically Modified Maize Reveals Signs of Hepatorenal Toxicity”. The authors of the paper pretend that, after the consumption of MON863, rats showed slight but dose-related significant variations in growth.\nThe objective of this exercise is to highlight the flaws in the methodology used to achieve this result, and show how to properly analyse the data.\nWe will restrict our study to the male rats of the study fed with 11% of maize (control and GMO)\n\nrats <- read_csv(\"../../data/ratWeight.csv\")\nmales11 <- filter(rats, gender == \"Male\" & dosage == \"11%\")\nmales11 %>% rmarkdown::paged_table()"
  },
  {
    "objectID": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#nonlinear-fixed-effect-model-reproducing-seralini-et-al.s-study",
    "href": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#nonlinear-fixed-effect-model-reproducing-seralini-et-al.s-study",
    "title": "Nonlinear Mixed Effects Models",
    "section": "2 Nonlinear fixed effect model reproducing Seralini et al.’s study",
    "text": "2 Nonlinear fixed effect model reproducing Seralini et al.’s study\n\nPlot the growth curves of the control and GMO groups.\nFit an Gompertz growth model f_1(t) = A \\exp(-\\exp(-b(t-c))) to the complete data (males fed with 11% of maize) using a least square approach, with the same parameters for the control and GMO groups.\nFit a Gompertz growth model to the complete data using a least square approach, with different parameters for the control and GMO groups.\n\nHint: write the model as\n y_{ij} = A_{0} e^{-e^{-b_0 (t_{ij}-c_0)}} \\mathbf{1}_{\\{{\\rm regime\\}}_i={\\rm Control}} +\nA_{1} e^{- e^{-b_1 (t_{ij}-c_1)}} \\mathbf{1}_{\\{{\\rm regime}_i={\\rm GMO}\\}} + \\varepsilon_{ij}\n\nCheck out the results of the paper displayed Table 1, for the 11% males.\nPlot the residuals and explain why the results of the paper are wrong."
  },
  {
    "objectID": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#nonlinear-mixed-effect-model-with-saemix",
    "href": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#nonlinear-mixed-effect-model-with-saemix",
    "title": "Nonlinear Mixed Effects Models",
    "section": "3 Nonlinear mixed effect model with SAEMIX",
    "text": "3 Nonlinear mixed effect model with SAEMIX\nWe propose to use instead a mixed effects model for testing the effect of the regime on the growth of the 11% male rats.\n\n3.1 Gompertz model with random effects\nThe codes below show how to fit a Gompertz model to the data\n\nassuming the same population parameters for the two regime groups,\nusing lognormal distributions for the 3 parameters (setting transform.par=c(1,1,1))\nassuming a diagonal covariance matrix \\Omega (default)\n\n\n3.1.1 Data declaration\nCreate first the saemixData object\n\nsaemix_data <- saemixData(\n  name.data       = males11, \n  name.group      = \"id\",\n  name.predictors = \"week\",\n  name.response   = \"weight\"\n  )\n\nUsing the object called males11 in this R session as the data.\n\n\nThe following SaemixData object was successfully created:\n\nObject of class SaemixData\n    longitudinal data for use with the SAEM algorithm\nDataset males11 \n    Structured data: weight ~ week | id \n    Predictor: week () \n\n\n\n\n3.1.2 Model declaration\nImplement then the structural model and create the saemixModel object. Initial values for the population parameters should be provided.\n\ngompertz <- function(psi, id, x) { \n  t <- x[,1]\n  A <- psi[id, 1]\n  b <- psi[id, 2]\n  c <- psi[id, 3]\n  ypred <- A*exp(-exp(-b*(t-c)))\n  ypred\n}\n\npsi0 <- c(A = 500, b = 0.2, c = 0.2)\ndistribParam <- c(1, 1, 0)\nNLMM_gompertz <- saemixModel(model = gompertz, psi0 = psi0, transform.par = distribParam)\n\n\n\n3.1.3 Model estimation\nRun saemix for estimating the population parameters, computing the individual estimates, computing the FIM and the log-likelihood (linearization)\n\nsaemix_options <- saemixControl(\n  map   = TRUE,\n  fim   = TRUE,\n  ll.is = FALSE,\n  displayProgress = FALSE,\n  directory = \"output_saemix\",\n  seed = 12345)\nNLMM_gompertz_fit <- saemix(NLMM_gompertz , saemix_data, saemix_options)\n\n\nsummary(NLMM_gompertz_fit)\n\n----------------------------------------------------\n-----------------  Fixed effects  ------------------\n----------------------------------------------------\n  Parameter Estimate     SE  CV(%)\n1         A  527.870 8.2753   1.57\n2         b    0.217 0.0071   3.30\n3         c    0.061 0.0794 129.56\n4        a.   12.238 0.4049   3.31\n----------------------------------------------------\n-----------  Variance of random effects  -----------\n----------------------------------------------------\n  Parameter Estimate     SE CV(%)\nA  omega2.A    0.009 0.0021 23.93\nb  omega2.b    0.028 0.0093 32.74\nc  omega2.c    0.191 0.0553 28.92\n----------------------------------------------------\n------  Correlation matrix of random effects  ------\n----------------------------------------------------\n         omega2.A omega2.b omega2.c\nomega2.A 1.00     0.00     0.00    \nomega2.b 0.00     1.00     0.00    \nomega2.c 0.00     0.00     1.00    \n----------------------------------------------------\n---------------  Statistical criteria  -------------\n----------------------------------------------------\nLikelihood computed by linearisation\n      -2LL= 4696.628 \n      AIC = 4710.628 \n      BIC = 4722.451 \n----------------------------------------------------\n\n\n\n\n3.1.4 Diagnostic plots:\n\n# Individual plot for subject 1 to 9, \nsaemix.plot.fits(NLMM_gompertz_fit, ilist = c(1:9), smooth=TRUE)\n\n\n\n\n\n# Diagnostic plot: observations versus population predictions\nsaemix.plot.obsvspred(NLMM_gompertz_fit)\n\n\n\n\n\n# Scatter plot of residuals\nsaemix.plot.scatterresiduals(NLMM_gompertz_fit, level=1)\n\n\n\n\n\n\n3.1.5 Correlation matrix of the estimates\n\nfim <- NLMM_gompertz_fit@results@fim # Fisher information matrix\ncov_hat <- solve(fim)                # covariance matrix of the estimates\nd <- sqrt(diag(cov_hat))             # s.e. of the estimates\ncorrplot(cov_hat / (d %*% t(d)), type = 'upper', diag = FALSE)\n\n\n\ncov_hat / (d %*% t(d))               # correlation matrix of the estimates\n\n            [,1]       [,2]        [,3]          [,4]        [,5]          [,6]\n[1,]  1.00000000 -0.1571634 -0.01843078  0.000000e+00  0.00000000  0.000000e+00\n[2,] -0.15716344  1.0000000  0.13584772  0.000000e+00  0.00000000  0.000000e+00\n[3,] -0.01843078  0.1358477  1.00000000  0.000000e+00  0.00000000  0.000000e+00\n[4,]  0.00000000  0.0000000  0.00000000  1.000000e+00 -0.02503999 -8.038155e-05\n[5,]  0.00000000  0.0000000  0.00000000 -2.503999e-02  1.00000000 -2.123566e-02\n[6,]  0.00000000  0.0000000  0.00000000 -8.038155e-05 -0.02123566  1.000000e+00\n[7,]  0.00000000  0.0000000  0.00000000 -1.142047e-02 -0.07726570 -5.797004e-02\n            [,7]\n[1,]  0.00000000\n[2,]  0.00000000\n[3,]  0.00000000\n[4,] -0.01142047\n[5,] -0.07726570\n[6,] -0.05797004\n[7,]  1.00000000\n\n\n\n\n\n3.2 Questions\n\nFit the same model to the same data, assuming different population parameters for the control and GMO groups. Can we conclude that the regime has an effect on the growth of the 11% male rats?\nUse an asymptotic regression model f(t) = w_{\\infty} + (w_0 -w_{\\infty})e^{-k\\,t} to test the effect of the regime on the growth of the 11% male rats.\nShould we accept the hypothesis that the random effects are uncorrelated?"
  },
  {
    "objectID": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#references",
    "href": "docs/mixed-models/map566-lab-nonlinear-mixed-model.html#references",
    "title": "Nonlinear Mixed Effects Models",
    "section": "4 References",
    "text": "4 References\n\n\n\n\nSéralini, Gilles-Eric, Dominique Cellier, and Joël Spiroux de Vendomois. 2007. “New Analysis of a Rat Feeding Study with a Genetically Modified Maize Reveals Signs of Hepatorenal Toxicity.” Archives of Environmental Contamination and Toxicology 52 (4): 596–602. https://link.springer.com/article/10.1007/s00244-006-0149-5."
  },
  {
    "objectID": "docs/mixed-models/map566-lab-linear-mixed-model.html#preliminary",
    "href": "docs/mixed-models/map566-lab-linear-mixed-model.html#preliminary",
    "title": "Linear Mixed Models",
    "section": "Preliminary",
    "text": "Preliminary\nFunctions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. We also need the lme4 package which is the standard for fitting mixed-model. lattice is used for graphical representation of quantities such as random and fixed effects in the mixed models.\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lattice)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/mixed-models/map566-lab-linear-mixed-model.html#the-orange-data",
    "href": "docs/mixed-models/map566-lab-linear-mixed-model.html#the-orange-data",
    "title": "Linear Mixed Models",
    "section": "1 The Orange data",
    "text": "1 The Orange data\nThe Orange data is a classical data set provided with the datasets package included with R. It consists in a 35-row data frame with records of the growth of 3 orange trees, giving their circumference against their age:\n\ndata(Orange)\n## we force the sorting of the levels (1,2,3)\nOrange$Tree <- factor(Orange$Tree, levels = unique(Orange$Tree))\nOrange %>% as_tibble() %>% rmarkdown::paged_table()\n\n\n  \n\n\n\n\n\nShow the code\npl <- \n  Orange %>% \n    ggplot() + aes(x = age, y = circumference, color = Tree) + \n  geom_point() + geom_line() \npl\n\n\n\n\n\n\nFit different linear (fixed effect) models to this data and compare them\nFit different linear mixed effect models to this data and compare them (with/without random effect for the slope/intercept)\nCompute confidence intervals on the parameters, compute the individual parameters and plot the random effects for the best model\nCompare the ML and the REML estimates for this model"
  },
  {
    "objectID": "docs/mixed-models/map566-lab-linear-mixed-model.html#the-pastes-data",
    "href": "docs/mixed-models/map566-lab-linear-mixed-model.html#the-pastes-data",
    "title": "Linear Mixed Models",
    "section": "2 The Pastes data",
    "text": "2 The Pastes data\nThe Pastes data is a another classical data set provided with the lme4 package. It describes the strength of a chemical paste product whose quality depends on the delivery batch, and the cask within the delivery. There are 60 observations, 10 levels of batch and 3 levels of cask.\n\ndata(Pastes)\nPastes %>% as_tibble() %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nWe can have a quick look on summary statistics in each level of each factor:\n\nPastes %>% \n  group_by(batch, cask) %>% \n  summarise(mean_strengh = mean(strength), sd_strength = sd(strength), n_replicat = n()) %>% \n  rmarkdown::paged_table()\n\n`summarise()` has grouped output by 'batch'. You can override using the `.groups` argument.\n\n\n\n  \n\n\n\nThere is obviously some structure in the data set and an effect of the batch level:\n\n\nShow the code\nPastes %>% \n  ggplot() + aes(x = fct_reorder(batch, strength, median), y = strength) + geom_boxplot() + geom_jitter()\n\n\n\n\n\nWe can plot the cask as the function of the strength, conditionning on batch (and ) the later being reorder by strength)\n\n\nShow the code\nlattice::dotplot(cask ~ strength | reorder(batch, strength), Pastes,\n        strip = FALSE, strip.left = TRUE, layout = c(1, 10),\n        ylab = \"Cask within batch\",\n        xlab = \"Paste strength\", jitter.y = TRUE)\n\n\n\n\n\n\nBuild simple linear models for this data (with batches, then with casks effects)\nBuild mixed models for this data (with random effects on batches, then casks effects)\nWhat is the main cause of the variability of the paste strength?"
  },
  {
    "objectID": "docs/mixed-models/map566-lab-linear-mixed-model.html#the-oats-data",
    "href": "docs/mixed-models/map566-lab-linear-mixed-model.html#the-oats-data",
    "title": "Linear Mixed Models",
    "section": "3 The Oats data",
    "text": "3 The Oats data\nThese data have been introduced by Yates (1935) as an example of a split-plot design. The objective is to assess the effect of treatment (level of nitrogen) on the yield of different varieties of oat.\nThe treatment structure used in the experiment was a 3 x 4 full factorial, with three varieties of oats and four concentrations of nitrogen. The experimental units were arranged into six blocks, each with three whole-plots subdivided into four subplots. The varieties of oats were assigned randomly to the whole-plots and the concentrations of nitrogen to the subplots. All four concentrations of nitrogen were used on each whole-plot.\nThe data are available from the nlme package:\n\ndata(Oats, package = \"nlme\")\n# keep the original order of the levels of Block and Variety\nOats$Block   <- factor(Oats$Block  , levels = unique(Oats$Block))  \nOats$Variety <- factor(Oats$Variety, levels = unique(Oats$Variety))\nOats %>% as_tibble() %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nThe xtabs function creates a contingency table using the handy formula notation which allows us to check that the data indeed follows a full factorial design:\n\nxtabs(~ Block + Variety, Oats)\n\n     Variety\nBlock Victory Golden Rain Marvellous\n  I         4           4          4\n  II        4           4          4\n  III       4           4          4\n  IV        4           4          4\n  V         4           4          4\n  VI        4           4          4\n\n\n\nPlot the data in such a way as to visualize the effect of the fertilizer-concentration on the yield as well as possible differences between blocks and varieties.\nFit the Oats data with linear mixed effects model, assuming a linear effect of the fertilizer concentration on the yield."
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-linear-mixed-model.html#preliminary",
    "href": "docs/mixed-models/map566-lecture-linear-mixed-model.html#preliminary",
    "title": "Linear Mixed Effects Models",
    "section": "Preliminary",
    "text": "Preliminary\nOnly functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. The package lme4 and nlme are used for adjusting mixed models. lattice gives additional tools for plotting output of linear mixed-effect models.\n\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(lme4)\nlibrary(nlme)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-linear-mixed-model.html#introduction-the-orthodont-data",
    "href": "docs/mixed-models/map566-lecture-linear-mixed-model.html#introduction-the-orthodont-data",
    "title": "Linear Mixed Effects Models",
    "section": "1 Introduction: the orthodont data",
    "text": "1 Introduction: the orthodont data\nThe Orthodont data has 108 rows and 4 columns of the change in an orthodontic measurement over time for 27 young subjects. Here, distance is a numeric vector of distances from the pituitary to the pterygomaxillary fissure (mm). These distances are measured on x-ray images of the skull.\n\n\ndata(\"Orthodont\", package = \"nlme\")\nOrthodont %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nLet us plot the data, i.e. the distance versus age.\n\n\nShow the code\northodont_plot <- Orthodont %>% \n  ggplot() + aes(x = age, y = distance, color = Sex) + geom_point(size = 2)\northodont_plot\n\n\n\n\n\nA linear model by definition assumes there is a linear relationship between the observations (y_j, 1\\leq j \\leq n) and m series of variables (x_{j}^{(1)}, \\ldots , x_{j}^{(p)} ,1\\leq j \\leq n):\n\ny_j = c_0 + c_1 x_{j}^{(1)} + c_2 x_{j}^{(2)} + \\cdots + c_m x_{j}^{(m)} + \\varepsilon_j , \\quad \\quad 1\\leq j \\leq n ,\n where (\\varepsilon_j, 1\\leq j \\leq n) is a sequence of residual errors.\nIn our example, the observations (y_j, 1\\leq j \\leq n) are the n=108 measured distances.\nWe can start by fitting a linear model to these data using age as a regression variable:\n\\text{linear model 1:} \\quad \\quad y_j = c_0 + c_1 \\times{\\rm age}_j + \\varepsilon_j\n\nlm1 <- lm(distance~age, data=Orthodont)\nsummary(lm1)\n\n\nCall:\nlm(formula = distance ~ age, data = Orthodont)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5037 -1.5778 -0.1833  1.3519  6.3167 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  16.7611     1.2256  13.676  < 2e-16 ***\nage           0.6602     0.1092   6.047 2.25e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.537 on 106 degrees of freedom\nMultiple R-squared:  0.2565,    Adjusted R-squared:  0.2495 \nF-statistic: 36.56 on 1 and 106 DF,  p-value: 2.248e-08\n\n\nLet us plot the predicted distance \\hat{a}_0 + \\hat{a}_1 \\times {\\rm age} together with the observed distances\n\n\nShow the code\northodont_plot + geom_line(aes(x = age, y = predict(lm1))) + theme_bw()\n\n\n\n\n\nIf we now display explicitly the boys and girls, we see that we are missing something: we underestimate the distance for the boys and overestimate it for the girls.\n\n\nShow the code\northodont_plot + geom_line(aes(x = age, y = predict(lm1))) + facet_grid( ~ Sex ) + theme_bw()\n\n\n\n\n\nWe can then assume the same slope but different intercepts for boys and girls,\n\\text{linear model 2:} \\quad \\quad y_j = c_0 +  \\delta_{0F}\\times \\mathbf{1}_{{\\rm Sex}_j={\\rm F}} + c_1 \\times{\\rm age}_j  + \\varepsilon_j\nHere, c_{0}=c_{0M} is the intercept for the boys and c_0 + \\delta_{0F}=c_{0F} the intercept for the girls.\n\nlm2 <- lm(distance~age+Sex, data=Orthodont)\nsummary(lm2)\n\n\nCall:\nlm(formula = distance ~ age + Sex, data = Orthodont)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9882 -1.4882 -0.0586  1.1916  5.3711 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17.70671    1.11221  15.920  < 2e-16 ***\nage          0.66019    0.09776   6.753 8.25e-10 ***\nSexFemale   -2.32102    0.44489  -5.217 9.20e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.272 on 105 degrees of freedom\nMultiple R-squared:  0.4095,    Adjusted R-squared:  0.3983 \nF-statistic: 36.41 on 2 and 105 DF,  p-value: 9.726e-13\n\nOrthodont$pred_lm2 <- predict(lm2)\n\n\n\nShow the code\northodont_plot +   \n  geom_line(aes(x = age, y = predict(lm2))) + theme_bw()\n\n\n\n\n\nWe could instead assume the same intercept but different slopes for boys and girls:\n\\text{linear model 3:} \\quad \\quad y_j = c_0 + c_{1M} \\times{\\rm age}_j \\times \\mathbf{1}_{{\\rm Sex}_j={\\rm M}} + c_{1F}\\times{\\rm age}_j\\times \\mathbf{1}_{{\\rm Sex}_j={\\rm F}} + \\varepsilon_j Here, c_{1M} is the slope for the boys and c_{1F} the slope for the girls.\n\nlm3 <- lm(distance~age:Sex , data=Orthodont)\nsummary(lm3)\n\n\nCall:\nlm(formula = distance ~ age:Sex, data = Orthodont)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7424 -1.2424 -0.1893  1.2681  5.2669 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   16.76111    1.08613  15.432  < 2e-16 ***\nage:SexMale    0.74767    0.09807   7.624 1.16e-11 ***\nage:SexFemale  0.53294    0.09951   5.355 5.07e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.249 on 105 degrees of freedom\nMultiple R-squared:  0.4215,    Adjusted R-squared:  0.4105 \nF-statistic: 38.26 on 2 and 105 DF,  p-value: 3.31e-13\n\nOrthodont$pred_lm3 <- predict(lm3)\n\n\n\nShow the code\northodont_plot + geom_line(aes(x = age, y = predict(lm3))) + theme_bw()\n\n\n\n\n\nWe can also combine these two models by assuming different intercepts and different slopes:\n\\text{linear model 4:} \\quad \\quad y_j = c_0 +  \\delta_{0F}\\times \\mathbf{1}_{{\\rm Sex}_j={\\rm F}} + c_{1M} \\times{\\rm age}_j \\times \\mathbf{1}_{{\\rm Sex}_j={\\rm M}} + c_{1F}\\times{\\rm age}_j\\times \\mathbf{1}_{{\\rm Sex}_j={\\rm F}}  + \\varepsilon_j\n\nlm4 <- lm(distance ~ age:Sex + Sex, data = Orthodont)\nsummary(lm4)\n\n\nCall:\nlm(formula = distance ~ age:Sex + Sex, data = Orthodont)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6156 -1.3219 -0.1682  1.3299  5.2469 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    16.3406     1.4162  11.538  < 2e-16 ***\nSexFemale       1.0321     2.2188   0.465  0.64279    \nage:SexMale     0.7844     0.1262   6.217 1.07e-08 ***\nage:SexFemale   0.4795     0.1522   3.152  0.00212 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.257 on 104 degrees of freedom\nMultiple R-squared:  0.4227,    Adjusted R-squared:  0.4061 \nF-statistic: 25.39 on 3 and 104 DF,  p-value: 2.108e-12\n\nOrthodont$pred_lm4 <- predict(lm4)\n\n\n\nShow the code\northodont_plot + geom_line(aes(x = age, y = predict(lm4))) + theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\n\nThe p-value cannot be used as such since the design matrix is not orthogonal\n\n\n\nC <- crossprod(model.matrix(lm4))\nC / sqrt(outer(diag(C), diag(C)))\n\n              (Intercept) SexFemale age:SexMale age:SexFemale\n(Intercept)     1.0000000 0.6382847   0.7543719     0.6254922\nSexFemale       0.6382847 1.0000000   0.0000000     0.9799579\nage:SexMale     0.7543719 0.0000000   1.0000000     0.0000000\nage:SexFemale   0.6254922 0.9799579   0.0000000     1.0000000\n\n\n\nsummary(lm(distance ~ age , data = subset(Orthodont, Sex == \"Male\")))\n\n\nCall:\nlm(formula = distance ~ age, data = subset(Orthodont, Sex == \n    \"Male\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6156 -1.6844 -0.2875  1.2641  5.2469 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  16.3406     1.4544  11.236  < 2e-16 ***\nage           0.7844     0.1296   6.054 9.02e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.318 on 62 degrees of freedom\nMultiple R-squared:  0.3715,    Adjusted R-squared:  0.3614 \nF-statistic: 36.65 on 1 and 62 DF,  p-value: 9.024e-08\n\n\n\n\nA different parameterization for this model could be used in an equivalent way:\n\n\n\\text{linear model 4':} \\quad \\quad y_j = c_{0} +  \\delta_{0F}\\times \\mathbf{1}_{{\\rm Sex}_j={\\rm F}} + (c_{1}  +\n\\delta_{1F}\\times{\\rm age}_j\\times \\mathbf{1}_{{\\rm Sex}_j={\\rm F}})\\times{\\rm age}_j  + \\varepsilon_j\nwhere c_{1}=c_{1M} and c_{1} + \\delta_{1F}=c_{1F}. This model is implemented by defining the interaction between Sex and age as age*Sex:\n\nlm4b <- lm(distance~age*Sex, data=Orthodont)\nsummary(lm4b)\n\n\nCall:\nlm(formula = distance ~ age * Sex, data = Orthodont)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6156 -1.3219 -0.1682  1.3299  5.2469 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    16.3406     1.4162  11.538  < 2e-16 ***\nage             0.7844     0.1262   6.217 1.07e-08 ***\nSexFemale       1.0321     2.2188   0.465    0.643    \nage:SexFemale  -0.3048     0.1977  -1.542    0.126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.257 on 104 degrees of freedom\nMultiple R-squared:  0.4227,    Adjusted R-squared:  0.4061 \nF-statistic: 25.39 on 3 and 104 DF,  p-value: 2.108e-12\n\n\nBoth parametrizations give the same predictions\n\ncbind(head(predict(lm4)), head(predict(lm4b)))\n\n      [,1]     [,2]\n1 22.61562 22.61562\n2 24.18437 24.18437\n3 25.75312 25.75312\n4 27.32187 27.32187\n5 22.61562 22.61562\n6 24.18437 24.18437\n\n\n\n\nDifferent criteria for model selection, including BIC, seem to prefer lm3.\n\nBIC(lm1, lm2, lm3, lm4)\n\n    df      BIC\nlm1  3 519.6234\nlm2  4 499.4121\nlm3  4 497.1948\nlm4  5 501.6524\n\n\nShould we then consider that lm3 is our final model?\nLet us look at the individual fits for 8 subjects,\n\n\nShow the code\nselected <- c(paste0(\"M0\",5:8),paste0(\"F0\",2:5))\nOrthodont_selected <- filter(Orthodont, Subject %in% selected)\nOrthodont_selected %>% \n  ggplot() + geom_point(aes(x = age,y = distance), color=\"red\", size=3) + \n  geom_line(aes(x = age, y = predict(lm3, newdata=Orthodont_selected))) + facet_wrap(~Subject, nrow=2) \n\n\n\n\n\nWe see that the model for the boys, respectively for the girls, seems to underestimate or overestimate the individual data of the four boys, respectively the four girls.\nIndeed, we didn’t take into account the fact that the data are repeated measurements made on the same subjects. A more convenient plot for this type of data consists in joining the data of a same individual:\n\n\nShow the code\nggplot(data = Orthodont) + \n  geom_point(aes(x = age, y = distance, color = Subject), size = 3) + \n  geom_line(aes(x = age, y = distance, color = Subject)) + facet_grid(~Sex)\n\n\n\n\n\nWe see on this plot, that even if the distance seems to increase linearly for each individual, the intercept and the slope may change from a subject to another one, including within the same Sex group.\nWe therefore need to extend our linear model in order to take into account this inter-individual variability."
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-linear-mixed-model.html#mathematical-definition-of-a-linear-mixed-effects-models",
    "href": "docs/mixed-models/map566-lecture-linear-mixed-model.html#mathematical-definition-of-a-linear-mixed-effects-models",
    "title": "Linear Mixed Effects Models",
    "section": "2 Mathematical definition of a linear mixed effects models",
    "text": "2 Mathematical definition of a linear mixed effects models\nThe linear model introduced above concerns a single individual. Suppose now that a study is based on N individuals and that we seek to build a global model for all the collected observations for the N individuals. We will denote y_{ij} the jth observation taken of individual i and x_{ij}^{(1)}, \\ldots , x_{ij}^{(m)} the values of the m explanatory variables for individual i. If we assume that the parameters of the model can vary from one individual to another, then for any subject i, 1\\leq i \\leq N, the linear model becomes\n\ny_{ij} = c_{i0}^{\\ } + c_{i1}^{\\ } x_{ij}^{(1)} + c_{i2}^{\\ } x_{ij}^{(2)} + \\cdots + c_{im}^{\\ } x_{ij}^{(m)} + \\varepsilon_{ij},   \\quad 1\\leq j \\leq n_i.\n\nSuppose to begin with that each individual parameter c_{ik} can be additively broken down into a fixed component \\beta_k and an individual component \\eta_{ik}, i.e.,\nc_{ik} = \\beta_k + \\eta_{ik}\nwhere \\eta_{ik} represents the deviation of c_{ik} from the “typical” value \\beta_k in the population for individual i, with \\eta_{ik} a normally distributed random variable with mean 0.\nUsing this parametrization, the model becomes\n\ny_{ij} = \\beta_{0}^{\\ } + \\beta_{1}^{\\ } x_{ij}^{(1)}  + \\cdots + \\beta_{m}^{\\ } x_{ij}^{(m)} +\n\\eta_{i0}^{\\ } + \\eta_{i1}^{\\ } x_{ij}^{(1)}  + \\ldots + \\eta_{im}^{\\ } x_{ij}^{(m)} + \\varepsilon_{ij}.\n\nWe can then rewrite the model in matrix form:\n\ny_i = X_i \\, \\beta + X_i \\, \\eta_i + \\varepsilon_i ,\n\nwhere\n\ny_i = \\left( \\begin{array}{c}\ny_{i1} \\\\ y_{i2} \\\\ \\vdots \\\\ y_{in_i}\n\\end{array}\\right)\n\\quad ,\\quad\nX_i = \\left( \\begin{array}{cccc}\n1 & x_{i1}^{(1)} & \\cdots & x_{i1}^{(m)} \\\\\n1 & x_{i2}^{(1)} & \\cdots & x_{i2}^{(m)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{in}^{(1)} & \\cdots & x_{in}^{(m)}\n\\end{array}\\right)\n\\quad , \\quad\n\\beta = \\left( \\begin{array}{c}\n\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{m}\n\\end{array}\\right)\n\\quad , \\quad\n\\eta_i = \\left( \\begin{array}{c}\n\\eta_{i0} \\\\ \\eta_{i1} \\\\ \\vdots \\\\ \\eta_{im}\n\\end{array}\\right)\n\\quad , \\quad\n\\varepsilon_i = \\left( \\begin{array}{c}\n\\varepsilon_{i1} \\\\ \\varepsilon_{i2} \\\\ \\vdots \\\\ \\varepsilon_{in}\n\\end{array}\\right)\n\nHere, y_i is the n_i vector of observations for individual i, X_i is the n_i \\times d design matrix (with d = m+1), \\beta is a p-vector of fixed effects (i.e. common to all individuals of the population), \\eta_i is a p-vector of random effects (i.e. specific to each individual) and \\varepsilon_i is a n_i-vector of residual errors.\nThe model is called linear mixed effects model because it is a linear combination of fixed and random effects.\nThe random effects are assumed to be normally distributed in a linear mixed effects model:\n\n\\eta_i \\sim^{\\mathrm{iid}} \\mathcal{N}(0_p \\ , \\ \\Omega)\n\n\\Omega is the d\\times d variance-covariance matrix of the random effects. This matrix is diagonal if the components of \\eta_i are independent.\nThe vector of residual errors \\varepsilon_i is also normally distributed:\n\n\\varepsilon_i \\sim^{\\mathrm{iid}} \\mathcal{N}(0_{n_i} \\ , \\ \\Sigma_i)\n\nThe particular case of a diagonal matrix with constant diagonal terms, i.e. \\Sigma_i = \\sigma^2 \\, I_{n_i}, means that, for any individual i, the residual errors (\\varepsilon_{ij}, 1 \\leq j \\leq n_i) are independent and identically distributed:\n\n\\varepsilon_{ij} \\sim^{\\mathrm{iid}} \\mathcal{N}(0, \\sigma^2)\n\nWe can extend this model to models invoking more complicated design matrices that may even differ for fixed and random effects:\n y_i = X_i \\, \\beta + A_i \\, \\eta_i + \\varepsilon_i\nAs an example, consider the following model\n\\begin{aligned}\ny_{ij}& = c_{i0}^{\\ } + c_{i1}^{\\ } x_{ij}^{(1)} + c_{i2}^{\\ } x_{ij}^{(2)} + \\varepsilon_{ij} \\\\\n& = \\beta_{0}^{\\ } + \\beta_{1}^{\\ } x_{ij}^{(1)}  + \\beta_{2}^{\\ } x_{ij}^{(2)} +\n\\eta_{i0}^{\\ } + \\eta_{i1}^{\\ } x_{ij}^{(1)}  + \\eta_{i2}^{\\ } x_{ij}^{(2)} + \\varepsilon_{ij}\n\\end{aligned}\nThe variance covariance matrix \\Omega of the vector of random effects (\\eta_{i0},\\eta_{i1},\\eta_{i2}) is a 3\\times3 matrix.\nAssume now that parameter c_{i2} does not vary from one individual to another. Then c_{i2} = \\beta_2 for all i which means that \\eta_{i2}=0 for all i. A null variance for \\eta_{i2} means that \\Omega_{33}, the third diagonal term of \\Omega is 0.\nInstead of considering a variance-covariance matrix \\Omega with null diagonal terms, it is more convenient to rewrite the model a follows\n\ny_{ij} = \\beta_{0}^{\\ } + \\beta_{1}^{\\ } x_{ij}^{(1)}  + \\beta_{2}^{\\ } x_{ij}^{(2)} +\n\\eta_{i0}^{\\ } + \\eta_{i1}^{\\ } x_{ij}^{(1)}  + \\varepsilon_{ij} \\quad , \\quad 1 \\leq j \\leq n_i\n or, in a matricial form, as\n\n\\left( \\begin{array}{c}\ny_{i1} \\\\ y_{i2} \\\\ \\vdots \\\\ y_{in_i}\n\\end{array}\\right)\n= \\left( \\begin{array}{ccc}\n1 & x_{i1}^{(1)}  & x_{i1}^{(2)} \\\\\n1 & x_{i2}^{(1)}  & x_{i2}^{(m)} \\\\\n\\vdots & \\vdots  & \\vdots \\\\\n1 & x_{in}^{(1)}  & x_{in}^{(m)}\n\\end{array}\\right)\n\\left( \\begin{array}{c}\n\\beta_0 \\\\ \\beta_1 \\\\  \\beta_{2}\n\\end{array}\\right)\n+ \\left( \\begin{array}{cc}\n1 & x_{i1}^{(1)}   \\\\\n1 & x_{i2}^{(1)}   \\\\\n\\vdots & \\vdots   \\\\\n1 & x_{in}^{(1)}  \n\\end{array}\\right)\n\\left( \\begin{array}{c}\n\\eta_{i0} \\\\ \\eta_{i1}\n\\end{array}\\right)\n+ \\left( \\begin{array}{c}\n\\varepsilon_{i1} \\\\ \\varepsilon_{i2} \\\\ \\vdots \\\\ \\varepsilon_{in}\n\\end{array}\\right)\n\n\\Omega is now the 2\\times 2 variance-covariance matrix of (\\eta_{i0},\\eta_{i1})^\\top."
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-linear-mixed-model.html#statistical-inference-in-linear-mixed-effects-models",
    "href": "docs/mixed-models/map566-lecture-linear-mixed-model.html#statistical-inference-in-linear-mixed-effects-models",
    "title": "Linear Mixed Effects Models",
    "section": "3 Statistical inference in linear mixed effects models",
    "text": "3 Statistical inference in linear mixed effects models\n\n3.1 Estimation of the population parameters\nThe model parameters are the vector of fixed effects \\beta, the variance-covariance matrix \\Omega of the random effects and the variance \\sigma^2 of the residual errors (assuming i.i.d. residual errors).\nLet \\theta = (\\beta,\\Omega,\\sigma^2) be the set of model parameters.\nWe easily deduce from the matricial representation of the model y_i = X_i \\, \\beta + A_i \\, \\eta_i + \\varepsilon_i, that y_i is normally distributed:\ny_i \\sim \\mathcal{N}\\bigg(X_i  \\beta \\ , \\ A_i \\Omega A_i^\\top + \\sigma^2 I_{n_i}\\bigg) \nLet y = (y_i, 1\\leq i \\leq N) be the set of observations for the N individuals. The maximum likelihood (ML) estimator of \\theta maximizes the log-likelihood function defined as\n\\begin{aligned}\n\\log\\ell(\\theta) & = \\log(\\mathbb{P}(y \\ ; \\ \\theta)) = \\sum_{i=1}^{N}\\log(\\mathbb{P}(y_i \\ ; \\ \\theta))\\\\\n& = \\sum_{i=1}^{N} \\left\\{ -\\frac{n_i}{2}\\log(2\\pi) - \\frac{1}{2}\\log(|A_i \\Omega A_i^\\top + \\sigma^2 I_{n_i}|) - \\frac{1}{2}(y_i - X_i  \\beta)^\\top (A_i \\Omega A_i^\\top + \\sigma^2 I_{n_i})^{-1} (y_i - X_i  \\beta) \\right\\}\n\\end{aligned}\nThere is no analytical solution to this maximization problem. Nevertheless, numerical methods such as the Newton-Raphson and the EM algorithms, can be used for maximizing \\log\\ell(\\theta).\nThe restricted maximum likelihood (REML) approach is a variant of the ML approach. In contrast to the earlier maximum likelihood estimation, REML can produce unbiased estimates of variance and covariance parameters.\nConsider the linear model y=X\\beta+\\varepsilon as an example where \\beta is a d-vector of unknown coefficients and where \\varepsilon_j \\sim^{\\mathrm{iid}} \\mathcal{N}(0, \\sigma^2) for 1\\leq j \\leq n. Both the ML and the REML estimators of \\beta reduce to the least-squares estimator \\hat{\\beta} = (X^\\top X)^{-1}X^\\top y, but the estimator of the variance component \\sigma^2 differs according to the method:\n\\hat{\\sigma}^2_{\\rm ML} = \\frac{1}{n}\\| y - X\\hat\\beta\\|^2 \\quad ; \\quad \\hat{\\sigma}^2_{\\rm REML} = \\frac{1}{n-p}\\| y - X\\hat\\beta\\|^2  \nStandard errors (se) of the parameter estimate \\hat{\\theta} can be obtained by computing the Fisher information matrix\n I(\\hat{\\theta}) = -\\mathbb{E}\\left( \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^\\top} \\log\\mathbb{P}(y;\\hat\\theta)\\right)\nThen, the standard errors are the square roots of the diagonal elements of the inverse matrix of I(\\hat{\\theta}).\n\n\n3.2 Estimation of the individual parameters\n\n3.2.1 Estimation of the random effects\nIndividual parameters for individual i are the individual coefficients (c_{ik} , 0 \\leq k \\leq m).\nOnce the set of population parameters \\theta = (\\beta,\\Omega,\\sigma^2) has been estimated, the \\ell-vector of nonzero random effects \\eta_i can be estimated using the conditional distribution \\mathbb{P}(\\eta_i \\ | \\ y_i \\ ; \\ \\hat\\theta).\nSince the marginal distributions of y_i and \\eta_i are both Gaussian, this conditional distribution is also Gaussian with a mean and a variance that can be computed. Indeed, from Bayes Theorem,\n\\begin{aligned}\n\\mathbb{P}(\\eta_i \\, | \\, y_i \\, ; \\, \\theta) &= \\frac{\\mathbb{P}(y_i \\, | \\, \\eta_i \\, ; \\, \\theta)\\mathbb{P}(\\eta_i \\, ; \\, \\theta)}{\\mathbb{P}( y_i \\, ; \\, \\theta)} \\\\\n&= \\frac{(2\\pi\\sigma^2)^{-\\frac{n_i}{2}}(2\\pi)^{-\\frac{\\ell}{2}}|\\Omega|^{-\\frac{1}{2}}}\n{(2\\pi)^{-\\frac{n_i}{2}}|A_i\\Omega A_i^\\top + \\sigma^2 I_{n_i}|^{-\\frac{1}{2}}}\n\\frac{\n\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\| y_i-X_i\\beta-A_i\\eta_i \\|^2\n-\\frac{1}{2}\\eta_i^\\top\\Omega^{-1} \\eta_i \\right\\}\n}{\n\\exp\\left\\{-\\frac{1}{2}(y_i-X_i\\beta)^\\top(A\\Omega A^\\top + \\Sigma)^{-1} (y_i-X_i\\beta)\\right\\}\n}\n\\end{aligned}\nThen, we can show that\n\\mathbb{P}(\\eta_i \\, | \\, y_i\\, ; \\, \\theta) = (2\\pi)^{-\\frac{\\ell}{2}}|\\Gamma_i|^{-\\frac{1}{2}}\n   \\exp\\left\\{-\\frac{1}{2}(\\eta_i-\\mu_i)^\\top\\Gamma_i^{-1} (\\eta_i-\\mu_i)\\right\\} \nwhere\n\n\\Gamma_i =  \\left(\\frac{A_i^\\top A_i}{\\sigma^2} + \\Omega^{-1}\\right)^{-1}\n\\quad ; \\quad\n\\mu_i =  \\frac{\\Gamma_i A_i^\\top(y_i - X_i\\beta)}{\\sigma^2}  \n\nWe can therefore estimate the conditional mean \\mu_i and the conditional variance \\Gamma_i of \\eta_i using these formulas and the estimated parameters \\hat\\beta, \\hat\\Omega and \\hat\\sigma^2:\n\n\\hat{\\Gamma}_i =  \\left(\\frac{A_i^\\top A_i}{\\hat\\sigma^2} + \\hat\\Omega^{-1}\\right)^{-1} ,\n\\qquad  \n\\hat\\mu_i =  \\frac{\\hat\\Gamma_i A_i^\\top(y_i - X_i\\hat\\beta)}{\\hat\\sigma^2}  \n\nSince the conditional distribution of \\eta_i is Gaussian, \\hat\\mu_i is also the conditional mode of this distribution. This estimator of \\eta_i is the so-called maximum a posteriori (MAP) estimator of \\eta_i. It is also called empirical Bayes estimator (EBE).\n\n\n3.2.2 Deriving individual parameter estimates and individual predictions\nEstimation of the p individual parameters is straightforward once the \\ell nonzero random effects have been estimated:\n\\hat{c}_{ik} = \\left\\{ \\begin{array}{ll}  \n\\hat{\\beta}_k & \\text{if } \\eta_{ik} \\equiv 0 \\\\\n\\hat{\\beta}_k + \\hat{\\eta}_{ik} & \\text{otherwise }\n\\end{array}\\right. \nWe see that, for a parameter c_{ik} with no random component (i.e. \\eta_{ik} \\equiv 0, \\hat{c}_{ik} = \\hat{\\beta}_k is the maximum likelihood estimator of c_{ik}, i.e. the parameter value that maximizes the likelihood of making the observations.\nOn the other hand, if c_{ik} is a random parameter (\\eta_{ik} \\neq 0), then \\hat{c}_{ik} = \\hat{\\beta}_k+\\hat{\\eta}_{ik} is the MAP estimator of c_{ik}, i.e. the most likely value of c_{ik}, given the observations y_i and its estimated prior distribution.\nFor any set of explanatory variable (x^{(1)},x^{(2)}, \\ldots x^{(m)}), individual prediction of the response variable is then obtained using the individual estimated parameters:  \\hat{y}_i = \\hat{c}_{i0}^{\\ } + \\hat{c}_{i1}^{\\ } x^{(1)} + \\hat{c}_{i2}^{\\ } x^{(2)} + \\cdots + \\hat{c}_{im}^{\\ } x^{(m)} \n\n\n3.2.3 About the MAP estimator in a linear mixed effects model\nAs an example, consider a model where all the individual parameters are random parameters:\n y_i = X_i c_i + \\varepsilon_i \nwhere c_i = \\beta + \\eta_i \\sim \\mathcal{N}(\\beta \\ , \\ \\Omega).\nThen, the conditional distribution of c_i given y_i is also a normal distribution:\nc_i | y_i \\sim \\mathcal{N}(m_i, \\Gamma_i)\nwhere\n \\begin{aligned}\nm_i &=  \\mu_i + \\beta = \\Gamma_i \\left(\\frac{X_i^\\top}{\\sigma^2} y_i  + \\Omega^{-1}\\beta\\right) \\\\\n&= \\left(\\frac{X_i^\\top X_i}{\\sigma^2} + \\Omega^{-1}\\right)^{-1}\n\\left(\\frac{X_i^\\top X_i}{\\sigma^2} (X_i^\\top X_i)^{-1} X_i^\\top y_i  + \\Omega^{-1}\\beta\\right)\n\\end{aligned}\nWe see that the MAP estimator of c_i is a weighted average of the least square estimator of c_i, (X_i^\\top X_i)^{-1} X_i^\\top y_i, which maximizes the conditional distribution of the observations \\mathbb{P}(y_i|c_i,\\theta), and \\beta which maximizes the prior distribution of c_i. The relative weights of these two terms depend on the design and the parameters of the model:\n\nA lot of information about c_i in the data and small residual errors will make (X_i^\\top X_i)/\\sigma^2 large: the estimate of c_i will be close to the least-square estimate which only depends on the observations.\nA very informative prior will make \\Omega^{-1} large: the estimate of c_i will be close to the prior mean \\beta."
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-linear-mixed-model.html#fitting-linear-mixed-effects-models-to-the-orthodont-data",
    "href": "docs/mixed-models/map566-lecture-linear-mixed-model.html#fitting-linear-mixed-effects-models-to-the-orthodont-data",
    "title": "Linear Mixed Effects Models",
    "section": "4 Fitting linear mixed effects models to the orthodont data",
    "text": "4 Fitting linear mixed effects models to the orthodont data\n\n4.1 Fitting a first model\nA first linear mixed effects model assumes that the birth distance and the growth rate (i.e. the intercept and the slope) may depend on the individual:\n\\begin{aligned}\n\\text{lmem:} \\quad \\quad y_{ij} &= c_{i0} + c_{i1} \\times{\\rm age}_{ij} + \\varepsilon_{ij} \\\\\n&= \\beta_0 + \\beta_1 \\times{\\rm age}_{ij} + \\eta_{i0}  + \\eta_{i1} \\times{\\rm age}_{ij} + \\varepsilon_{ij}\n\\end{aligned}\nWe can use the function lmer from the package lme4 for fitting this model. By default, the restricted maximum likelihood (REML) method is used.\n\nlmem <- lmer(distance ~ age + (age|Subject), data = Orthodont) \nsummary(lmem)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ age + (age | Subject)\n   Data: Orthodont\n\nREML criterion at convergence: 442.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2229 -0.4938  0.0073  0.4721  3.9161 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n Subject  (Intercept) 5.41660  2.3274        \n          age         0.05128  0.2264   -0.61\n Residual             1.71616  1.3100        \nNumber of obs: 108, groups:  Subject, 27\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 16.76111    0.77528  21.620\nage          0.66019    0.07126   9.265\n\nCorrelation of Fixed Effects:\n    (Intr)\nage -0.848\n\n\nThe estimated fixed effects are \\hat{\\beta}_0 = 16.7611111 and \\hat{\\beta}_1 = 0.6601852.\nThe standard errors and correlation of these estimates are\n{\\rm se}(\\hat\\beta_0) = 0.77525 \\quad , \\quad {\\rm se}(\\hat\\beta_1) = 0.07125\n\\quad , \\quad {\\rm corr}(\\hat\\beta_0, \\hat\\beta_1) = -0.848\nThe estimated standard deviations and correlation of the random effects are\n\\widehat{\\rm sd}(\\eta_{i0}) = 2.3270 \\quad , \\quad \\widehat{\\rm sd}(\\eta_{i1}) = 0.2264\n\\quad , \\quad \\widehat{\\rm corr}(\\eta_{i0},\\eta_{i1}) = -0.61\nThe estimated variance-covariance matrix of the random effects is therefore\n \\hat\\Omega = \\left(\\begin{array}{cc} 5.41509 & -0.32137 \\\\ -0.32137 & 0.05127 \\end{array}\\right)\nFinally, the estimated variance of the residual errors is\n\\hat\\sigma^2 = 1.71620 \nNote that functions fixef and VarCorr return these estimated parameters:\n\npsi.pop <- fixef(lmem)\n\n\nOmega <- VarCorr(lmem)$Subject[,]\n\n\nsigma2 <- sigma(lmem)^2\n\nThe estimated individual parameters for our 8 selected individuals can be obtained using function coef\n\ncoef(lmem)$Subject[selected, ]\n\n    (Intercept)       age\nM05    15.58423 0.6858038\nM06    17.97889 0.7433649\nM07    16.15301 0.6950961\nM08    17.62162 0.5654305\nF02    15.74909 0.6700581\nF03    15.98816 0.7108418\nF04    17.83047 0.6303060\nF05    17.27813 0.4922090\n\n\nusing the formula obtained in the previous section, we can check that these estimated parameters are the empirical Bayes estimates, i.e. the conditional means of the individual parameters,\n\nOrthodont_i <- filter(Orthodont, Subject==\"M05\")\ny_i <- Orthodont_i$distance\nA_i <- cbind(1, Orthodont_i$age)\ni_O <- solve(Omega)\nGamma_i <- solve(crossprod(A_i) /sigma2 + i_O)\nmu_i <- Gamma_i %*% ( crossprod(A_i, y_i) /sigma2 + i_O %*% fixef(lmem))\nmu_i\n\n                  [,1]\n(Intercept) 15.5842345\nage          0.6858038\n\n\nIndividual predicted distances can also be computed and plotted with the observed distances\n\n\nShow the code\nOrthodont$pred_lmem <- fitted(lmem)\nOrthodont %>% filter(Subject %in% selected) %>% \n  ggplot() + geom_point(aes(x = age, y = distance), color=\"red\", size=3) + \n  geom_line(aes(x = age, y = pred_lmem)) + facet_wrap(~ Subject, ncol=4) \n\n\n\n\n\nWe can check that the predicted distances for a given individual (“M05” for instance)\n\nfilter(Orthodont, Subject == \"M05\")\n\nGrouped Data: distance ~ age | Subject\n   distance age Subject  Sex pred_lm2 pred_lm3 pred_lm4 pred_lmem\n17     20.0   8     M05 Male 22.98819 22.74244 22.61562  21.07066\n18     23.5  10     M05 Male 24.30856 24.23777 24.18437  22.44227\n19     22.5  12     M05 Male 25.62894 25.73310 25.75312  23.81388\n20     26.0  14     M05 Male 26.94931 27.22843 27.32187  25.18549\n\n\nare given by the linear model c_0+c_1\\, {\\rm age} using the individual estimated parameters\n \\widehat{\\rm distance}_i = \\hat{c}_{i0}^{\\ } + \\hat{c}_{i1}^{\\ } \\times  {\\rm age} \n\nmu_i[1] + mu_i[2]*c(8,10,12,14)\n\n[1] 21.07066 22.44227 23.81388 25.18549\n\n\n\n\n4.2 Some extensions of this first model\n\nWe can fit the same model to the same data via maximum likelihood (ML) instead of REML\n\n\nlmer(distance ~ age + (age|Subject), data = Orthodont, REML = FALSE) \n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: distance ~ age + (age | Subject)\n   Data: Orthodont\n      AIC       BIC    logLik  deviance  df.resid \n 451.2116  467.3044 -219.6058  439.2116       102 \nRandom effects:\n Groups   Name        Std.Dev. Corr \n Subject  (Intercept) 2.1941        \n          age         0.2149   -0.58\n Residual             1.3100        \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)          age  \n    16.7611       0.6602  \n\n\nThe estimated fixed effects are the same with the two methods. The variance components slightly differ since REML provides an unbiased estimate of \\Omega and \\sigma^2.\n\nBy default, the variance-covariance matrix \\Omega is estimated as a full matrix, assuming that the random effects are correlated. It is possible with lmer to constrain \\Omega to be a diagonal matrix by defining the random effects model using || instead of |\n\n\nlmer(distance ~ age + (age || Subject), data = Orthodont) \n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ age + ((1 | Subject) + (0 + age | Subject))\n   Data: Orthodont\nREML criterion at convergence: 443.3146\nRandom effects:\n Groups    Name        Std.Dev.\n Subject   (Intercept) 1.3860  \n Subject.1 age         0.1493  \n Residual              1.3706  \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)          age  \n    16.7611       0.6602  \n\n\n\n\n4.3 Fitting other models\nThe mixed effects model combines a model for the fixed effects and a model for the random effects. Let us see some possible combinations.\n\nIn this model, we assume that i) the birth distance, is the same in average for boys and girls but randomly varies between individuals, ii) the distance increases with the same rate for all the individuals. Here is the mathematical representation of this model:\n\n\\begin{aligned}\ny_{ij} &= c_{i0} + \\beta_1 \\times{\\rm age}_{ij} + \\varepsilon_{ij} \\\\\n&= \\beta_0 + \\beta_1 \\times{\\rm age}_{ij} + \\eta_{i0}  + \\varepsilon_{ij}\n\\end{aligned}\n\nlmer(distance ~ age + (1|  Subject), data = Orthodont) \n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ age + (1 | Subject)\n   Data: Orthodont\nREML criterion at convergence: 447.0025\nRandom effects:\n Groups   Name        Std.Dev.\n Subject  (Intercept) 2.115   \n Residual             1.432   \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)          age  \n    16.7611       0.6602  \n\n\n\nWe extend the previous model, assuming now different mean birth distances and different growth rates for boys and girls. The growth rate remains the same for individuals of same Sex,\n\n\\begin{aligned}\ny_{ij} &= \\beta_0 + \\beta_{0M} \\times \\mathbf{1}_{{\\rm Sex}_i={\\rm M}} + \\beta_{1M} \\times{\\rm age}_{ij}\\times \\mathbf{1}_{{\\rm Sex}_i={\\rm M}}  + \\beta_{1F} \\times{\\rm age}_{ij}\\times \\mathbf{1}_{{\\rm Sex}_i={\\rm F}}  + \\eta_{i0}  + \\varepsilon_{ij}\n\\end{aligned}\n\nlmer(distance ~ Sex + age:Sex + (1 | Subject), data = Orthodont) \n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ Sex + age:Sex + (1 | Subject)\n   Data: Orthodont\nREML criterion at convergence: 433.7572\nRandom effects:\n Groups   Name        Std.Dev.\n Subject  (Intercept) 1.816   \n Residual             1.386   \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n  (Intercept)      SexFemale    SexMale:age  SexFemale:age  \n      16.3406         1.0321         0.7844         0.4795  \n\n\n\nWe can instead assume the same birth distance for all the individuals, but different growth rates for individuals of same Sex,\n\n\\begin{aligned}\ny_{ij} &= \\beta_0  + \\beta_{1M} \\times{\\rm age}_{ij}\\times \\mathbf{1}_{{\\rm Sex}_i={\\rm M}}  + \\beta_{1F} \\times{\\rm age}_{ij}\\times \\mathbf{1}_{{\\rm Sex}_i={\\rm F}}  + \\eta_{i1} \\times{\\rm age}_{ij} + \\varepsilon_{ij}\n\\end{aligned}\n\nlmer(distance ~ age:Sex  + (0 + age | Subject), data = Orthodont) \n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ age:Sex + (0 + age | Subject)\n   Data: Orthodont\nREML criterion at convergence: 439.7694\nRandom effects:\n Groups   Name Std.Dev.\n Subject  age  0.1597  \n Residual      1.4126  \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n  (Intercept)    age:SexMale  age:SexFemale  \n      16.7611         0.7477         0.5329  \n\n\n\n\n\n\n\n\nRemark\n\n\n\nBy default, the standard deviation of a random effect (\\eta_{i0} or \\eta_{i1}) is the same for all the individuals. If we put a random effect on the intercept, for instance, it is then possible to consider different variances for males and females:\n\ny_{ij} = \\beta_0 + \\beta_1 \\times{\\rm age}_{ij} + \\eta_{i0}^{\\rm F}\\mathbf{1}_{{\\rm Sex}_i={\\rm F}} + \\eta_{i0}^{\\rm M}\\mathbf{1}_{{\\rm Sex}_i={\\rm M}}  + \\varepsilon_{ij}\n\nwhere \\eta_{i0}^{\\rm F} \\sim \\mathcal{N}(0, \\omega_{0F}^2) and \\eta_{i0}^{\\rm M} \\sim \\mathcal{N}(0, \\omega_{0M}^2)\n\nlmer(distance ~ age + (-1 + Sex|Subject), data = Orthodont)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ age + (-1 + Sex | Subject)\n   Data: Orthodont\nREML criterion at convergence: 446.152\nRandom effects:\n Groups   Name      Std.Dev. Corr\n Subject  SexMale   1.778        \n          SexFemale 2.574    0.25\n Residual           1.432        \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)          age  \n    17.1000       0.6602  \noptimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings \n\n\nIn this example, \\hat\\omega_{0F}=2.574 and \\hat\\omega_{0M}=1.778.\n\n\n\n\n4.4 Comparing linear mixed effects models\nIf we want to compare all the possible linear mixed effect models, we need to fit all these models and use some information criteria in order to select the “best one”.\nIn our model y_{ij} = c_{i0} + c_{i1} \\times{\\rm age}_{ij} + \\varepsilon_{ij}, each of the two individual coefficients c_{i0} and c_{i1}\n\nmay depend on the explanatory variable Sex or not,\nmay include a random component or not\n\nFurthermore,\n\nwhen the model includes two random effects (one for the intercept and one for the slope), these two random effects may be either correlated or independent,\nthe variance of a random effect may depend on the variable Sex or not.\n\nAt the end, there would be a very large number of models to fit and compare…\nLet us restrict ourselves to models with correlated random effects, with the same variance for males and females. We therefore have 2\\times 2 \\times 2 \\times 2 = 16 models to fit and compare if we want to perform an exhaustive comparison.\nFor a sake of simplicity in the notations, let us define the 2 numerical explanatory variables s_i=\\mathbf{1}_{\\{{\\rm Sex}_i=M\\}} and a_i = {\\rm age}_i.\n\\begin{aligned}\n\\text{M1}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0 + \\beta_1 a_{ij} + \\varepsilon_{ij} \\\\\n\\text{M2}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0 + \\beta_{0{\\rm M}}s_i + \\beta_1 a_{ij} + \\varepsilon_{ij} \\\\\n\\text{M3}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0  + \\beta_{1{\\rm M}} s_i a_{ij} + \\beta_{1{\\rm F}}(1-s_i) a_{ij}  + \\varepsilon_{ij} \\\\\n\\text{M4}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0  + \\beta_{0{\\rm M}}s_i  + \\beta_{1{\\rm M}} s_i a_{ij}+ \\beta_{1{\\rm F}}(1-s_i) a_{ij} + \\varepsilon_{ij} \\\\\n\\text{M5}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0 + \\beta_1 a_{ij}+ \\eta_{i0} + \\varepsilon_{ij} \\\\\n\\text{M6}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0 + \\beta_{0{\\rm M}}s_i + \\beta_1 a_{ij} + \\eta_{i0}+ \\varepsilon_{ij} \\\\\n\\text{M7}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0  + \\beta_{1{\\rm M}} s_i a_{ij} + \\beta_{1{\\rm F}}(1-s_i) a_{ij}  + \\eta_{i0}+ \\varepsilon_{ij} \\\\\n\\text{M8}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0  + \\beta_{0{\\rm M}}s_i  + \\beta_{1{\\rm M}} s_i a_{ij} + \\beta_{1{\\rm F}}(1-s_i) a_{ij} + \\eta_{i0}  + \\varepsilon_{ij} \\\\\n\\text{M9}  \\ \\ \\quad \\quad  y_{ij} &= \\beta_0 + \\beta_1 a_{ij}+ \\eta_{i1} a_{ij} + \\varepsilon_{ij} \\\\\n\\text{M10} \\quad \\quad  y_{ij} &= \\beta_0 + \\beta_{0{\\rm M}}s_i + \\beta_1 a_{ij}+ \\eta_{i1} a_{ij} + \\varepsilon_{ij} \\\\\n\\text{M11} \\quad \\quad  y_{ij} &= \\beta_0  + \\beta_{1{\\rm M}} s_i a_{ij} + \\beta_{1{\\rm F}}(1-s_i) a_{ij}+ \\eta_{i1} a_{ij}  + \\varepsilon_{ij} \\\\\n\\text{M12} \\quad \\quad  y_{ij} &= \\beta_0  + \\beta_{0{\\rm M}}s_i  + \\beta_{1{\\rm M}} s_i a_{ij} + \\beta_{1{\\rm F}}(1-s_i) a_{ij}+ \\eta_{i1} a_{ij}+ \\varepsilon_{ij} \\\\\n\\text{M13}   \\quad \\quad  y_{ij} &= \\beta_0 + \\beta_1 a_{ij} + \\eta_{i0} + \\eta_{i1} a_{ij} + \\varepsilon_{ij} \\\\\n\\text{M14} \\quad \\quad  y_{ij} &= \\beta_0 + \\beta_{0{\\rm M}}s_i + \\beta_1 a_{ij} + \\eta_{i0} + \\eta_{i1} a_{ij} + \\varepsilon_{ij} \\\\\n\\text{M15} \\quad \\quad  y_{ij} &= \\beta_0  + \\beta_{1{\\rm M}} s_i a_{ij} + \\beta_{1{\\rm F}}(1-s_i) a_{ij} + \\eta_{i0} + \\eta_{i1} a_{ij}  + \\varepsilon_{ij} \\\\\n\\text{M16} \\quad \\quad  y_{ij} &= \\beta_0  + \\beta_{0{\\rm M}}s_i  + \\beta_{1{\\rm M}} s_i a_{ij} + \\beta_{1{\\rm F}}(1-s_i) a_{ij} + \\eta_{i0} + \\eta_{i1} a_{ij}+ \\varepsilon_{ij}\n\\end{aligned}\n\nm1  <- lm(distance ~ age , data=Orthodont) \nm2  <- lm(distance ~ Sex + age , data=Orthodont) \nm3  <- lm(distance ~ 1 + age:Sex , data=Orthodont) \nm4  <- lm(distance ~ Sex + age:Sex , data=Orthodont) \nm5  <- lmer(distance ~ age + (1|Subject) , data=Orthodont) \nm6  <- lmer(distance ~ Sex + age + (1|Subject) , data=Orthodont) \nm7  <- lmer(distance ~ 1 + age:Sex + (1|Subject) , data=Orthodont) \nm8  <- lmer(distance ~ Sex + age:Sex + (1|Subject) , data=Orthodont) \nm9  <- lmer(distance ~ age + (-1+age|Subject) , data=Orthodont) \nm10 <- lmer(distance ~ Sex + age + (-1+age|Subject) , data=Orthodont) \nm11 <- lmer(distance ~ 1 + age:Sex + (-1+age|Subject) , data=Orthodont) \nm12 <- lmer(distance ~ Sex + age:Sex + (-1+age|Subject) , data=Orthodont) \nm13 <- lmer(distance ~ age + (age|Subject) , data=Orthodont) \nm14 <- lmer(distance ~ Sex + age + (age|Subject) , data=Orthodont) \nm15 <- lmer(distance ~ 1 + age:Sex + (age|Subject) , data=Orthodont) \nm16 <- lmer(distance ~ Sex + age:Sex + (age|Subject) , data=Orthodont) \n\n\nBIC(m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11,m12,m13,m14,m15,m16)\n\n    df      BIC\nm1   3 519.6234\nm2   4 499.4121\nm3   4 497.1948\nm4   5 501.6524\nm5   4 465.7310\nm6   5 460.9232\nm7   5 460.3152\nm8   6 461.8500\nm9   4 463.8142\nm10  5 462.7684\nm11  5 463.1800\nm12  6 464.8139\nm13  6 470.7295\nm14  7 468.0088\nm15  7 468.5409\nm16  8 470.0387\n\n\nThe best model, according to BIC, is model M7 that assumes different fixed slopes for males and females and a random intercept.\n\nOrthodont$pred_final <- fitted(m7)\nOrthodont %>% \n  ggplot() + geom_point(aes(x = age, y = distance), color=\"red\", size=2) + \n  geom_line(aes(x = age, y = pred_final)) + facet_wrap(~Subject, ncol=5) \n\n\n\n\nWe can compute 95% profile-based confidence intervals for the parameters of the model:\n\nconfint(m7)\n\n                   2.5 %     97.5 %\n.sig01         1.2911611  2.4196733\n.sigma         1.1850214  1.6142498\n(Intercept)   15.2918183 18.2304039\nage:SexMale    0.6288955  0.8810452\nage:SexFemale  0.3866637  0.6576257\n\n\nParametric bootstrap can also be used for computing confidence intervals:\n\nconfint(m7, method=\"boot\")\n\nComputing bootstrap confidence intervals ...\n\n\n                   2.5 %     97.5 %\n.sig01         1.2429167  2.3654419\n.sigma         1.1757287  1.5900091\n(Intercept)   15.3131071 18.1130782\nage:SexMale    0.6339549  0.8825424\nage:SexFemale  0.3754437  0.6597467\n\n\nThere is only one random effect in the final model. We can plot 95% prediction intervals on the random effects (\\eta_i):\n\ndotplot(ranef(m7, condVar=TRUE), strip=FALSE)\n\n$Subject"
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-linear-mixed-model.html#some-examples-of-models-and-designs",
    "href": "docs/mixed-models/map566-lecture-linear-mixed-model.html#some-examples-of-models-and-designs",
    "title": "Linear Mixed Effects Models",
    "section": "5 Some examples of models and designs",
    "text": "5 Some examples of models and designs\n\n5.1 One factor (or one-way) classification\nA “one-way classification” of data refers to data sets that are grouped according to one criterion. It can result from designed experiments, sample surveys, or observational studies.\n\n5.1.1 Repeated measures\ndataset: Rail (package: nlme)\nExperiment: Six rails chosen at random, three measurements of travel time of a ultrasonic wave through each rail.\n\ndata(Rail, package = \"nlme\")\nRail$Rail <- factor(Rail$Rail, level = unique(Rail$Rail), ordered = FALSE)\nRail %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nConsider the following linear model:\ny_{ij} = \\beta_i + \\varepsilon_{ij} \\quad , \\quad i = 1, \\ldots , 6 \\ ,  \\quad j = 1, 2, 3\nRemark the use of the notation 0 + to avoid a global intercept:\n\nlm.rail <- lm(travel ~ 0 + Rail, data = Rail)\n\nThe estimated intercepts (\\hat\\mu_i =\\hat\\mu_1+\\hat\\beta_i) for the 6 rails are\n\ncoef(lm.rail)\n\n   Rail1    Rail2    Rail3    Rail4    Rail5    Rail6 \n54.00000 31.66667 84.66667 96.00000 50.00000 82.66667 \n\n\nThese intercepts are the 6 empirical means of the travel times for the 6 rails (\\bar{y}_i,1 \\leq i \\leq 6):\n\nRail %>% group_by(Rail) %>%  summarize(mean = mean(travel))\n\n# A tibble: 6 x 2\n  Rail   mean\n  <fct> <dbl>\n1 1      54  \n2 2      31.7\n3 3      84.7\n4 4      96  \n5 5      50  \n6 6      82.7\n\n\nA linear mixed effects model considers that the 6 rails were randomly selected from a ``population’’ of rails. The rail effect is therefore treated as a random effect:\ny_{ij} = \\mu + \\eta_i + \\varepsilon_{ij} \\quad , \\quad i = 1, \\ldots , 6 \\ ,  \\quad j = 1, 2, 3\nwhere \\eta_i is the deviation from the population intercept \\mu for the i-th rail: \\eta_i \\sim^{\\mathrm{iid}} \\mathcal{N}(0, \\omega^2).\n\nlme.rail <- lmer(travel ~ 1 + (1 | Rail), data = Rail)\n\nThe population intercept \\mu is estimated by the empirical mean of the 18 travel times\n\nmean(Rail$travel)\n\n[1] 66.5\n\n\nThe estimated individual predicted travel times (\\hat{\\mu}_i) are\n\ncoef(lme.rail)\n\n$Rail\n  (Intercept)\n1    54.10852\n2    31.96909\n3    84.50894\n4    95.74388\n5    50.14325\n6    82.52631\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nThese individual parameter estimates are not anymore the empirical means (\\bar{y}_i,1 \\leq i \\leq 6). Indeed, the MAP estimate of \\mu_i combines the least square estimate \\bar{y}_i and the estimated population intercept \\hat\\mu:\n\\hat\\mu_i = \\frac{n_i \\hat\\omega^2}{n_i \\hat\\omega^2 + \\hat\\sigma^2}\\ \\bar{y}_i +\n\\frac{\\hat\\sigma^2}{n_i \\hat\\omega^2 + \\hat\\sigma^2}\\ \\hat\\mu\nwhere n_i is the number of observations for rail i (here, n_i=3).\n\nni <- 3\nomega2_hat <- VarCorr(lme.rail)$Rail[1]\nsigma2_hat <- attr(VarCorr(lme.rail)[],\"sc\")^2\nmu_hat <- fixed.effects(lme.rail)\nyi_hat <- \n  Rail %>% group_by(Rail) %>% summarize(mean_rail = mean(travel)) %>% \n  add_column(mean_pop = fixef(lme.rail))\nyi_hat <- \n  yi_hat %>% mutate(\n    map = ni*omega2_hat/(ni*omega2_hat+sigma2_hat)*mean_rail + sigma2_hat/(ni*omega2_hat+sigma2_hat)*mu_hat    \n  )\nyi_hat\n\n# A tibble: 6 x 4\n  Rail  mean_rail mean_pop   map\n  <fct>     <dbl>    <dbl> <dbl>\n1 1          54       66.5  54.1\n2 2          31.7     66.5  32.0\n3 3          84.7     66.5  84.5\n4 4          96       66.5  95.7\n5 5          50       66.5  50.1\n6 6          82.7     66.5  82.5\n\n\nWe can also check that \\hat{\\mu}_i = \\hat{\\mu}+ \\hat{\\eta}_i, where (\\hat{\\eta}_i) are the estimated random effects\n\nranef(lme.rail)\n\n$Rail\n  (Intercept)\n1   -12.39148\n2   -34.53091\n3    18.00894\n4    29.24388\n5   -16.35675\n6    16.02631\n\nwith conditional variances for \"Rail\" \n\n\n\n\n\n5.2 Two factors block design\n\n5.2.1 Design with no replications\ndataset: ergoStool (package: nlme)\nExperiment: Nine testers had to sit in four different ergonomic stools and their effort to raise was measured once.\n\ndata(ergoStool)\n# define \"Subject\" as a factor with unorderedlevels\nergoStool$Subject <- factor(unclass(ergoStool$Subject))\nhead(ergoStool)\n\nGrouped Data: effort ~ Type | Subject\n  effort Type Subject\n1     12   T1       8\n2     15   T2       8\n3     12   T3       8\n4     10   T4       8\n5     10   T1       9\n6     14   T2       9\n\nxtabs(~ Type + Subject, ergoStool)\n\n    Subject\nType 1 2 3 4 5 6 7 8 9\n  T1 1 1 1 1 1 1 1 1 1\n  T2 1 1 1 1 1 1 1 1 1\n  T3 1 1 1 1 1 1 1 1 1\n  T4 1 1 1 1 1 1 1 1 1\n\n\n\n5.2.1.1 Model with one fixed and one random factor\nIn this model, the stool type is considered as a fixed effect (\\beta_j) while the testing subject is treated as a random effect (\\eta_i)\ny_{ij} = \\mu + \\beta_j + \\eta_i + \\varepsilon_{ij} \\quad , \\quad i = 1, \\ldots , 9 \\ ,  \\quad j = 1, \\ldots , 4 where \\beta_1=0.\n\nlme.ergo1 <- lmer(effort ~ Type + (1 | Subject), data = ergoStool)\n\nEven if it is of very little interest, we could instead consider the stool type as a random effect (\\eta_j) and the testing subject as a fixed effect (\\beta_i)\ny_{ij} = \\mu + \\beta_i + \\eta_j + \\varepsilon_{ij} \\quad , \\quad i = 1, \\ldots , 9 \\ ,  \\quad j = 1, \\ldots , 4\nwhere \\beta_1=0.\n\nlme.ergo2 <- lmer(effort ~ Subject + (1 | Type) , data = ergoStool)\n\n\n\n5.2.1.2 Model with two random factors\nBoth effects (stool type and testing subject) can be treated as random effects:\ny_{ij} = \\mu + \\eta_i + \\eta_j + \\varepsilon_{ij} \\quad , \\quad i = 1, \\ldots , 9 \\ ,  \\quad j = 1, \\ldots , 4\n\nlme.ergo3 <- lmer(effort ~ (1|Subject) + (1|Type) , data = ergoStool)\n\n\n\n5.2.1.3 Comparison between these models\nWe can compare these 3 models with a linear model assuming only one fixed factor\ny_{ij} = \\mu  + \\beta_j + \\varepsilon_{ij} \\quad , \\quad i = 1, \\ldots , 9 \\ ,  \\quad j = 1, \\ldots , 4 where \\beta_1=0.\n\nlm.ergo <- lm(effort ~ Type , data = ergoStool)\ncat(\"Residual standard error: \",summary(lm.ergo)$sigma)\n## Residual standard error:  1.728037\n\n\nBIC(lm.ergo, lme.ergo1, lme.ergo2, lme.ergo3)\n\n          df      BIC\nlm.ergo    5 155.2240\nlme.ergo1  6 142.6319\nlme.ergo2 11 143.0005\nlme.ergo3  4 148.6678\n\n\n\nRemark. The interaction between the testing subject and the stool type cannot be taken into account with this design as there is no replication.\n\n\n\n\n5.2.2 Design with replications\ndataset: Machines (package: nlme)\nExperiment: Six workers were chosen randomly among the employees of a factory to operate each machine three times. The response is an overall productivity score taking into account the number and quality of components produced.\n\ndata(Machines)\nMachines$Worker <- factor(Machines$Worker, levels=unique(Machines$Worker))\nMachines %>% rmarkdown::paged_table()\n\n\n  \n\n\nxtabs(~ Machine + Worker, Machines)\n\n       Worker\nMachine 1 2 3 4 5 6\n      A 3 3 3 3 3 3\n      B 3 3 3 3 3 3\n      C 3 3 3 3 3 3\n\n\n\n5.2.2.1 Model with one fixed and one random factor, without interaction\nAlthough the operators represent a sample from the population of potential operators, the three machines are the specific machines of interest. That is, we regard the levels of Machine as fixed levels and the levels of Worker as a random sample from a population.\nA first model considers therefore the machine as a fixed effect (\\beta_j) and the subject (or worker in this example) as a random effect (\\eta_i). We don’t assume any interaction between the worker and the machine in this first model.\ny_{ijk} = \\mu + \\beta_j + \\eta_i + e_{ijk} \\quad , \\quad i = 1, \\ldots , 6 \\ ,  \\quad j = 1, 2, 3 \\ ,  \\quad k = 1, 2, 3 \\ \\text{replications}\nwhere \\beta_1=0.\n\nlme.machine1 <- lmer(score ~ Machine + (1|Worker), data = Machines)\n\n\n\n5.2.2.2 Model with one fixed and one random factor, with interaction\nWe can furthermore assume that there exists an interaction between the worker and the machine. This interaction is treated as a random effect \\eta_{ij}:\ny_{ijk} = \\mu + \\beta_j + \\eta_i + \\eta_{ij} + e_{ijk} \\quad , \\quad i = 1, \\ldots , 6 \\ ,  \\quad j = 1, 2, 3 \\ ,  \\quad k = 1, 2, 3 \\ \\text{replications}\n\nlme.machine2 <- lmer(score ~ Machine +  (1|Worker) + (1|Worker:Machine), data = Machines)\n\n\n\n5.2.2.3 Model with two random factors without interaction\nThe effect of the machine could be considered as a random effect, instead of a fixed one:\ny_{ijk} = \\mu + \\eta_i + \\eta_j + e_{ijk} \\quad , \\quad i = 1, \\ldots , 6 \\ ,  \\quad j = 1, 2, 3 \\ ,  \\quad k = 1, 2, 3 \\ \\text{replications}\n\nlme.machine3 <- lmer(score ~ (1|Machine) + (1|Worker), data = Machines)\n\n\n\n5.2.2.4 Model with two random factors with interaction\ny_{ijk} = \\mu + \\eta_i + \\eta_j + \\eta_{ij} + e_{ijk} \\quad , \\quad i = 1, \\ldots , 6 \\ ,  \\quad j = 1, 2, 3 \\ ,  \\quad k = 1, 2, 3 \\ \\text{replications}\n\nlme.machine4 <- lmer(score ~ (1|Machine) + (1|Worker) + (1|Worker:Machine), data = Machines)\n\nModel comparison:\n\nBIC(lme.machine1, lme.machine2, lme.machine3, lme.machine4)\n\n             df      BIC\nlme.machine1  5 306.8231\nlme.machine2  6 239.6215\nlme.machine3  4 317.3822\nlme.machine4  5 250.1806"
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-EM-linear-mixed-model.html#the-model",
    "href": "docs/mixed-models/map566-lecture-EM-linear-mixed-model.html#the-model",
    "title": "An EM Algorithm for Linear Mixed Effects Models",
    "section": "1 The model",
    "text": "1 The model\nConsider the following model:\n\ny_i = X_i \\, \\beta + A_i \\, \\eta_i + \\varepsilon_i   \\quad ;  \\quad 1 \\leq i \\leq N\n where\n\ny_i is a n_i-vector of observations for individual i\nX_i is a n_i \\times p design matrix\n\\beta is a p-vector of fixed effects\n\\eta_i is a q-vector of random effects\n\\varepsilon_i is a n_i-vector of residual errors\n\nThe random effects are normally distributed: \n\\eta_i \\sim^{\\mathrm{iid}} \\mathcal{N}(0_p \\ , \\ \\Omega)\n\nThe vector of residual errors \\varepsilon_i is also normally distributed. Furthermore the components \\varepsilon_{ij} are supposed to be independent and identically distributed:\n\n\\varepsilon_i \\sim \\mathcal{N}(0_{n_i} \\ , \\ \\sigma^2 I_{n_i})\n\nThen, y_i is also normally distributed:\ny_i \\sim \\mathcal{N}(X_i  \\beta \\ , \\ A_i \\Omega A_i^\\prime + \\sigma^2 I_{n_i}) \nWe can rewrite the model in matrix form for the whole data as follows:\ny  = X\\beta + A\\eta + \\varepsilon\nwhere\n\ny = \\left( \\begin{array}{c}\ny_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{N}\n\\end{array}\\right)\n\\quad ,\\quad\nX = \\left( \\begin{array}{c}\nX_{1} \\\\ X_{2} \\\\ \\vdots \\\\ X_{N}\n\\end{array}\\right)\n\\quad , \\quad\nA = \\left( \\begin{array}{cccc}\nA_{1} & 0 & \\ldots & 0 \\\\ 0 & A_{2} & \\ldots & 0 \\\\ \\vdots &\\vdots &\\ddots &\\vdots   \\\\ 0&0&\\ldots & A_{N}\n\\end{array}\\right)\n\\quad ,\\quad\n\\eta = \\left( \\begin{array}{c}\n\\eta_{1} \\\\ \\eta_{2} \\\\ \\vdots \\\\ \\eta_{N}\n\\end{array}\\right)\n\\quad ,\\quad\n\\varepsilon = \\left( \\begin{array}{c}\n\\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{N}\n\\end{array}\\right)"
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-EM-linear-mixed-model.html#maximum-likelihood-estimation-of-the-model-parameters",
    "href": "docs/mixed-models/map566-lecture-EM-linear-mixed-model.html#maximum-likelihood-estimation-of-the-model-parameters",
    "title": "An EM Algorithm for Linear Mixed Effects Models",
    "section": "2 Maximum likelihood estimation of the model parameters",
    "text": "2 Maximum likelihood estimation of the model parameters\n\n2.1 Maximization of the complete likelihood\nLet \\theta = (\\beta,\\Omega,\\sigma^2) be the set of model parameters.\nIf \\eta is known, the ML estimator of \\theta maximizes the complete log-likelihood\n\\begin{aligned}\n\\log\\ell_c(\\theta) & = \\log(\\mathbb{P}(y, \\eta \\ ; \\ \\theta))\\\\\n& = \\log(\\mathbb{P}(y | \\eta \\ ; \\ \\theta)) + \\log(\\mathbb{P}(\\eta \\ ; \\ \\theta)) \\\\\n& = \\log(\\mathbb{P}(y | \\eta \\ ; \\ \\beta, \\sigma^2)) + \\log(\\mathbb{P}(\\eta \\ ; \\ \\Omega))\n\\end{aligned}\n\nThen, (\\hat{\\beta}_c, \\hat{\\sigma}_c^2) minimizes\n -2\\log(\\mathbb{P}(y | \\eta \\ ; \\ \\beta, \\sigma^2)) =\nn\\log(2\\pi\\sigma^2) + \\frac{\\| y - X\\beta - A\\eta \\|^2 }{\\sigma^2}\n\nwhere n = \\sum_{i=1}^N n_i is the total number of observations, while \\hat{\\Omega}_c minimizes\n -2\\log(\\mathbb{P}( \\eta \\ ; \\ \\Omega)) =\nN\\log(2\\pi) + N\\log(|\\Omega|) + \\sum_{i=1}^N \\eta_i^\\prime \\Omega^{-1} \\eta_i\n\nThen,\n \\begin{aligned}\n\\hat{\\beta}_c &= (X^\\prime X)X^\\prime (y - A\\eta) \\\\\n\\hat{\\Omega}_c &= \\frac{1}{N} \\sum_{i=1}^N \\eta_i  \\eta_i^\\prime \\\\\n\\hat{\\sigma}_c^2 &= \\frac{1}{n}\\| y - X\\hat{\\beta}_c - A\\eta \\|^2 =\n\\frac{1}{n} \\left( \\| y - X\\hat{\\beta}_c \\|^2  + \\|A\\eta \\|^2 - 2 <y - X\\hat{\\beta}_c, A\\eta>  \\right)\n\\end{aligned} \nRemark that\n \\begin{aligned}\n\\|A\\eta \\|^2 & = \\sum_{i=1}^N \\|A_i\\eta_i \\|^2 \\\\\n&= \\sum_{i=1}^N \\eta_i^\\prime A_i^\\prime A_i \\eta_i \\\\\n&= \\sum_{i=1}^N {\\rm Tr}\\left(\\eta_i^\\prime A_i^\\prime A_i \\eta_i \\right) \\\\\n&= \\sum_{i=1}^N {\\rm Tr}\\left(A_i^\\prime A_i \\eta_i \\eta_i^\\prime  \\right)\n\\end{aligned} \nThe set of individual statistics used for estimating \\theta is therefore\nS(y,\\eta) = (\\eta_1, \\eta_2, \\ldots, \\eta_N,\n\\eta_1 \\eta_1^\\prime,  \\eta_2 \\eta_2^\\prime, \\ldots, , \\eta_N \\eta_N^\\prime) \nIndeed, the definition of (\\hat{\\beta}_c, \\hat{\\Omega}_c, \\hat{\\sigma}_c^2 ) above defines a function \\hat{\\Theta} such that\n \\hat{\\theta}_c = \\hat{\\Theta}(S(y,\\eta))\n\n\n2.2 The EM algorithm\nThe maximum likelihood (ML) estimator of \\theta maximizes the log-likelihood function defined as\n\\begin{aligned}\n\\log\\ell(\\theta) & = \\log(\\mathbb{P}(y_1, y_2, \\ldots , y_N \\ ; \\ \\theta))\\\\\n& = \\sum_{i=1}^{N}\\log(\\mathbb{P}(y_i \\ ; \\ \\theta))\\\\\n&= \\sum_{i=1}^{N} \\left\\{ -\\frac{n_i}{2}\\log(2\\pi) - \\frac{1}{2}\\log(|A_i \\Omega A_i^\\prime + \\sigma^2 I_{n_i}|) - \\frac{1}{2}(y_i - X_i  \\beta)^\\prime (A_i \\Omega A_i^\\prime + \\sigma^2 I_{n_i})^{-1} (y_i - X_i  \\beta) \\right\\}\n\\end{aligned}\nWhen the random effects (\\eta_i, 1\\leq i \\leq N) are unknown, the statistics S(y \\eta) cannot be computed. Then, the idea of EM is to replace S(y,\\eta) by its conditional expectation \\mathbb{E}[S(y,\\eta)|y ;\\theta].\nThe problem is that this conditional expectation depends on the unknown parameter \\theta. EM is therefore an iterative procedure, where, at iteration k:\n\nthe E-step computes S_k(y) = \\mathbb{E}[S(y,\\eta)|y ;\\theta_{k-1}]\nthe M-step updates the parameter estimate:  \\theta_k = \\hat\\Theta(S_k(y)) \n\nHere, computing \\mathbb{E}[S(y,\\eta)|y ;\\theta] reduces to computing \\mathbb{E}[\\eta_i|y_i ;\\theta] and \\mathbb{E}[\\eta_i \\eta_i^\\prime|y_i ;\\theta] for i=1, 2, \\ldots, N.\nSince the marginal distributions of y_i and \\eta_i are both Gaussian, the conditional distribution of \\eta_i is also Gaussian with a mean and a variance that can be computed. Indeed, from Bayes Theorem,\n\\begin{aligned}\n\\mathbb{P}(\\eta_i \\, | \\, y_i \\, ; \\, \\theta) &= \\frac{\\mathbb{P}(y_i \\, | \\, \\eta_i \\, ; \\, \\theta)\\mathbb{P}(\\eta_i \\, ; \\, \\theta)}{\\mathbb{P}( y_i \\, ; \\, \\theta)} \\\\\n&= C_1 \\times\n\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\| y_i-X_i\\beta-A_i\\eta_i \\|^2\n-\\frac{1}{2}\\eta_i^\\prime\\Omega^{-1} \\eta_i \\right\\} \\\\\n& = C_2 \\times \\exp\\left\\{-\\frac{1}{2}(\\eta_i-\\mu_i)^\\prime\\Gamma_i^{-1} (\\eta_i-\\mu_i) \\right\\}\n\\end{aligned}\nwhere\n\n\\Gamma_i =  \\left(\\frac{A_i^\\prime A_i}{\\sigma^2} + \\Omega^{-1}\\right)^{-1}\n\\quad ; \\quad\n\\mu_i =  \\frac{\\Gamma_i A_i^\\prime(y_i - X_i\\beta)}{\\sigma^2}  \n\nThen,\n\\begin{aligned}\n\\mathbb{E}[\\eta_i|y_i ;\\theta] &= \\mu_i \\\\\n\\mathbb{E}[\\eta_i \\eta_i^\\prime|y_i ;\\theta] &= \\mathbb{V}[\\eta_i|y_i ;\\theta] + \\mathbb{E}[\\eta_i|y_i ;\\theta] \\mathbb{E}[\\eta_i|y_i ;\\theta]^\\prime \\\\\n&= \\Gamma_i + \\mu_i \\mu_i^\\prime\n\\end{aligned}\nThen, the k-th iteration of the EM algorithm for a linear mixed effects model consists in\n\ncomputing \\mathbb{E}[\\eta_i|y_i ;\\theta_{k-1}] and \\mathbb{E}[\\eta_i\\eta_i^\\prime|y_i ;\\theta_{k-1}] for i=1,2,\\ldots,N ,\ncomputing \\theta_{k}= (\\beta_k, \\Omega_k,\\sigma_{k}^2) where\n\n \\begin{aligned}\n{\\beta}_k &= (X^\\prime X)X^\\prime (y - A\\mathbb{E}[\\eta|y ;\\theta_{k-1}]) \\\\\n{\\Omega}_k &= \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E}[\\eta_i \\eta_i^\\prime|y ;\\theta_{k-1}] \\\\\n{\\sigma}_k^2 &= \\frac{1}{n} \\left( \\| y - X{\\beta}_k \\|^2  \n+  \\sum_{i=1}^N {\\rm Tr}\\left(A_i^\\prime A_i \\mathbb{E}[\\eta_i \\eta_i^\\prime|y ;\\theta_{k-1}] \\right)\n- 2 \\sum_{i=1}^N  (y_i - X_i{\\beta}_k)^\\prime A_i \\mathbb{E}[\\eta|y ;\\theta_{k-1}]  \\right)\n\\end{aligned} \nOf course, some arbitrary initial estimates \\theta_0 should also be provided.\nThe following function returns the EM estimate \\theta_K and the log-likelihood (\\log(\\mathbb{P}(y\\ ; \\ \\theta_K), 1 \\leq k \\leq K):\n\nem.lmem <- function(y,id,X,A=X,niter=50) {\n  uid <- unique(id)\n  y <- as.matrix(y)\n  X <- as.matrix(X)\n  A <- as.matrix(A)\n  N <- length(uid)\n  n <- length(y)\n  nb.eta <- ncol(A)\n  \n  beta <- as.vector(solve(t(X)%*%X)%*%t(X)%*%y)\n  Omega <- diag(rep(1,nb.eta))\n  sigma2 <- 1\n  z <- as.vector(y - X%*%beta)\n  for (k in 1:niter) {\n    iO <- solve(Omega)\n    T <- R <- C <- 0\n    mu <- u <- NULL\n    for (i in uid ) {\n      row.i <- which(id==i)\n      Xi <- X[row.i,]\n      Ai <- A[row.i,]\n      AAi <- t(Ai)%*%Ai\n      zi <- z[row.i]\n      Gammai <- solve(AAi/sigma2 + iO)\n      mui <- (Gammai%*%t(Ai)%*%zi)/sigma2\n      mu <- c(mu, mui)\n      u <- c(u, Ai%*%mui)\n      Si <- Gammai + mui%*%t(mui)\n      R <- R + Si\n      T <- T + sum(diag(Si%*%AAi))\n      C <- C + t(mui)%*%t(Ai)%*%zi\n    }\n    beta <- as.vector(solve(t(X)%*%X)%*%t(X)%*%(y-u))\n    z <- as.vector(y - X%*%beta)\n    sigma2 <- (sum(z^2) -2*C[1] + T)/n\n    Omega <- as.matrix(R/N)\n  }\n  z <- as.vector(y - X%*%beta)\n  LL <- -0.5*n*log(2*pi)\n  for (i in uid ) {\n    row.i <- which(id==i)\n    Ai <- A[row.i,]\n    zi <- z[row.i]\n    Gi <- Ai%*%Omega%*%t(Ai) + diag(sigma2, nrow=length(row.i))\n    LL <- LL -0.5*log(det(Gi)) -0.5*t(zi)%*%solve(Gi)%*%zi\n  }\n  nb.param <- length(beta) + nb.eta*(nb.eta+1)/2 + 1\n  AIC <- -2*LL + 2*nb.param\n  BIC <- -2*LL + log(n)*nb.param\n  names(beta) <- colnames(X)\n  return(list(beta=beta, Omega=Omega, sigma2=sigma2, LL=c(logLik=LL, AIC=AIC, BIC=BIC)))\n}\n\n\n\n2.3 A slightly simplified version of EM\nBy construction,\n \\begin{aligned}\n\\mathbb{E}[ <y - X\\hat{\\beta} - A \\eta, X \\hat\\beta > | y , \\hat{\\theta} ] &=\n<y - A \\mathbb{E}[\\eta | y , \\hat{\\theta}] - X\\hat{\\beta}, X \\hat\\beta > = 0\n\\end{aligned} \nOn the other hand,\n\\mathbb{E}[ <y - X\\hat{\\beta} - A \\eta, A \\eta > | y , \\hat{\\theta} ] = 0 \nThen, \\mathbb{E}[\\| A\\eta \\|^2 | y , \\hat{\\theta}] = <y - X\\hat{\\beta}, A \\mathbb{E}[\\eta | y , \\hat{\\theta}] > and\n \\begin{aligned}\n\\hat{\\sigma}^2 &= \\frac{1}{n} \\left( \\| y - X\\hat{\\beta} \\|^2  \n-   <y - X\\hat{\\beta}, A \\mathbb{E}[\\eta|y ;\\hat\\theta]>  \\right) \\\\\n&= \\frac{1}{n} <y, y -  X\\hat{\\beta} - A \\mathbb{E}[\\eta|y ;\\hat\\theta]>\n\\end{aligned} \nImplementation of EM is then simplified:\n\nem2.lmem <- function(y,id,X,A=X,niter=50,C=NULL) {\n  uid <- unique(id)\n  y <- as.matrix(y)\n  X <- as.matrix(X)\n  A <- as.matrix(A)\n  N <- length(uid)\n  n <- length(y)\n  if (is.null(C)) \n    C <- matrix(1,ncol=ncol(A),nrow=ncol(A))\n  \n  beta <- as.vector(solve(t(X)%*%X)%*%t(X)%*%y)\n  Omega <- diag(rep(1,ncol(A)))\n  sigma2 <- 1\n  LL <- NULL\n  for (k in 1:niter) {\n    iO <- solve(Omega)\n  z <- as.vector(y - X%*%beta)\n    R <- 0\n    u <- NULL\n    for (i in uid ) {\n      row.i <- which(id==i)\n      Ai <- A[row.i,]\n      zi <- z[row.i]\n      Gammai <- solve(t(Ai)%*%Ai/sigma2 + iO)\n      mui <- (Gammai%*%t(Ai)%*%zi)/sigma2\n      u <- c(u, Ai%*%mui)\n      R <- R + Gammai + mui%*%t(mui)\n    }\n    beta <- as.vector(solve(t(X)%*%X)%*%t(X)%*%(y-u))\n    Omega <- as.matrix(R/N)*C\n    sigma2 <- mean(y*(y - X%*%beta - u))\n  }\n  names(beta) <- row.names(Omega)\n  return(list(beta=beta, Omega=Omega, sigma2=sigma2))\n}"
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-EM-linear-mixed-model.html#application-to-rat-weight-data",
    "href": "docs/mixed-models/map566-lecture-EM-linear-mixed-model.html#application-to-rat-weight-data",
    "title": "An EM Algorithm for Linear Mixed Effects Models",
    "section": "3 Application to rat weight data",
    "text": "3 Application to rat weight data\n\n3.1 Fitting a polynomial model\nLet us use our EM algorithm with the rat weight data, by fitting a polynomial of degree 2 with individual coefficients to each individual series of weights.\n\nd <- read.csv(file=\"../../data/ratWeight.csv\")\nd['week2'] <- d[\"week\"]^2\nX <- cbind(1,d[\"week\"],d[\"week2\"])\nres.em1 <- em.lmem(y=d[\"weight\"],id=d[[\"id\"]],X=X,A=X)\nprint(res.em1)\n\n$beta\n         1       week      week2 \n169.030083  31.241507  -1.102003 \n\n$Omega\n               1       week      week2\n1     823.291463 284.806010 -9.3176817\nweek  284.806010 157.156986 -5.4430666\nweek2  -9.317682  -5.443067  0.2012885\n\n$sigma2\n[1] 66.24102\n\n$LL\n   logLik       AIC       BIC \n-8691.351 17402.701 17459.821 \n\n\nLes us check that the two versions of EM give the same results:\n\nres.em1b <- em2.lmem(y=d[\"weight\"],id=d[[\"id\"]],X=X,A=X)\nprint(res.em1b)\n\n$beta\n         1       week      week2 \n169.030150  31.241562  -1.102004 \n\n$Omega\n               1       week      week2\n1     823.291479 284.806000 -9.3176812\nweek  284.806000 157.156984 -5.4430666\nweek2  -9.317681  -5.443067  0.2012885\n\n$sigma2\n[1] 66.241\n\n\nWe can compare these results with those provided by the lmer function\n\nlibrary(lme4)\nr1 <- lmer(weight ~ week + week2 + (week +  week2 |id), data=d, REML=FALSE)\nres.lmer1 <- list(beta=fixef(r1), Omega=VarCorr(r1)$id[,], sigma2=attr(VarCorr(r1), \"sc\")^2)\nprint(res.lmer1)\n\n$beta\n(Intercept)        week       week2 \n 169.087783   31.268990   -1.102913 \n\n$Omega\n            (Intercept)       week      week2\n(Intercept)   808.24868 280.687036 -9.1923803\nweek          280.68704 155.524942 -5.3886311\nweek2          -9.19238  -5.388631  0.1994466\n\n$sigma2\n[1] 66.37679\n\nprint(c(logLik=logLik(r1), AIC=AIC(r1), BIC=BIC(r1)))\n\n   logLik       AIC       BIC \n-8691.365 17402.731 17459.851 \n\n\nLet us now fit a model assuming different coefficients for males and females: ::: {.cell hash=‘map566-lecture-EM-linear-mixed-model_cache/html/unnamed-chunk-12_c7098cf5fbc911cba7c317918b856128’}\nX <- cbind(1,d[\"week\"],d[\"week2\"],(d[\"gender\"]==\"Male\"),d[\"week\"]*(d[\"gender\"]==\"Male\"),d[\"week2\"]*(d[\"gender\"]==\"Male\"))\ncolnames(X) <- c(\"Intercept\",\"week\", \"week2\", \"gender\", \"Male:week\", \"Male:week2\")\nres.em2 <- em.lmem(y=d[\"weight\"],id=d[[\"id\"]],X=X,A=X[,1:3])\nprint(res.em2)\n\n$beta\n  Intercept        week       week2      gender   Male:week  Male:week2 \n142.7060852  19.9228293  -0.7266020  52.7738330  22.7150997  -0.7533026 \n\n$Omega\n            Intercept       week       week2\nIntercept 127.3827390 -14.612490  0.61391916\nweek      -14.6124903  28.483661 -1.17684359\nweek2       0.6139192  -1.176844  0.05979207\n\n$sigma2\n[1] 66.25732\n\n$LL\n   logLik       AIC       BIC \n-8480.746 16987.493 17061.749 \n\nr2 <- lmer(weight ~ gender*week + gender*week2  + (week +  week2 |id), data=d, REML=FALSE)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 1.00951 (tol = 0.002, component 1)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue\n - Rescale variables?\n\nres.lmer2 <- list(beta=fixef(r2), Omega=VarCorr(r2)$id[,], sigma2=attr(VarCorr(r2), \"sc\")^2)\nprint(res.lmer2)\n\n$beta\n     (Intercept)       genderMale             week            week2 \n     142.7060852       52.7644663       19.9228293       -0.7266020 \n genderMale:week genderMale:week2 \n      22.6914568       -0.7525101 \n\n$Omega\n            (Intercept)       week      week2\n(Intercept) 144.4469725 -17.308995  0.7504677\nweek        -17.3089953  28.766028 -1.1928112\nweek2         0.7504677  -1.192811  0.0606887\n\n$sigma2\n[1] 65.79438\n\nprint(c(logLik=logLik(r2), AIC=AIC(r2), BIC=BIC(r2)))\n\n   logLik       AIC       BIC \n-8481.058 16988.116 17062.372 \n\nanova(r1,r2)\n\nData: d\nModels:\nr1: weight ~ week + week2 + (week + week2 | id)\nr2: weight ~ gender * week + gender * week2 + (week + week2 | id)\n   npar   AIC   BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nr1   10 17403 17460 -8691.4    17383                         \nr2   13 16988 17062 -8481.1    16962 420.61  3  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n:::\n\n\n3.2 Testing differences between control and GMO groups\nLet us test if coefficients for the control and GMO male groups are different (intercepts are assumed to be the same).\nUsing the EM algorithm,\n\ndm <- subset(d, gender==\"Male\")\nX <- cbind(1,dm[\"week\"],dm[\"week2\"],dm[\"week\"]*(dm[\"regime\"]==\"GMO\"),dm[\"week2\"]*(dm[\"regime\"]==\"GMO\"))\nem.H0 <- em.lmem(y=dm[\"weight\"],id=dm[[\"id\"]],X=X[,1:3],A=X[,1:3]) #H0:  no difference\nprint(em.H0)\n\n$beta\n         1       week      week2 \n195.460168  42.647597  -1.480638 \n\n$Omega\n               1       week      week2\n1     154.907759 -29.912537  1.3426990\nweek  -29.912537  49.469278 -2.0907193\nweek2   1.342699  -2.090719  0.1097153\n\n$sigma2\n[1] 103.8647\n\n$LL\n   logLik       AIC       BIC \n-4480.915  8981.830  9031.996 \n\nem.H1 <- em.lmem(y=dm[\"weight\"],id=dm[[\"id\"]],X=X,A=X[,1:3]) #H1: different coefficients c1 and c2\nprint(em.H1)\n\n$beta\n          1        week       week2        week       week2 \n195.4576401  44.0082633  -1.5548673  -2.7121645   0.1479159 \n\n$Omega\n               1       week      week2\n1     154.731659 -29.443131  1.3185729\nweek  -29.443131  47.596812 -1.9866343\nweek2   1.318573  -1.986634  0.1040984\n\n$sigma2\n[1] 103.8273\n\n$LL\n   logLik       AIC       BIC \n-4479.098  8982.196  9042.395 \n\n\nOr using the lmer function\n\nlmer.H0 <- lmer(weight ~ week  + week2  + (week +  week2 |id), data=dm, REML=FALSE)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.687327 (tol = 0.002, component 1)\n\nlmer.H1 <- lmer(weight ~ week + week:regime + week2 + week2:regime  + (week +  week2 |id), data=dm, REML=FALSE)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0982178 (tol = 0.002, component 1)\n\nsummary(lmer.H1)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: weight ~ week + week:regime + week2 + week2:regime + (week +  \n    week2 | id)\n   Data: dm\n\n     AIC      BIC   logLik deviance df.resid \n  8982.2   9042.4  -4479.1   8958.2     1103 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-12.6683  -0.4326   0.0197   0.4653   6.1631 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n id       (Intercept) 155.5611 12.4724             \n          week         47.6115  6.9001  -0.34      \n          week2         0.1042  0.3228   0.33 -0.89\n Residual             103.8052 10.1885             \nNumber of obs: 1115, groups:  id, 80\n\nFixed effects:\n                 Estimate Std. Error t value\n(Intercept)     195.45587    1.75361 111.459\nweek             43.90084    1.11954  39.213\nweek2            -1.55142    0.05600 -27.705\nweek:regimeGMO   -2.55616    1.48377  -1.723\nregimeGMO:week2   0.14290    0.07396   1.932\n\nCorrelation of Fixed Effects:\n            (Intr) week   week2  wk:GMO\nweek        -0.348                     \nweek2        0.354 -0.888              \nweek:rgmGMO  0.000 -0.663  0.577       \nregmGMO:wk2 -0.001  0.579 -0.663 -0.873\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0982178 (tol = 0.002, component 1)\n\nanova(lmer.H0, lmer.H1)\n\nData: dm\nModels:\nlmer.H0: weight ~ week + week2 + (week + week2 | id)\nlmer.H1: weight ~ week + week:regime + week2 + week2:regime + (week + week2 | id)\n        npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlmer.H0   10 8981.9 9032.0 -4480.9   8961.9                     \nlmer.H1   12 8982.2 9042.4 -4479.1   8958.2 3.6816  2     0.1587\n\n\nWe see that, according to AIC, BIC or LRT, H_0 cannot be rejected: based on this experiment, there is no good reason for concluding that the growth curves are different for the two groups."
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#preliminary",
    "href": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#preliminary",
    "title": "Nonlinear Mixed Effects Models",
    "section": "Preliminary",
    "text": "Preliminary\nFunctions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. We also need the lme4 and saemix package for fitting (nonlinear) mixed-model. lattice is used for graphical representation of quantities such as random and fixed effects in the mixed models.\n\nlibrary(tidyverse)\nlibrary(ggfortify)\nlibrary(lme4)\nlibrary(saemix)\nlibrary(lattice)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#introduction-the-theophylline-data-set",
    "href": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#introduction-the-theophylline-data-set",
    "title": "Nonlinear Mixed Effects Models",
    "section": "1 Introduction: the theophylline data set",
    "text": "1 Introduction: the theophylline data set\nThe theophyllineData file reports data from a study of the kinetics of the anti-asthmatic drug theophylline. Twelve subjects were given oral doses of theophylline then serum concentrations were measured at 11 time points over the next 25 hours.\n\ntheophylline <- read_csv(\"../../data/theophyllineData.csv\")\ntheophylline %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nHere, time is the time since drug administration when the sample was drawn (h), concentration is the measured theophylline concentration (mg/L) and weight is the weight of the subject (kg).\nThe 12 subjects received 320 mg of theophylline at time 0.\nLet us plot the data, i.e. the concentration versus time:\n\n\nShow the code\ntheo_plot <- theophylline %>% \n  ggplot() + aes(x = time, y = concentration) + geom_point(color=\"#993399\", size=2) +\n  xlab(\"time (h)\") + ylab(\"concentration (mg/l)\")\ntheo_plot + geom_line(color=\"#993399\", aes(group = id))\n\n\n\n\n\nWe can also plot the 12 individual concentration profiles on 12 separated plots,\n\n\nShow the code\ntheo_plot + geom_line() + facet_wrap( ~ id)\n\n\n\n\n\nThe pattern is similar for the 12 individuals: the concentration first increases during the absorption phase and then decreases during the elimination phase. Nevertheless, we clearly see some differences between these profiles which are not only due to the residual errors. In particular, we see that the patients absorb and eliminate the drug more or less rapidly.\nA population approach and the use of mixed effects models will allow us to take into account this inter individual variability."
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#fitting-nonlinear-models-to-the-data",
    "href": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#fitting-nonlinear-models-to-the-data",
    "title": "Nonlinear Mixed Effects Models",
    "section": "2 Fitting nonlinear models to the data",
    "text": "2 Fitting nonlinear models to the data\n\n2.1 Fitting a nonlinear model to a single subject\nLet us consider the first subject of this study (id=1)\n\nsubject1 <- theophylline %>% \n  filter(id == 1) %>% select(\"time\",\"concentration\")\nsubject1_plot <- subject1 %>% \n  ggplot() + aes(x = time, y = concentration) + geom_point( color=\"#993399\", size=3) + \n  xlab(\"time (h)\") + ylab(\"concentration (mg/l)\") + ylim(c(0,11))\nsubject1_plot + geom_line(color=\"#993399\")\n\n\n\n\nWe may want to fit a nonlinear model to this data of the form\ny_j = f(t_j ,\\psi) + \\varepsilon_j \\quad , \\quad 1\\leq j \\leq n\nwhere (y_j, 1\\leq j \\leq n) are the n measurements for this subject, f is the model (e.g. from pharmacokinetics), \\psi is the vector of parameters for this subject and (\\varepsilon_j, 1\\leq j \\leq n) are residual errors.\nA one compartment model with first order absorption and linear elimination to this data writes\nf(t_j ,\\psi) = \\frac{ D \\, k_a}{V(k_a-k_e)}\\, \\left( e^{- k_e \\, t} - e^{- k_a \\, t} \\right)\nwhere \\psi=(k_a,V,k_e) are the PK parameters of the model and D is the amount of drug given to the patient (here, D=320mg).\nLet us compute the least squares estimate of \\psi defined as\n\\hat\\psi = \\arg\\min_{\\psi}  \\sum_{j=1}^{n} (y_{j} - f(t_{j} ,\\psi))^2\nWe first need to implement the pharmacokinetics (PK) model:\n\nf1 <- function(psi, t){\n  D  <- 320; ka <- psi[1]; V  <- psi[2]; ke <- psi[3]\n  f  <- D*ka/V/(ka-ke)*(exp(-ke*t)-exp(-ka*t)) \n  f\n}\n\nWe can then use the nls function for fitting this (nonlinear) model to the data\n\nmodel_1 <- nls(concentration ~ f1(psi, time), start = list(psi=c(ka=1, V=40, ke=0.1)), data=subject1)\ncoef(model_1)\n\n       psi1        psi2        psi3 \n 1.77741125 29.39415966  0.05395458 \n\n\nand plot the predicted concentration f(t,\\hat\\psi)\n\n\nShow the code\ndplot <- data.frame(time = seq(0, 40, by=0.2))\ndplot$pred_1 <- predict(model_1, newdata = dplot)\nsubject1_plot + geom_line(data = dplot, aes(x = time, y=pred_1), colour=\"#339900\", size=1)\n\n\n\n\n\n\n\n2.2 Fitting a unique nonlinear model to several subjects\nInstead of fitting this model to a single patient, we may want to fit the same model to all the patients:\ny_{ij} = f(t_{ij} ,\\psi) + \\varepsilon_{ij} \\quad , \\quad 1\\leq i \\leq N \\ , \\ 1\\leq j \\leq n_i\nwhere (y_{ij}, 1\\leq j \\leq n_i) are the n_i measurements for subject i. Here, \\psi is the vector of parameters shared by the N subjects.\nIn this model, the least squares estimate of \\psi is defined as\n\\hat\\psi = \\arg\\min_{\\psi} \\sum_{i=1}^N \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij} ,\\psi))^2\nLet use the nls function with the pooled data from the 12 subjects.\n\nmodel_all <- nls(concentration ~ f1(psi, time), start = list(psi=c(ka=1, V=40, ke=0.1)), data=theophylline)\ncoef(model_all)\n\n       psi1        psi2        psi3 \n 1.57975379 33.42183643  0.07931028 \n\n\n\n\nShow the code\ndplot$pred_all <- predict(model_all, newdata = dplot)\ntheo_plot + geom_line(data = dplot, aes(x = time, y = pred_all), colour=\"#339900\", size=1)\n\n\n\n\n\nThese estimated parameters are typical parameters and this profile is a typical profile for this sample of patients: by definition, they do not take into account the variability between the patients and therefore do not provide good individual predictions.\n\n\nShow the code\ntheo_plot +  \n  geom_line(data = dplot, aes(x = time, y=pred_all), colour=\"#339900\", size=1) + \n  facet_wrap(~ id)\n\n\n\n\n\n\n\n2.3 Fitting several nonlinear models to several subjects\nWe can instead fit the same PK model with different parameters to each subject, exactly a we did above with the first patient:\ny_{ij} = f(t_{ij} ,\\psi_i) + \\varepsilon_{ij} \\quad , \\quad 1\\leq i \\leq N \\ , \\ 1\\leq j \\leq n_i\nwhere \\psi_i is the vector of parameters for patient i.\nIn this model, the least squares estimate of \\psi_i is defined as\n\\hat\\psi_i = \\arg\\min_{\\psi}  \\sum_{j=1}^{n_i} \\Big(y_{ij} - f(t_{ij} ,\\psi)\\Big)^2 \\quad , \\quad 1\\leq i \\leq N\nEach individual predicted concentration f(t,\\hat\\psi_i) seems to predict quite well the observed concentrations for the 12 subjects:\n\n\nShow the code\nres <- split(theophylline, theophylline$id) %>% \n  map(~{\n  model_i <- nls(concentration ~ f1(psi, time), \n              start = list(psi=c(ka=1, V=40,k=0.08)), \n              data = .x)\n    list(psi = coef(model_i),\n       y_hat = predict(model_i, newdata = dplot),\n       id = unique(.x$id))\n    })\npsi_hat <- map_df(res, \"psi\") %>% \n  setNames(c(\"ka\",\"V\",\"ke\")) %>% \n  add_column(id = factor(map_dbl(res, \"id\"))) \ntheo_pred <-\n  map_df(res, \"y_hat\") %>% \n  pivot_longer(everything(), names_to = \"id\", values_to = \"concentration\") %>% \n  add_column(time = rep(dplot$time, each = length(res)))\n\ntheo_plot + geom_line(data = theo_pred, aes(x=time,y=concentration), colour=\"#339900\", size=0.75) + facet_wrap(~id)"
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#nonlinear-mixed-effects-nlme-model",
    "href": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#nonlinear-mixed-effects-nlme-model",
    "title": "Nonlinear Mixed Effects Models",
    "section": "3 Nonlinear mixed effects (NLME) model",
    "text": "3 Nonlinear mixed effects (NLME) model\n\n3.1 A first basic model\nUntil now, the individual parameters (\\psi_i) were considered as fixed effects: we didn’t make any assumption about there possible values.\nIn a population approach, the N subjects are assumed to be randomly sampled from a same population of individuals. Then, each individual parameter \\psi_i is treated as a random variable.\nWe will start assuming that the \\psi_i’s are independent and normally distributed:\n\\psi_i \\sim^{\\mathrm{iid}} \\mathcal{N}(\\psi_{\\rm pop} , \\Omega)\nwhere \\psi_{\\rm pop} is a p-vector of population parameters and \\Omega a p\\times p variance-covariance matrix.\n\nRemark. This normality assumption allows us to decompose each individual parameter \\psi_i into a fixed effect \\psi_{\\rm pop} and a random effect \\eta_i:\n\\psi_i = \\psi_{\\rm pop} + \\eta_i\nwhere \\eta_i \\sim^{\\mathrm{iid}} \\mathcal{N}(0 , \\Omega).\n\nWe will also start assuming that the residual errors (\\varepsilon_{ij}) are independent and normally distributed: \\varepsilon_{ij} \\sim^{\\mathrm{iid}} \\mathcal{N}(0 , a^2).\nIn summary, we can equivalently represent a (nonlinear) mixed effects model\ni) using equations:\n\\begin{aligned}\ny_{ij} & = f(t_{ij} ,\\psi_i) + \\varepsilon_{ij}  \\\\\n\\psi_i & = \\psi_{\\rm pop} + \\eta_i\n\\end{aligned} where \\varepsilon_{ij} \\sim^{\\mathrm{iid}} \\mathcal{N}(0 , a^2) and \\eta_i \\sim^{\\mathrm{iid}} \\mathcal{N}(0 , \\Omega),\nii) or using probability distributions:\n\\begin{aligned}\ny_{ij} &\\sim \\mathcal{N}(f(t_{ij} ,\\psi_i) \\ , \\ a^2)  \\\\\n\\psi_i & \\sim \\mathcal{N}(\\psi_{\\rm pop} , \\Omega)\n\\end{aligned}\nThe model is the joint probability distribution of (y,\\psi), where y=(y_{ij}, 1\\leq i \\leq N, 1 \\leq j \\leq n_i) is the complete set of observations and \\psi = (\\psi_i, 1\\leq i \\leq N) the N vectors of individual parameters,\n\\begin{aligned}\n\\mathbb{P}(y,\\psi;\\theta) &= \\prod_{i=1}^N \\mathbb{P}(y_i,\\psi_i;\\theta) \\\\\n&= \\prod_{i=1}^N \\mathbb{P}(y_i|\\psi_i;\\theta)\\mathbb{P}(\\psi_i;\\theta)\n\\end{aligned}\n\n\n3.2 Tasks, methods and algorithms\nA detailed presentation of all the existing methods and algorithms for nonlinear mixed effect models goes far beyond the scope of this course. We will restrict ourselves to the methods and algorithms implemented in the saemix package.\n\n3.2.1 Estimation of the population parameters\nThe parameters of the model are \\theta=(\\psi_{\\rm pop}, \\Omega, a^2). Maximum likelihood estimation of \\theta consists of maximizing with respect to \\theta the observed likelihood function defined by\n\\begin{aligned}\n\\ell(\\theta,y) &\\triangleq \\mathbb{P}(y ; \\theta) \\\\\n&= \\int \\mathbb{P}(y,\\psi ;\\theta) \\, d \\psi \\\\\n&= \\prod_{i=1}^N\\int \\mathbb{P}(y_i|\\psi_i ;\\theta)\\mathbb{P}(\\psi_i ;\\theta) \\, d \\psi_i .\n\\end{aligned}\nIf f is a nonlinear function of \\psi_i, then y_i is not a Gaussian vector and the likelihood function \\ell(\\theta,y) cannot be computed in a closed form.\nSeveral algorithms exists for maximum likelihood estimation in nonlinear mixed effects models. In particular, the stochastic approximation EM algorithm (SAEM) is an iterative algorithm that converges to a maximum of the likelihood function under general conditions.\n\n\n3.2.2 Estimation of the individual parameters\nOnce \\theta has been estimated, the conditional distribution \\mathbb{P}(\\psi_i | y_i ; \\hat{\\theta}) can be used for each individual i for estimating the vector of individual parameters \\psi_i.\nThe mode of this conditional distribution is defined as\n\\begin{aligned}\n\\hat{\\psi}_i &= \\arg\\max_{\\psi_i}\\mathbb{P}(\\psi_i | y_i ; \\hat{\\theta}) \\\\\n&= \\arg\\min_{\\psi_i} \\left\\{ -2\\log(\\mathbb{P}(\\psi_i | y_i ; \\hat{\\theta})) \\right\\}\\\\\n&= \\arg\\min_{\\psi_i} \\left\\{-2\\log\\mathbb{P}( y_i | \\psi_i ; \\hat{\\theta}) -2 \\log\\mathbb{P}(\\psi_i  ; \\hat{\\theta}) \\right\\}\\\\\n&= \\arg\\min_{\\psi_i} \\left\\{ \\frac{1}{\\hat{a}^2}\\|y_i - f(t_i,\\psi_i)\\|^2 + (\\psi_i-\\hat\\psi_{\\rm pop})^\\top\\Omega^{-1}(\\psi_i-\\hat\\psi_{\\rm pop}) \\right\\}\n\\end{aligned}\nThis estimate is called the maximum a posteriori (MAP) estimate, or the empirical Bayes estimate (EBE) of \\psi_i.\n\nRemark. Since f is a nonlinear function of \\psi_i, there is no analytical expression of \\hat\\psi_i. A Newton-type algorithm should then be used to carry out this minimization problem.\n\nWe can then use the conditional mode for computing predictions, taking the philosophy that the most likely values of the individual parameters are the most suited for computing the most likely predictions:\n\\widehat{f(t_{ij} , \\psi_i)} = f(t_{ij} , \\hat\\psi_i).\n\n\n3.2.3 Estimation of the likelihood function\nPerforming likelihood ratio tests and computing information criteria for a given model requires computation of the log-likelihood \\log\\ell(\\hat{\\theta};y) \\triangleq \\log(\\mathbb{P}(y;\\hat{\\theta})).\nThe log-likelihood cannot be computed in closed-form for nonlinear mixed effects models. In the case of continuous data, approximation of the model by a Gaussian linear one allows us to approximate the log-likelihood.\nIndeed, we can linearize the model for the observations (y_{ij},\\, 1\\leq j \\leq n_i) of individual i around the vector of predicted individual parameters \\hat\\psi_i.\nLet \\partial_{\\psi}{f(t , \\psi)} be the row vector of derivatives of f(t , \\psi) with respect to \\psi. Then,\n\\begin{aligned}\ny_{ij} &\\simeq f(t_{ij} , \\hat\\psi_i) + \\nabla_{\\psi}{f(t_{ij} , \\hat\\psi_i)^\\top} \\, (\\psi_i - \\hat\\psi_i)  + \\varepsilon_{ij} \\\\\n&\\simeq f(t_{ij} , \\hat\\psi_i) +  \\nabla_{\\psi}{f(t_{ij} , \\hat\\psi_i)^\\top} \\, (\\hat\\psi_{\\rm pop} - \\hat\\psi_i)\n+  \\nabla_{\\psi}{f(t_{ij} , \\hat\\psi_i)^\\top} \\, \\eta_i  + \\varepsilon_{ij} .\n\\end{aligned}\nFollowing this, we can approximate the marginal distribution of the vector y_i by a normal one:\n\ny_{i} \\approx \\mathcal{N}\\left(\\mu_i\\, , \\, \\Gamma_i \\right),\n\nwhere\n\\begin{aligned}\n\\mu_i & = f(t_{i} , \\hat\\psi_i) +  \\nabla_{\\psi}{f(t_{i} , \\hat\\psi_i)^\\top}(\\hat\\psi_{\\rm pop} - \\hat\\psi_i) \\\\\n\\Gamma_i &=\n\\nabla_{\\psi}{f(t_{i} , \\hat\\psi_i)^\\top} \\, \\hat\\Omega \\, \\nabla_{\\psi}{f(t_{i} , \\hat\\psi_i)}^{\\top}  + \\hat{a}^2\\, I_{n_i}\n\\end{aligned}\nThe log-likelihood function is then approximated by\n\\begin{aligned}\n\\log\\ell(\\hat{\\theta};y) & = \\sum_{i=1}^N \\log\\ell(\\hat{\\theta};y_i)\\\\\n& \\simeq \\sum_{i=1}^N \\left\\{ -\\frac{n_i}{2}\\log(2 \\pi) -\\frac{1}{2}\\log(|\\Gamma_i|)\n-\\frac{1}{2} (y_i - \\mu_i)^\\top \\Gamma_i^{-1} (y_i - \\mu_i) \\right\\}\n\\end{aligned}\n\n\n3.2.4 Estimation of the Fisher information matrix\nUsing the linearized model, the variance of the maximum likelihood estimate (MLE) \\hat{\\theta}, and thus confidence intervals, can be derived from the observed Fisher information matrix (FIM), itself derived from the observed likelihood:\n\\begin{aligned}\nI({\\hat\\theta}) &  \\triangleq \\  - \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^\\top} \\log\\ell(\\hat{\\theta};y) \\\\\n& \\simeq \\frac{1}{2}\\sum_{i=1}^N \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^\\top}\\left\\{ \\log(|\\Gamma_i|)\n+ (y_i - \\mu_i)^\\top \\Gamma_i^{-1} (y_i - \\mu_i) \\right\\}\n\\end{aligned}\nThe variance-covariance matrix of \\hat\\theta can then be estimated by the inverse of the observed FIM. Standard errors (s.e.) for each component of \\hat\\theta are the standard deviations, i.e., the square root of the diagonal elements of the variance-covariance matrix."
  },
  {
    "objectID": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#fitting-a-nlme-model-to-the-theophylline-data",
    "href": "docs/mixed-models/map566-lecture-nonlinear-mixed-model.html#fitting-a-nlme-model-to-the-theophylline-data",
    "title": "Nonlinear Mixed Effects Models",
    "section": "4 Fitting a NLME model to the theophylline data",
    "text": "4 Fitting a NLME model to the theophylline data\nLet us see how to use the saemix package for fitting our model to the theophylline data.\nWe first need to create a SaemixData object, defining which column of the data file should be used and their role. In our example, concentration is the response variable y, time is the explanatory variable (or predictor) t and id is the grouping variable.\n\nsaemix_data <- saemixData(name.data       = theophylline,\n                          name.group      = \"id\",\n                          name.predictors = \"time\",\n                          name.response   = \"concentration\")\n\nThe structural model is the one compartment model with first order absorption and linear elimination previously used,\n\nmodel1_nlme <- function(psi,id,x) {\n  D   <- 320\n  t   <- x[,1]\n  ka  <- psi[id,1]\n  V   <- psi[id,2]\n  ke  <- psi[id,3]\n  fpred <- D*ka/(V*(ka-ke))*(exp(-ke*t)-exp(-ka*t))\n  fpred\n}\n\nThe model is defined in a saemixModel object. The structural model and some initial values for the vector of population parameters \\psi_{\\rm pop} are required\n\nsaemix_model <- saemixModel(model = model1_nlme,\n                            psi0  = c(ka=1,V=20,ke=0.5))\n\nSeveral options for selecting and running the algorithms can be defined, including the estimation of the individual parameters (map=TRUE), the estimation of the Fisher information matrix and the log-likelihood by linearization (fim=TRUE), or the estimation of the log-likelihood by importance sampling (ll.is=TRUE, ignored for this course).\nseed is a integer used for the random number generator: running the algorithms several times with the same seed ensures that the results will be the same.\n\nsaemix_options <- list(map=TRUE, fim=TRUE, ll.is=FALSE, displayProgress=FALSE, save=FALSE, seed=632545)\nsaemix_fit1    <- saemix(saemix_model, saemix_data, saemix_options)\n\nA summary of the results of the estimation algorithm can be displayed\n\nsaemix_fit1@results\n\nFixed effects\n Parameter Estimate   SE     CV(%)\n ka         1.8214  0.34262 18.81 \n V         32.4677  1.58635  4.89 \n ke         0.0888  0.00638  7.19 \n a.         0.7473  0.05658  7.57 \n\nVariance of random effects\n Parameter Estimate   SE      CV(%)\n omega2.ka 1.22e+00 5.54e-01 45.4  \n omega2.V  2.23e+01 1.10e+01 49.7  \n omega2.ke 2.14e-04 1.77e-04 83.0  \n\nStatistical criteria\nLikelihood computed by linearisation\n      -2LL= 345.2624 \n       AIC= 359.2624 \n       BIC= 362.6567 \n\n\nThe individual parameters estimates are also available\n\npsi <- psi(saemix_fit1)\npsi\n\n          ka        V         ke\n1  1.6668651 28.28289 0.06332123\n2  2.0102983 32.79537 0.09486541\n3  2.2742589 33.39745 0.08651181\n4  1.1962182 31.34693 0.08683357\n5  1.5237417 27.49594 0.08581835\n6  1.1129174 39.42106 0.09912336\n7  0.7322574 33.98073 0.09299638\n8  1.3679036 35.30709 0.09206904\n9  4.3241733 36.34144 0.09574774\n10 0.7043168 25.61639 0.07601221\n11 3.2113234 36.65196 0.09863764\n12 0.9488625 26.06094 0.09102658\n\n\nThese individual parameter estimates can be used for computing and plotting individual predictions\n\nsaemix_fit <- saemix.predict(saemix_fit1)\nsaemix.plot.fits(saemix_fit1)\n\n\n\n\nSeveral diagnostic fit plots can be displayed, including the plot of the observations versus individual predictions\n\nsaemix.plot.obsvspred(saemix_fit1,level=1)\n\n\n\n\nand the plot of the residuals versus time, and versus individual predictions,\n\nsaemix.plot.scatterresiduals(saemix_fit1, level=1)\n\n\n\n\n\n4.1 Some extensions of the model\n\n4.1.1 The residual error model\nIn the model y_{ij} = f(t_{ij} ,\\psi_i) + \\varepsilon_{ij}, the residual errors (\\varepsilon_{ij}) are assumed to be Gaussian random variables with mean 0. Different models can be used for the variance of the (\\varepsilon_{ij}) in a nonlinear mixed effects model. The following are some of them (more details about residual error models here).\nConstant error model:\nThe residual errors (\\varepsilon_{ij}) are independent and identically distributed:\n\\varepsilon_{ij}  \\sim^{\\mathrm{iid}} \\mathcal{N}(0 \\ , \\ a^2)\nThe variance of y_{ij} is therefore constant over time:\ny_{ij}  = f(t_{ij} ,\\psi_i) + a \\varepsilon_{ij}\nwhere \\varepsilon_{ij} \\sim^{\\mathrm{iid}} \\mathcal{N}(0, 1).\nThe error model can be defined as an argument of saemixModel (default is the constant error model)\n\nsaemix_model <- saemixModel(model=model1_nlme, psi0=c(ka=1,V=20,ke=0.5), error.model=\"constant\")\nfit.constant <- saemix(saemix_model, saemix_data, saemix_options)\n\n\nfit.constant@results\n\nFixed effects\n Parameter Estimate   SE     CV(%)\n ka         1.8214  0.34262 18.81 \n V         32.4677  1.58635  4.89 \n ke         0.0888  0.00638  7.19 \n a.         0.7473  0.05658  7.57 \n\nVariance of random effects\n Parameter Estimate   SE      CV(%)\n omega2.ka 1.22e+00 5.54e-01 45.4  \n omega2.V  2.23e+01 1.10e+01 49.7  \n omega2.ke 2.14e-04 1.77e-04 83.0  \n\nStatistical criteria\nLikelihood computed by linearisation\n      -2LL= 345.2624 \n       AIC= 359.2624 \n       BIC= 362.6567 \n\n\nProportional error model:\nProportional error models assume that the standard deviation of \\varepsilon_{ij} is proportional to the predicted response: \\varepsilon_{ij} = \\ b\\, f(t_{ij} ,\\psi_i) \\, \\varepsilon_{ij} where \\varepsilon_{ij} \\sim^{\\mathrm{iid}} \\mathcal{N}(0, 1). Then,\ny_{ij}  = f(t_{ij} ,\\psi_i) + b f(t_{ij} ,\\psi_i) \\, \\varepsilon_{ij}\n\nsaemix_model <- saemixModel(model=model1_nlme, psi0=c(ka=1,V=20,ke=0.5), error.model=\"proportional\")\nfit.proportional <- saemix(saemix_model, saemix_data, saemix_options)\n\n\nfit.proportional@results\n\nFixed effects\n Parameter Estimate   SE     CV(%)\n ka         1.7315  0.31518 18.20 \n V         32.9307  1.58820  4.82 \n ke         0.0873  0.00471  5.40 \n b.         0.1619  0.01233  7.62 \n\nVariance of random effects\n Parameter Estimate   SE      CV(%)\n omega2.ka  1.01082 4.69e-01 46.4  \n omega2.V  20.81666 1.15e+01 55.3  \n omega2.ke  0.00018 1.02e-04 56.5  \n\nStatistical criteria\nLikelihood computed by linearisation\n      -2LL= 339.6016 \n       AIC= 353.6016 \n       BIC= 356.996 \n\n\nCombined error model:\nA combined error model additively combines a constant and a proportional error model: \\varepsilon_{ij} = (a +\\ b\\, f(t_{ij} ,\\psi_i)) \\, \\varepsilon_{ij} where \\varepsilon_{ij} \\sim^{\\mathrm{iid}} \\mathcal{N}(0, 1). Then,\ny_{ij}  = f(t_{ij} ,\\psi_i) +  (a + b f(t_{ij} ,\\psi_i)) \\, \\varepsilon_{ij}\n\nsaemix_model <- saemixModel(model=model1_nlme, psi0=c(ka=1,V=20,ke=0.5), error.model=\"combined\")\nfit.combined <- saemix(saemix_model, saemix_data, saemix_options)\n\n\nfit.combined@results\n\nFixed effects\n Parameter Estimate   SE     CV(%)\n ka         1.7563  0.32470 18.49 \n V         32.4770  1.58184  4.87 \n ke         0.0891  0.00599  6.73 \n a.         0.4100  0.11978 29.21 \n b.         0.0632  0.02335 36.96 \n\nVariance of random effects\n Parameter Estimate   SE      CV(%)\n omega2.ka 1.09e+00  0.49750 45.6  \n omega2.V  2.14e+01 11.04619 51.7  \n omega2.ke 2.17e-04  0.00016 73.5  \n\nStatistical criteria\nLikelihood computed by linearisation\n      -2LL= 338.7119 \n       AIC= 354.7119 \n       BIC= 358.5912 \n\n\nExponential error model:\nIf y is known to take non negative values, a log transformation can be used. We can then write the model with one of two equivalent representations: \\begin{aligned}\n\\log(y_{ij})  & = \\log(f(t_{ij} ,\\psi_i)) + a \\varepsilon_{ij} \\\\\ny_{ij}  & = f(t_{ij} ,\\psi_i) \\ e^{a \\varepsilon_{ij}}\n\\end{aligned}\n\nsaemix_model <- saemixModel(model=model1_nlme, psi0=c(ka=1,V=20,ke=0.5), error.model=\"exponential\")\nfit.exponential <- saemix(saemix_model, saemix_data, saemix_options)\n\n\nfit.exponential@results\n\nFixed effects\n Parameter Estimate   SE     CV(%)\n ka         1.4688  0.26865 18.29 \n V         32.2163  1.55149  4.82 \n ke         0.0889  0.00498  5.60 \n a.         0.1769  0.01343  7.59 \n\nVariance of random effects\n Parameter Estimate   SE      CV(%)\n omega2.ka 7.31e-01  0.33904 46.4  \n omega2.V  1.76e+01 10.65453 60.6  \n omega2.ke 1.89e-04  0.00011 58.5  \n\nStatistical criteria\nLikelihood computed by linearisation\n      -2LL= 364.4892 \n       AIC= 378.4892 \n       BIC= 381.8836 \n\n\n\n\n4.1.2 Transformation of the individual parameters\nClearly, not all distributions are Gaussian. To begin with, the normal distribution has support \\mathbb[R}, unlike many parameters that take values in precise intervals. For instance, some variables take only positive values (e.g., volumes and transfer rate constants) and others are restricted to bounded intervals.\nFurthermore, the Gaussian distribution is symmetric, which is not a property shared by all distributions. One way to extend the use of Gaussian distributions is to consider that some transformation of the parameters in which we are interested is Gaussian.\ni.e., assume the existence of a monotonic function h such that h(\\psi_i) is normally distributed. For a sake of simplicity, we will consider here a scalar parameter \\psi_i. We then assume that\n h(\\psi_i) \\sim  \\mathcal{N}(h(\\psi_{\\rm pop}) \\ , \\ \\omega^2).\nOr, equivalently,\nh(\\psi_i) = h(\\psi_{\\rm pop}) + \\eta_i\nwhere \\eta_i \\sim \\mathcal{N}(0,\\omega^2).\nLet us now see some examples of transformed normal pdfs.\nLog-normal distribution:\nA log-normal distributions ensures nonnegative values and is widely used for describing distributions of physiological parameters.\nIf \\psi_i is log-normally distributed, the 3 following representations are equivalent:\n\\begin{aligned}\n\\log(\\psi_i) & \\sim \\mathcal{N}( \\log(\\psi_{\\rm pop}) \\ , \\omega^2)  \\\\\n\\log(\\psi_i) &= \\log(\\psi_{\\rm pop}) + \\eta_i \\\\\n\\psi_i &= \\psi_{\\rm pop} e^{\\eta_i}\n\\end{aligned}\nLogit-normal distribution:\nThe logit function is defined on (0,1) and take its value in \\mathbb[R}: For any x in (0,1),\n \\mathrm{logit}(x) = \\log \\left(\\frac{x}{1-x}\\right) \\Longleftrightarrow\nx = \\frac{1}{1+e^{-\\mathrm{logit}(x)}}\n\nAn individual parameter \\psi_i with a logit-normal distribution takes its values in (0,1). The logit of \\psi is normally distributed, i.e.,\n\\mathrm{logit}(\\psi_i) = \\log \\left(\\frac{\\psi_i}{1-\\psi_i}\\right)  \\ \\sim \\ \\ \\mathcal{N}( \\mathrm{logit}(\\psi_{\\rm pop}) \\ , \\ \\omega^2),\nProbit-normal distribution:\nThe probit function is the inverse cumulative distribution function (quantile function) \\psi^{-1} associated with the standard normal distribution \\mathcal{N}(0,1). For any x in (0,1), \n\\mathrm{probit}(x) = \\psi^{-1}(x) \\ \\Longleftrightarrow\n\\prob{\\mathcal{N}(0,1) \\leq \\mathrm{probit}(x)} = x\n\nAn individual parameter \\psi_i with a probit-normal distribution takes its values in (0,1). The probit of \\psi_i is normally distributed:\n\\mathrm{probit}(\\psi_i) = \\psi^{-1}(\\psi_i) \\ \\sim \\  \\mathcal{N}( \\mathrm{probit}(\\psi_{\\rm pop}) \\ , \\ \\omega^2) . \nThe distribution of each individual parameter can be defined using the argument transform.par (0=normal, 1=log-normal, 2=probit, 3=logit). Default are normal distributions, i.e. a vector of 0.\nIf we want to use, for example, a normal distribution for V and log-normal distributions for k_a and k_e, then, transform.par should be the vector c(1,0,1):\n\nsaemix_model<-saemixModel(model         = model1_nlme,\n                          psi0          = c(ka=1,V=20,ke=0.5),\n                          transform.par = c(1,0,1))\n\nsaemix_fit2 <-saemix(saemix_model, saemix_data, saemix_options)\n\n\nsaemix_fit2@results\n\nFixed effects\n Parameter Estimate   SE     CV(%)\n ka         1.5866  0.30502 19.22 \n V         32.5432  1.68413  5.18 \n ke         0.0877  0.00589  6.72 \n a.         0.7316  0.05551  7.59 \n\nVariance of random effects\n Parameter Estimate   SE     CV(%)\n omega2.ka  0.3937   0.1753  44.5 \n omega2.V  26.0922  12.4489  47.7 \n omega2.ke  0.0196   0.0198 101.0 \n\nStatistical criteria\nLikelihood computed by linearisation\n      -2LL= 338.1239 \n       AIC= 352.1239 \n       BIC= 355.5183 \n\n\n\nRemark. Here, \\omega^2_{ka} and \\omega^2_{ke} are the variances of \\log(k_{a_i}) and \\log(k_{e_i}) while \\omega^2_{V} is the variance of V_i.\n\n\n\n4.1.3 Models with covariates\nLet c_i = (c_{i1},c_{i2}, \\ldots , c_{iL}) be a vector of individual covariates, i.e. a vector of individual parameters available with the data. We may want to explain part of the variability of the non observed individual parameters (\\psi_i) using these covariates.\nWe will only consider linear models of the covariates. More precisely, assuming that h(\\psi_i) is normally distributed, we will decompose h(\\psi_i) into fixed and random effects:\nh(\\psi_i) = h(\\psi_{\\rm pop})  + \\sum_{\\ell=1}^L c_{i\\ell}\\beta_{\\ell} + \\eta_i\n\nRemark. \\psi_{\\rm pop} is the typical value of \\psi_i if the covariates c_{i1}, …, c_{iL} are zero for a typical individual of the population.\n\nLet us consider a model where the volume V_i is normally distributed and is a linear function of the weight w_i:\nV_i = \\beta_0 + \\beta \\, w_i + \\eta_{V,i}\nAssuming that the weight of a typical individual of the population is w_{\\rm pop}, the predicted volume for this individual is not \\beta_0 but \\beta_0 + \\beta w_{\\rm pop}.\nIf we use instead the centered weight w_i-w_{\\rm pop}, we can now write the model as\nV_i = V_{\\rm pop} + \\beta \\, (w_i-w_{\\rm pop}) + \\eta_{V,i} \\ .\nIndeed, the predicted volume for a typical individual is now V_{\\rm pop}.\nAssume that we decide to use 70kg as typical weight in the theophylline study. The saemixData object now needs to include w_i-70:\n\ntheophylline$w70 <- theophylline$weight - 70\nsaemix_data <- saemixData(name.data       = theophylline,\n                          name.group      = c(\"id\"),\n                          name.predictors = c(\"time\"),\n                          name.response   = c(\"concentration\"),\n                          name.covariates = c(\"w70\"))\n\nHere, only the volume V is function of the weight. The covariate model is therefore encoded as vector (0,1,0).\n\nsaemix_model <- saemixModel(model           = model1_nlme,\n                            psi0            = c(ka=1,V=20,ke=0.5),\n                            transform.par   = c(1,0,1),\n                            covariate.model = c(0,1,0))\n\nsaemix_fit3 <- saemix(saemix_model, saemix_data, saemix_options)\n\n\nsaemix_fit3@results\n\nFixed effects\n Parameter   Estimate   SE     CV(%) p-value\n ka           1.5927  0.31047 19.49  -      \n V           32.7203  1.38033  4.22  -      \n beta_w70(V)  0.3370  0.13798 40.94  0.00729\n ke           0.0871  0.00623  7.15  -      \n a.           0.7247  0.05485  7.57  -      \n\nVariance of random effects\n Parameter Estimate   SE    CV(%)\n omega2.ka  0.4063  0.1804 44.4  \n omega2.V  15.0473  8.0341 53.4  \n omega2.ke  0.0272  0.0218 80.3  \n\nStatistical criteria\nLikelihood computed by linearisation\n      -2LL= 333.2907 \n       AIC= 349.2907 \n       BIC= 353.1699 \n\n\nHere, \\hat\\beta_{w70} = 0.33 means that an increase of the weight of 1kg leads to an increase of the predicted volume of 0.33l.\nThe p-value of the test H_0: \\ \\beta_{w70}=0 versus H_1: \\ \\beta_{w70}\\neq 0 is 0.01 We can then reject the null and conclude that the predicted volume significantly increases with the weight.\nImagine that we now use a log-normal distribution for the volume V_i. It is now the log-volume which is a linear function of the transformed weight.\nWe can assume, for instance, that the log-volume is a linear function of the centered log-weight:\n\\log(V_i) = \\log(V_{\\rm pop}) + \\beta \\, \\log(w_i/w_{\\rm pop}) + \\eta_{V,i} \\ .\nOr, equivalently,\nV_i = V_{\\rm pop}  \\left(\\frac{w_i}{w_{\\rm pop}}\\right)^{\\beta} e^{\\eta_{V,i}} \\ .\nWe see that, using this model, the predicted volume for a typical individual is V_{\\rm pop}.\nThe saemixData object now needs to include \\log(w_i/70) a covariate,\n\ntheophylline$lw70 <- log(theophylline$weight/70)\nsaemix_data <- saemixData(name.data       = theophylline,\n                          name.group      = c(\"id\"),\n                          name.predictors = c(\"time\"),\n                          name.response   = c(\"concentration\"),\n                          name.covariates = c(\"lw70\"))\n\nThe covariate model is again encoded as the (row) vector (0,1,0) but the transformation is now encoded as 1 for the three parameters\n\nsaemix_model <- saemixModel(model           = model1_nlme,\n                            psi0            = c(ka=1,V=20,ke=0.5),\n                            transform.par   = c(1,1,1),\n                            covariate.model = c(0,1,0))\n\nsaemix_fit4 <- saemix(saemix_model, saemix_data, saemix_options)\n\n\nsaemix_fit4@results\n\nFixed effects\n Parameter    Estimate   SE    CV(%) p-value\n ka            1.5760  0.3071 19.49  -      \n V            32.3729  1.4005  4.33  -      \n beta_lw70(V)  0.7597  0.2994 39.42  0.00559\n ke            0.0881  0.0062  7.04  -      \n a.            0.7269  0.0550  7.57  -      \n\nVariance of random effects\n Parameter Estimate   SE     CV(%)\n omega2.ka 0.4056   0.18007 44.4  \n omega2.V  0.0149   0.00787 52.7  \n omega2.ke 0.0251   0.02099 83.6  \n\nStatistical criteria\nLikelihood computed by linearisation\n      -2LL= 333.2833 \n       AIC= 349.2833 \n       BIC= 353.1626 \n\n\n\n\n4.1.4 Correlations between random effects\nUntil now, the random effects were assumed to be uncorrelated, i.e. the variance-covariance matrix \\Omega was a diagonal matrix (default covariance model for saemix).\nCorrelations between random effects can be introduced with the input argument covariance.model, a square matrix of size equal to the number of parameters in the model, giving the variance-covariance structure of the model: 1s correspond to estimated variances (in the diagonal) or covariances (off-diagonal elements). The structure of the matrix \\Omega should be block.\nConsider, for instance a model where k_a is fixed in the population, i.e. \\omega_{k_a}=0 (and thus k_{a_i}={k_a}_{\\text{pop}} for all i), and where \\log(V) and \\log(k_e) are correlated, i.e \\eta_V and \\eta_{k_e}) are correlated:\n\nsaemix_model<-saemixModel(model           = model1_nlme,\n                          psi0            = c(ka=1,V=20,ke=0.5),\n                          transform.par   = c(1,1,1),\n                          covariate.model = t(c(0,1,0)),\n                          covariance.model = matrix(c(0,0,0,0,1,1,0,1,1),nrow=3))\n\nsaemix_fit5 <- saemix(saemix_model, saemix_data, saemix_options)\n\n\nsaemix_fit5@results\n\nFixed effects\n Parameter    Estimate   SE    CV(%) p-value\n ka            1.5385  0.1327  8.63  -      \n V            33.2076  1.7624  5.31  -      \n beta_lw70(V)  0.3536  0.3090 87.38  0.126  \n ke            0.0823  0.0098 11.91  -      \n a.            1.0804  0.0779  7.21  -      \n\nVariance of random effects\n Parameter Estimate   SE    CV(%)\n omega2.V  0.0179   0.0102 57.1  \n omega2.ke 0.0913   0.0569 62.4  \n\nStatistical criteria\n\nCorrelation matrix of random effects\n          omega2.V omega2.ke\nomega2.V   1.000   -0.319   \nomega2.ke -0.319    1.000   \nLikelihood computed by linearisation\n      -2LL= 392.5674 \n       AIC= 408.5674 \n       BIC= 412.4467"
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#preliminary",
    "href": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#preliminary",
    "title": "Graph clustering",
    "section": "Preliminary",
    "text": "Preliminary\nFunctions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. The package igraph is a great library for network data manipulation (interface exists in Python)\nWe will also use the package mixtools, which implements EM for simple mixture models to check our own implementation.\n\n$$ \n% definitions related to\n%\n%definitions related to convergences \n%————————————————————————-% % Definitions %————————————————————————-% \n\\newcommand{W}{} \\newcommand{W}{} \\newcommand{W}{} $$\n\n\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(aricode)\nlibrary(missSBM)\nlibrary(sbm)\ntheme_set(theme_bw())\n\n\noptions(tinytex.engine = 'xelatex')"
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#introduction",
    "href": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#introduction",
    "title": "Graph clustering",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n1.1 Network data and binary graphs: minimal notation\nA network is a collection of interacting entities. A graph is the mathematical representation of a network.\nIn what follow, a graph \\mathcal{G}=(\\mathcal{V},\\mathcal{E}) is a mathematical structure consisting of\n\na set \\mathcal{V}=\\left\\{1,\\dots,n\\right\\} of vertices or nodes\na set \\mathcal{E}=\\left\\{e_1,\\dots,e_p:e_k=(i_k,j_k)\\in (\\mathcal{V}\\times\\mathcal{V})\\right\\} of edges\nthe number of vertices |\\mathcal{V}| is called the order\nthe number of edges |\\mathcal{E}| is called the size\n\nThe connectivity of a binary undirected (symmetric) graph \\mathcal{G}= (\\mathcal{V},\\mathcal{E}) is captured by the |\\mathcal{V}|\\times |\\mathcal{V}| matrix Y, called the adjacency matrix \n  (Y)_{ij} = \\begin{cases}\n  1  & \\text{ if } i \\sim j,\\\\\n  0  & \\text{otherwise}.\n\\end{cases}\n For a valued of weighted graph, a similar definition would be\n\n  (Y)_{ij} = \\begin{cases}\n  w_{ij}  & \\text{ if } i \\sim j,\\\\\n  0  & \\text{otherwise}.\n\\end{cases}\n where w_{ij} is the weight associated with edge i\\sim j.\n\n\n1.2 The French political Blogosphere\nThe frenchblog2007 data is a network dataset which consists of a single day snapshot of over 200 political blogs automatically extracted the 14 October 2006 and manually classified by the “Observatoire Présidentielle” project. It is part of the missSBM package. It is provided as an igraph object with 196 nodes. The vertex attribute “party” provides a classification of the nodes.\n\ndata(\"frenchblog2007\")\nsummary(frenchblog2007)\n\nIGRAPH 7b93b75 UN-- 196 1432 -- \n+ attr: name (v/c), party (v/c)\n\nigraph::V(frenchblog2007)$party %>% table() %>% as_tibble() %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nA visual representation of the network data with nodes colored according to the political party each blog belongs to is achieved as follows:\n\n\nShow the code\nplot.igraph(frenchblog2007,\n  vertex.color = factor(V(frenchblog2007)$party),\n  vertex.label = NA\n )\n\n\n\n\n\nAnother commonly used representation is via a matrix view, where the adjacency matrix is re-ordered column-wise and row-wise according to a predefined classification. In the frenchblog2007 data, nodes are originally reordered according to their party:\n\n\nShow the code\nfrenchblog2007 %>% as_adj(sparse = FALSE) %>% plotMyMatrix()\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn this example, one can see that the pattern of connections between the nodes is highly related to the blog classification (the political party). However, just like with any kind of clustering, this is note always the case: the data may support a natural grouping of the node which is not necessarily related a predefined classification.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor convenience, in the following,\n\nwe remove the isolated nodes or node with degree equal to one1\nwe denote by Y the adjacency matrix encoding the network\nwe extract the political party of the nodes as a categorical variable\n\n** Our objective is now to automatically find a partitioning of the node, i.e. a clustering, that groups together nodes with similar connectivity pattern. This is known as graph clustering.**\n\n\n\nblog <- frenchblog2007 %>%  delete_vertices(which(degree(frenchblog2007) <= 1))\nparty <- V(blog)$party %>% as_factor()\nY     <- blog %>% as_adjacency_matrix()\nn_nodes <- gorder(blog)\nn_edges <- gsize(blog)\nparty %>% table() %>% as_tibble() %>% rmarkdown::paged_table()"
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#spectral-clustering",
    "href": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#spectral-clustering",
    "title": "Graph clustering",
    "section": "2 Spectral Clustering",
    "text": "2 Spectral Clustering\nWe start by a popular algorithm which can be seen as the equivalent of k-means algorithm for clustering network data: the spectral clustering (see Von Luxburg (2007)). This algorithm is based on the spectral properties of graph, and in particular of the Laplacian matrix, which we briefly recap here. A detail introduction is made by (Chung and Graham 1997).\nHere, we motivate the introduction of the Laplacian matrix via the graph-cut problem:\n\n2.1 Graph-cut\nFirst, we need to measure the importance or quantity of information associated to a node or a subset of nodes in the graph. The degree is a natural candidate: we define\n\n\\begin{aligned}\n\\mathrm{degree}_i & = d_i = \\sum_{j} Y_{ij}, \\\\\n\\mathrm{Vol}(\\mathcal{S}) & = \\sum_{i\\in\\mathcal{S}} d_i , \\\\\n\\end{aligned}\n where the volume of a subset \\mathcal{S} of nodes is the cumulated degrees2.\nFor instance, in the French blog data set, the volume associated to each party would be\n\ndegree(blog) %>% split(party) %>% map_dbl(sum) %>% \n  as_tibble() %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nSecond, let us define the cut between two set of nodes that form a partition in the graph:\n\n\\mathrm{cut}(\\mathcal{V}_A, \\mathcal{V}_B) = \\sum_{i\\in\\mathcal{V}_A, j\\in\\mathcal{V}_B} Y_{ij}, \\qquad \\mathcal{V}_A \\cup \\mathcal{V}_B = \\mathcal{V}\n that is, the cut is the sum of the weights of the edge set that connect the two components clV_A and \\mathcal{V}_B. For instance, in this simple binary graph, the graph cut between \\mathcal{V}_A= \\{1,2,3,4,10\\} and \\mathcal{V}_B= \\{5,6,7,8,9\\} is 2.\n\n\n\n\n\nWe can easily define a function to compute the cut\n\ncut <- function(graph, A, B) {\n  res <- sum(as_adj(graph, type = \"upper\")[A , B]) \n  res\n}\nV <- V(g)$name\nA <- V[1:5]\nB <- setdiff(V, A)\ncut(g, A, B)\n\n[1] 2\n\n\n\n\n\n\n\n\nIdea\n\n\n\nA natural criterion to cluster a graph into two homogeneous groups of node is to find the two sets (the partition) that minimizes the cut.\nBased on this principle, the normalized cut consider the connectivity between group relative to the volume of each groups:\n\n\\begin{aligned}\n\\mathop{\\mathrm{arg\\ min}}_{\\{\\mathcal{V}_A, \\mathcal{V}_B\\}} \\mathrm{cut}^{N}(\\mathcal{V}_A, \\mathcal{V}_B),  \n\\quad \\mathrm{cut}^{N}(\\mathcal{V}_A, \\mathcal{V}_B) & = \\frac{\\mathrm{cut}(\\mathcal{V}_A, \\mathcal{V}_B)}{\\mathrm{Vol}(\\mathcal{V}_A)} + \\frac{\\mathrm{cut}(\\mathcal{V}_A, \\mathcal{V}_B)}{\\mathrm{Vol}(\\mathcal{V}_B)} \\\\\n& =  \\mathrm{cut}(\\mathcal{V}_A, \\mathcal{V}_B)\\frac{\\mathrm{Vol}(\\mathcal{V}_A) + \\mathrm{Vol}(\\mathcal{V}_B)}{\\mathrm{Vol}(\\mathcal{V}_A)\\mathrm{Vol}(\\mathcal{V}_B)} \\\\\n\\end{aligned}\n\n\n\nOur function is easily amende to compute the normalized version of the graph-cut:\n\ncut <- function(graph, A, B, normalized  = TRUE) {\n\n  Y <- as_adj(graph, type=\"upper\")\n  res <- sum(Y[A , B]) \n\n  if (normalized) {\n    volA <- sum(Y[A, A])\n    volB <- sum(Y[B, B])\n    res <- res * (volA + volB) / (volA * volB)\n  }\n\n  res\n}\nA <- sample(1:gorder(blog), 100)\nB <- setdiff(1:gorder(blog), A)\ncut(blog, A, B)\n\n[1] 2.148412\n\n\nThe above problem can be formalized as follows: a partition into two clusters of the graph can be defined by a vector of \\{-1, 1\\}^n. Indeed,\n\nx = (x_i)_{i=1,\\dots,n} =\n\\begin{cases}\n-1 & \\mathrm{if} \\quad  i\\in \\mathcal{V}_A, \\\\\n1 & \\mathrm{if} \\quad  i\\in \\mathcal{V}_B. \\\\\n\\end{cases}\n Then, letting D the diagonal matrix of degrees, is not difficult to show that3\n\nx^\\top (D - Y) x = x^\\top D x - ( x^\\top D x - 2 \\mathrm{cut} (\\mathcal{V}_A, \\mathcal{V}_B)),\n so that\n\n\\mathrm{cut} (\\mathcal{V}_A, \\mathcal{V}_B) = \\frac12 x^\\top (D - Y) x.\n From this, we can show that minimizing the normalized graph-cut is equivalent to solving an integer programming problem:\n\\begin{aligned}\n& \\mathop{\\mathrm{arg\\ min}}_{\\{\\mathcal{V}_A, \\mathcal{V}_B\\}}  \\mathrm{cut}^{N}(\\mathcal{V}_A, \\mathcal{V}_B) \\\\[1.5ex]\n\\Leftrightarrow \\quad & \\mathop{\\mathrm{arg\\ min}}_{x\\in\\{-1, 1\\}^n} \\frac{x^\\top (D - Y) x}{x^\\top D x}, \\quad \\text{s.c.} \\quad x^\\top D \\mathbf{1}_n = 0,\n\\end{aligned}\n where the constraint imposes only discrete values in x.\nThis problem is combinatorial (and NP-hard). However, if we relax to x\\in[-1,1]^n, it turns to a simple eigenvalue problem\n\n\\mathop{\\mathrm{arg\\ min}}_{x\\in[-1, 1]^n} x^\\top (D - Y) x, \\quad \\text{s.c.} \\quad x^\\top D x = 1 \\Leftrightarrow (D - Y) x = \\lambda D x .\n where \\mathbf{L}= D - Y is called the Laplacian matrix of the graph \\mathcal{G}.\n\n\n\n\n\n\nProposition: Spectrum of \\mathbf{L}\n\n\n\nThe n\\times n matrix \\mathbf{L} has the following properties: \n  \\mathbf{x}^\\top \\mathbf{L}\\mathbf{x}= \\frac{1}{2} \\sum_{i,j} Y_{ij} (x_i - x_j)^2, \\quad \\forall \\mathbf{x}\\in\\mathbb{R}^n .\n\n\n\\mathbf{L} is a symmetric, positive semi-definite matrix,\n\\mathbf{1}_n is in the kernel of L since L \\mathbf{1}_n = 0,\nThe first normalized eigen vector with eigen value \\lambda> 0 is solution to the relaxed graph cut problem\n\n\n\nThe Laplacian is easily (and fastly) computed in R thanks to the igraph package. Let us compute this for the French blog graph:\n\nL <- laplacian_matrix(blog)\n\n\n\n\n\n\n\nHeuristics for spectral clustering\n\n\n\nSpectral clustering exploits the spectral property of \\mathbf{L}, by building heuristic based on the above properties. We review some variants in what follows.\n\n\n\n\n2.2 Bi-partionning and the Fiedler vector\nThe Fiedler vector is the named sometimes given to the normalized eigen vector associated with the smallest positive eigen-value of \\mathbf{L}. It thus solves the above relaxed graph-cut problem and can be used to compute a bi-partition of a graph.\nLet us check how we can use theses quantities to partition the French blogosphere.\nWe first extract the Fiedler vector\n\nspec_L <- eigen(L)\npractical_zero <- 1e-12\nlambda  <- min(spec_L$values[spec_L$values>practical_zero])\nfiedler <- spec_L$vectors[, which(spec_L$values == lambda)]\n\nThen, we plot the values of the Fiedler vector and color point according to the party to check if a part of the underlying structure of the network can indeed be found based on this quantity.\n\nqplot(y = fiedler, colour = party) + \n  viridis::scale_color_viridis(discrete = TRUE)\n\n\n\n\nAlso, and since the original motivation of the graph-cut is for two-way partionning, we collapse levels from the vector of party into a simplified left/right view (we keep the analysts into a third separated group)\n\nleft_vs_right <- \n  forcats::fct_collapse(party, \n    left = c(\"green\", \"left\", \"far-left\", \"center-left\"),\n    right = c(\"right\", \"liberal\", \"center-rigth\"),\n    analyst = \"analyst\"\n  )\n\n\nqplot(y = fiedler, colour = left_vs_right) + \n  viridis::scale_color_viridis(discrete = TRUE)\n\n\n\n\nWe can see that there exists an optimum value (or threshold) to separate left from right: if we compute the adjusted Rand index4 between a bi-partionning obtained by thresholding the Fidler vector and our reference vector left_vs_right, we can see that there exists an optimal threshold maximizing this quantity:\n\nthresholds <- seq(-.1, .1, len = 100)\nARIs <- map_dbl(thresholds, ~ARI(left_vs_right, fiedler > .))\nqplot(thresholds, ARIs) + geom_vline(xintercept = thresholds[which.max(ARIs)]) + theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3 Spectral clustering algorithms\nVarious heuristics generalize the approach defined above to more than 2 groups. They all rely on the spectral property of the Laplacian given above, from which we can derive the following additional properties:\n\n\n\n\n\n\nSpectrum and Graph\n\n\n\n\nThe multiplicity of the first eigen value (0) of \\mathbf{L} determines the number of connected components in the graph.\nThe larger the second non trivial (positive) eigenvalue, the higher the connectivity of \\mathcal{G}.\n\n\n\nBased on these two properties, spectral clustering algorithms follow the following general principles:\n\nCompute spectral decompostion of \\mathbf{L} to perform clustering in the eigen space\nFor a graph with K connected components, the first K eigen-vectors are \\mathbf{1} spanning the eigenspace associated with eigenvalue 0\nApplying a simple clustering algorithm to the rows of the K first eigenvectors separate the components\n\n\\rightsquigarrow The principle generalizes to a fully connected graph (with a single component): spectral clustering tends to separates groups of nodes which are highly connected together\n\n\n\n\n\n\nVariants in the definition of the graph Laplacian\n\n\n\nThe normalized Laplacian matrix \\mathbf{L} (see Ng, Jordan, and Weiss (2002)) is defined by\n\n      \\mathbf{L}_N = \\mathbf{D}^{-1/2}\\mathbf{L}\\mathbf{D}^{-1/2} = \\mathbf{I}- \\mathbf{D}^{-1/2} \\mathbf{A}\\mathbf{D}^{-1/2}.\n\nThe absolute Laplacian matrix \\mathbf{L}_{abs} (see Rohe, Chatterjee, and Yu (2011)) is defined by\n\n  \\mathbf{L}_{abs} = \\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2} = \\mathbf{I}- \\mathbf{L}_N,\n with eigenvalues 1-\\lambda_n \\leq \\dots \\leq 1-\\lambda_2 \\leq 1-\\lambda_1 = 1, where 0=\\lambda_1\\leq \\dots \\leq \\lambda_n are the eigenvalues of \\mathbf{L}_N.\n\n\n\n\n\n\n\n\nPseudo code for normalized spectral clustering\n\n\n\nAs described in Ng, Jordan, and Weiss (2002)\n\nCompute the laplacian matrix \\mathbf{L};\nCompute the n\\times K matrix \\mathbf{U} of eigen vectors with the K smallest eigen values\nNormalize \\mathbf{U} row-wise\nApply k-means to (\\tilde{\\mathbf{U}}_i)_{i=1,\\dots,n}\n\n\n\n\nspectral_clustering <- function(graph, nb_cluster, normalized = TRUE) {\n  \n  ## Compute Laplcian matrix\n  L <- laplacian_matrix(graph, normalized = normalized) \n  ## Generates indices of last (smallest) K vectors\n  selected <- rev(1:ncol(L))[1:nb_cluster] \n  ## Extract an normalized eigen-vectors\n  U <- eigen(L)$vectors[, selected, drop = FALSE]  # spectral decomposition\n  U <- sweep(U, 1, sqrt(rowSums(U^2)), '/')    \n  ## Perform k-means\n  res <- kmeans(U, nb_cluster, nstart = 40)$cl\n  \n  res\n}\n\nLet use perform spectral clsutering on the blogosphere for various number of group:\n\nnb_cluster <- 1:20\nmap(nb_cluster, ~spectral_clustering(blog, .)) %>% \n  map_dbl(ARI, party) %>% \n  qplot(nb_cluster, y = .) + geom_line() + theme_bw()\n\n\n\n\nOnce reorder according to the best clustering (obtained k=6) groups, the orginal data matrix looks as follows\n\n\nShow the code\nplotMyMatrix(as_adj(blog, sparse = FALSE),\n  clustering = list(row = spectral_clustering(blog, 6)))\n\n\n\n\n\n\n\n\n\n\n\nSome limitations\n\n\n\nHence, as expected, spectral clustering does a great job for recovering community structure in the network. Yet,\n\nWhat if other kind of patterns (like star/hub nodes) structure the network\nWhat if we do not have any clue on the target number of cluster?\n\nA model-based approach, like the one presented below, overcomes these issues"
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#model-based-clustering-for-graph-data",
    "href": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#model-based-clustering-for-graph-data",
    "title": "Graph clustering",
    "section": "3 Model-based clustering for graph data",
    "text": "3 Model-based clustering for graph data\n\n\n\n\n\n\nMotivation\n\n\n\nWe are still looking for an underlying organization in a observed network, yet with model-based approaches, so that statistical inference would be possible.\nThis session essentially aims to present the stochastic block model, a random graph model tailored for clustering vertices. As will be seen, this model is can be interpreted as a special mixture model for graph data. Hence, the relationship between spectral clustering for network and the Stochastic block model is the same as the one between the k-means clustering and Gaussian mixture models.\n\n\n\n3.1 The Erdös-Renyi model\nWe start by the most simple, yet natural model for random graph, the Erdös-Rényi model.\n\nDefinition 1 (Erdös-Rényi model) Let \\mathcal{V}= {1,\\dots,n} be a set of fixed vertices. The (simple) Erdös-Rényi model \\mathcal{G}(n,\\pi) assumes random edges between pairs of nodes with probability \\pi. In orther word, the (random) adjacency matrix Y is such that\n\n  Y_{ij} \\sim \\mathcal{B}(\\pi)\n\n\nA direct consequence is that the distribution of the (random) degree D_i of a vertex i follows a binomial distribution, i.e.,\nD_i \\sim b(n -1, \\pi).\n\nG1 <- igraph::sample_gnp(10, 0.1)\nG2 <- igraph::sample_gnp(10, 0.9)\nG3 <- igraph::sample_gnp(100, .02)\npar(mfrow=c(1,3))\nplot(G1, vertex.label=NA) ; plot(G2, vertex.label=NA)\nplot(G3, vertex.label=NA, layout=layout.circle)\n\n\n\n\nBecause of its simplicity, a lot of mathematical derivation can be done with this model, yet its utility for adjusting real-world network is very limited since\n\nthe degree distribution is too concentrated, with no high degree nodes,\nAll nodes are equivalent,\nNo modularity is observed.\n\nFor instance, for the graph G3 sampled above, the empirical degree distribution and and basic clustering support the over homogeneous structure of the ER model.\n\n\n\n\n\n\n\n3.2 The Stochastic Block Model (SBM)\nThe SBM generalizes the Erdös-Rényi model in a mixture framework (see Nowicki and Snijders (2001), Daudin, Picard, and Robin (2008)). It provides\n\na statistical framework to adjust and interpret the parameters\na flexible yet simple specification that fits many existing network data\n\n\n\n\n\n\n\nStochastic Block Model: definition\n\n\n\nLet\n\n\\{1, \\dots, n \\} be some fixed nodes,\nwith some unknown colors picked up from \\mathcal{C}=\\{\\color{#fab20a}{\\bullet},\\color{#0000ff}{\\bullet},\\color{#008000}{\\bullet}\\}\n\nDenote by\n\n\\alpha_\\bullet = \\mathbb{P}(i \\in \\bullet), \\bullet\\in\\mathcal{C} the prior probability of group memberships,\n\\pi_{\\color{#fab20a}{\\bullet}\\color{#0000ff}{\\bullet}} = \\mathbb{P}(i \\leftrightarrow j | i\\in\\color{#fab20a}{\\bullet},j\\in\\color{#0000ff}{\\bullet}), the probability of connexion between groups.\n\nIn the binary Stochastic Block Model, the adjacency matrix Y_{ij} is random, with probability of connexion between a dyad (i,j) being defined conditionnaly on their respective group memberships, described by a vector of random variables (Z_i)_{i=1,\\dots,n}.\n\n\\begin{aligned}\nZ_i = \\mathbf{1}_{\\{i \\in \\bullet\\}}  \\ & \\sim^{\\text{iid}} \\mathcal{M}(1,\\alpha), \\\\\nY_{ij} \\ | \\ \\{i\\in\\color{#fab20a}{\\bullet},j\\in\\color{#0000ff}{\\bullet}\\}\n& \\sim^{\\text{ind}} \\mathcal{B}(\\pi_{\\color{#fab20a}{\\bullet}\\color{#0000ff}{\\bullet}})\\\\\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.1 A generative model\nThe SBM does not assume assume any particular a priori structure of the network: because it is a probabilistic, generative model, we can easy simulate SBM-based network data with various topologies Here are a few examples:\n\nCommunity network\n\n\npi <- matrix(c(0.3,0.02,0.02,0.02,0.3,0.02,0.02,0.02,0.3),3,3)\ncommunities <- igraph::sample_sbm(100, pi, c(25, 50, 25))\nplot(communities, vertex.label=NA, vertex.color = rep(1:3,c(25, 50, 25)))\n\n\n\n\n\nStar network\n\n\npi <- matrix(c(0.05,0.3,0.3,0),2,2)\nstar <- igraph::sample_sbm(100, pi, c(4, 96))\nplot(star, vertex.label=NA, vertex.color = rep(1:2,c(4,96)))\n\n\n\n\n\n\n3.2.2 Degree distribution\nBecause it is defined as a simple mixture of Erdös-Rényi, the degree distribution of the binary SBM has a simple close form\n\nDefinition 2 (SBM: Degree distribution) The conditional degree distribution of a node i\\in q is\n\n  D_i | i \\in q \\sim \\mathrm{b}(n-1,\\bar\\pi) \\approx \\mathcal{P}(\\lambda_q), \\qquad \\bar\\pi_q = \\sum_{\\ell=1}^Q \\alpha_\\ell \\pi_{q\\ell}, \\quad \\lambda_q = (n-1)\\bar\\pi_q\n\nFrom this, we deduce the degree distribution of a node i, which can be approximated by a mixture of Poisson distributions:\n\n  \\mathbb{P}(D_i = k) = \\sum_{q=1}^Q\\alpha_q \\exp{\\left\\{-\\lambda_q\\right\\}} \\ \\frac{\\lambda_q^k}{k !}"
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#estimation-variational-inference-of-the-binary-sbm",
    "href": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#estimation-variational-inference-of-the-binary-sbm",
    "title": "Graph clustering",
    "section": "4 Estimation: Variational Inference of the binary SBM",
    "text": "4 Estimation: Variational Inference of the binary SBM\n\n4.1 SBM: a latent variable model\nRecall tha we have fixed nodes \\{1, \\dots, n \\} with hidden colors \\mathcal{C}=\\{\\color{#fab20a}{\\bullet},\\color{#0000ff}{\\bullet},\\color{#008000}{\\bullet}\\}. We observe the following\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe need to estimate the model parameters and the clustering:\n\n\\theta = \\{\\boldsymbol\\alpha = (\\alpha_\\bullet), \\boldsymbol\\Pi = (\\pi_{\\color{#fab20a}{\\bullet}\\color{#0000ff}{\\bullet}})\\}\nColors of i, i.e. the \\mathbf{Z}_i (the clustering)\n\nJust like with Gaussian mixture models, maximizing the marginal log likelihood is not straightforward\n\\ell_\\theta(\\mathbf{Y}_i) = \\log p_\\theta(\\mathbf{Y}_i) = \\log \\int_{\\mathcal{Z}} \\prod_{(i,j)} p_\\theta(Y_{ij} | Z_i, Z_j ) \\, p_\\theta(\\mathbf{Z}) \\mathrm{d}\\mathbf{Z}\nIntegration over \\mathcal{Z} = \\otimes_{k=0,\\dots,K}\\{1,\\dots, C_k\\}^{n_k} is intractable: we have \\mathrm{card}(C)^n terms!\nThe natural solution is to maximum the likelihood via an EM algorithm, which use the following decomposition of the loglikelihood:\n\\log p_\\theta(\\mathbf{Y}) = \\mathbb{E}_{p_\\theta(\\mathbf{Z}\\,|\\,\\mathbf{Y})} [\\log p_\\theta(\\mathbf{Y}, \\mathbf{Z})] + \\mathcal{H}[p_\\theta(\\mathbf{Z}\\,|\\,\\mathbf{Y})], \\quad \\text{ with } \\mathcal{H}(p) = -\\mathbb{E}_p(\\log(p))\n\n\n\n\n\n\nIntractable EM\n\n\n\nEM requires to evaluate (some moments of) p_\\theta(\\mathbf{Z}\\,|\\,\\mathbf{Y}), which is not known for the SBM (and was explicit for Gaussian mixture models).\nOne could use (at least)\n\nMCMC/Bayesian approaches for evaluating quantities depending on p_\\theta(\\mathbf{Z}\\,|\\,\\mathbf{Y})$\nVariational approaches, which generalize EM by approximating p_\\theta(\\mathbf{Z}\\,|\\,\\mathbf{Y})\n\n\n\n\n\n4.2 Variational approach: general case\nThe idea is to find a proxy q_\\psi(\\mathbf{Z}) \\approx p_\\theta(\\mathbf{Z} | \\mathbf{Y}) picked in a convenient class of distribution \\mathcal{Q}\nq(\\mathbf{Z})^\\star  \\arg\\min_{q\\in\\mathcal{Q}} D\\left(q(\\mathbf{Z}), p(\\mathbf{Z} | \\mathbf{Y})\\right).\nKüllback-Leibler is a popular choice .small[(error averaged wrt the approximated distribution)]\nKL\\left(q(\\mathbf{Z}), p(\\mathbf{Z} | \\mathbf{Y})\\right) = \\mathbb{E}_q\\left[\\log \\frac{q(z)}{p(z)}\\right] = \\int_{\\mathcal{Z}} q(z) \\log \\frac{q(z)}{p(z)} \\mathrm{d}z.\nFor mixture model, the natural class of distribution used for approximation is the multinomial\n\\mathcal{Q} = \\Big\\{q_\\psi: \\, q_\\psi(\\mathbf{Z}) = \\prod_i q_{\\psi_i}(\\mathbf{Z}_i), \\, q_{\\psi_i}(\\mathbf{Z}_i) = \\mathcal{M}\\left(\\mathbf{Z}_i; \\boldsymbol\\tau_i\\right), \\, \\psi_i = \\{\\boldsymbol{\\tau}_i\\}, \\boldsymbol{\\tau}_i \\in  \\mathbb{R}^{K} \\Big\\}\nAnd we maximize the ELBO (Evidence Lower BOund), a lower bound of the log-likelihood:\nJ(\\theta, \\psi) = \\log p_\\theta(\\mathbf{Y}) - KL[q_\\psi (\\mathbf{Z}) ||  p_\\theta(\\mathbf{Z} | \\mathbf{Y})]  = \\mathbb{E}_{q} [\\log p_\\theta(\\mathbf{Y}, \\mathbf{Z})] + \\mathcal{H}[q_\\psi(\\mathbf{Z})]\nThe variational EM has the following form\n\nInitialization: get \\mathbf{T}^0 = \\{\\tau_{ik}^0\\} with Absolute Spectral Clustering\nM step: update \\theta^h = \\{ \\boldsymbol\\alpha^h, \\boldsymbol\\Pi^h\\}\nVE step: find the optimal q_\\psi, by updating \\psi^h= (\\psi^h_{i})_i = \\mathbf{T}^{h} = \\mathbb{E}_{q^{h}} (\\mathbf{Z}):\n\n\\psi^h = \\arg \\max J(\\theta^h, \\psi) = \\arg\\min_{\\psi} KL[q_\\psi(\\mathbf{Z}) \\,||\\, p_{\\theta^h}(\\mathbf{Z}\\,|\\,\\mathbf{Y})]\n\\theta^h = \\arg\\max J(\\theta, \\psi^h) = \\arg\\max_{\\theta} \\mathbb{E}_{q_{\\psi^h}} [\\log p_{\\theta}(\\mathbf{Y}, \\mathbf{Z})]\n\n\n4.3 Variational EM for SBM: ingredients\nWe now derive the quantity for the special case of SBM:\n\n4.3.1 Variational bound\nJ(\\theta, \\tau ; \\mathbf{Y}) = \\sum_{(i,j)} \\sum_{(k,\\ell)} \\tau_{ik} \\tau_{j\\ell} \\log b(Y_{ij},\\pi_{k\\ell }) + \\sum_{i} \\sum_{k} \\tau _{ik} \\log (\\alpha_k/\\tau_{ik})\n\n\n4.3.2 M-step (Analytical)\n\\alpha_k = \\frac{1}{n} \\sum_{i} \\tau_{i k} , \\quad  \\pi_{k\\ell } = \\frac{\\sum_{(i,j)} \\tau_{ik}\\tau_{j\\ell} Y_{ij}}{\\tau_{ik}\\tau_{j\\ell}} \\qquad \\left({\\boldsymbol\\alpha} = \\mathbf{1}_n^\\top\\mathbf{T}, \\quad {\\boldsymbol\\Pi} =  \\frac{\\mathbf{T}^\\top \\mathbf{Y} \\mathbf{T}}{\\mathbf{T}^\\top  \\mathbf{T}} \\right)\n\n\n4.3.3 Variational E-step (fixed point)\n\\tau_{ik} \\varpropto \\alpha_k \\prod_{(i,j)} \\prod_{\\ell} b(Y_{ij} ; \\pi_{k\\ell})^{\\tau_{j\\ell}}\n\n\n4.3.4 Model Selection\n\\mathrm{vICL}(K) = \\mathbb{E}_{q} [\\log L(\\hat{\\theta)}; \\mathbf{Y}, \\mathbf{Z}] - \\frac{1}{2} \\left(\\frac{K(K+1)}{2} \\log \\frac{n(n-1)}{2} + (K-1) \\log (n) \\right)\n\n\n\n4.4 SBM: the french blogosphere\nThere exist a variety of packages to fit SBM: we advice here using sbm and misssbm to stick to the course5\n\nblocks <- 1:18\nsbm_full <- estimateMissSBM(as_adj(blog), blocks, \"node\")\n\n\n\n Adjusting Variational EM for Stochastic Block Model\n\n    Imputation assumes a 'node' network-sampling process\n\n Initialization of 18 model(s). \n Performing VEM inference\n    Model with 6 blocks.\n    Model with 13 blocks.\n    Model with 9 blocks.\n    Model with 7 blocks.\n    Model with 12 blocks.\n    Model with 15 blocks.\n    Model with 3 blocks.\n    Model with 8 blocks.\n    Model with 16 blocks.\n    Model with 18 blocks.\n    Model with 1 blocks.\n    Model with 11 blocks.\n    Model with 14 blocks.\n    Model with 5 blocks.\n    Model with 2 blocks.\n    Model with 17 blocks.\n    Model with 4 blocks.\n    Model with 10 blocks.\n Looking for better solutions\n Pass 1   Going forward +++++++++++++++++\n                                                                                                    \n Pass 1   Going backward +++++++++++++++++\n                                                                                                    \n\n\n\n4.4.1 Convergence monitoring (ELBO)\n\nplot(sbm_full, \"monitoring\")\n\n\n\n\n\n\n4.4.2 Model Selection (vICL)\n\nplot(sbm_full)\n\n\n\n\n\n\n4.4.3 Parameters\n\nplot(sbm_full$bestModel, \"meso\")\n\n\n\n\n\n\n4.4.4 Clustering I\n\nplot(sbm_full$bestModel, dimLabels = list(row = \"blogs\", col = \"blogs\"))\n\n\n\n\n\n\n4.4.5 Clustering II\n\nplot(sbm_full$bestModel, \"expected\", dimLabels = list(row = \"blogs\", col = \"blogs\"))\n\n\n\n\n\n\n4.4.6 Clustering III\n\nsp_clustering <- spectral_clustering(blog, sbm_full$bestModel$fittedSBM$nbBlocks)\nmap(sbm_full$models, \"fittedSBM\") %>%\n  map(\"memberships\") %>% \n  map_dbl(ARI, party) %>% \n  qplot(blocks, y = .)  + theme_bw()"
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#references",
    "href": "docs/mixture-models/map566-lecture-stochastic-blockmodels.html#references",
    "title": "Graph clustering",
    "section": "References",
    "text": "References\n\n\n\n\nChung, Fan RK, and Fan Chung Graham. 1997. Spectral Graph Theory. 92. American Mathematical Soc.\n\n\nDaudin, J-J, Franck Picard, and Stéphane Robin. 2008. “A Mixture Model for Random Graphs.” Stat. Comp. 18 (2): 173–83.\n\n\nNg, Andrew Y, Michael I Jordan, and Yair Weiss. 2002. “On Spectral Clustering: Analysis and an Algorithm.” In Advances in Neural Information Processing Systems, 849–56.\n\n\nNowicki, K., and T. A. B. Snijders. 2001. “Estimation and Prediction for Stochastic Blockstructures.” J. Am. Stat. Soc. 96 (455): 1077–87.\n\n\nRohe, Karl, Sourav Chatterjee, and Bin Yu. 2011. “Spectral Clustering and the High-Dimensional Stochastic Blockmodel.” The Annals of Statistics 39 (4): 1878–1915.\n\n\nVon Luxburg, Ulrike. 2007. “A Tutorial on Spectral Clustering.” Statistics and Computing 17 (4): 395–416."
  },
  {
    "objectID": "docs/mixture-models/map566-lab-mixture-models.html#preliminary",
    "href": "docs/mixture-models/map566-lab-mixture-models.html#preliminary",
    "title": "Mixture Models",
    "section": "1 Preliminary",
    "text": "1 Preliminary\nOnly functions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation.\n\nlibrary(tidyverse)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/mixture-models/map566-lab-mixture-models.html#faithful-data",
    "href": "docs/mixture-models/map566-lab-mixture-models.html#faithful-data",
    "title": "Mixture Models",
    "section": "2 Faithful data",
    "text": "2 Faithful data\nThe faithful data consist of the waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA.\n\ndata(\"faithful\")\nfaithful %>% \n  ggplot() + aes(x=waiting) + geom_histogram() + xlab(\"waiting (mm)\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFit variants of a mixture of two Gaussian distributions to the faithful data.\n\nassuming different proportions, means and variances for the 2 distributions\nassuming same variances\nassuming same means\nassuming same proportions"
  },
  {
    "objectID": "docs/mixture-models/map566-lab-mixture-models.html#epilepsy-data",
    "href": "docs/mixture-models/map566-lab-mixture-models.html#epilepsy-data",
    "title": "Mixture Models",
    "section": "3 Epilepsy data",
    "text": "3 Epilepsy data\nThe data seizures.csv consists of daily counts of epileptic seizures for 6 patients.\n\nseizures <- readr::read_csv('../../data/seizures.csv')\nseizures %>% \n  ggplot() + aes(x = time, y = nsz) + geom_point() + facet_wrap( ~ id)\n\n\n\n\n\nSelect the id 12 and fit a Poisson distribution to the number of seizures for this patient.\nImplement and use a EM algorithm for fitting a mixture of 2 Poisson distributions to this data\nCompare the two models"
  },
  {
    "objectID": "docs/mixture-models/map566-doc-mixture-models-example.html#preliminary",
    "href": "docs/mixture-models/map566-doc-mixture-models-example.html#preliminary",
    "title": "Clustering and classification with model based approaches",
    "section": "Preliminary",
    "text": "Preliminary\nFunctions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. We also need the package mclust, which are commonly used to fit mixture models in R, as weel as palmerpenguins for the illustrative data set. aricode is used for clustering comparison, VGAM to fit multinomial models.\n\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(mclust)\nlibrary(aricode)\nlibrary(VGAM)\nlibrary(palmerpenguins)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/mixture-models/map566-doc-mixture-models-example.html#the-palmer-penguins-data-set",
    "href": "docs/mixture-models/map566-doc-mixture-models-example.html#the-palmer-penguins-data-set",
    "title": "Clustering and classification with model based approaches",
    "section": "1 The Palmer penguins data set",
    "text": "1 The Palmer penguins data set\nThe palmerpenguins data 1 (Horst, Hill, and Gorman (2020)) contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.\nThese data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data were imported directly from the Environmental Data Initiative (EDI) Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the Palmer Station Data Policy.\nThis data et is an alternative to Anderson’s Iris data (see datasets::iris). There are both a nice example for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning.\n\ndata(\"penguins\", package = \"palmerpenguins\")\npenguins %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nWe remove row with NA values and columns year, island and sex to only keep the four continuous attributes and the species of each individual:\n\npenguins <- penguins %>% drop_na() %>% \n  select(-year, -island, -sex)\n\nThe pair plot show some structure that could find by clustering or descreibe by a Gaussian mixture model:\n\nspecies_col <- c(\"darkorange\",\"purple\",\"cyan4\")\nggpairs(penguins, columns = c(2:5), aes(color = species)) + \n    scale_color_manual(values = species_col) +\n    scale_fill_manual(values = species_col)\n\n\n\n\n\n1.1 Supervised classification\n\n1.1.1 Logistic regression for a binary variable\nLet y_i be a binary response that take its values in \\{0,1\\} and let c_{i1}, \\ldots, c_{iM} be M explanatory variables (or predictors).\nFormally, the logistic regression model is that\n\n\\log\\left(\\frac{\\mathbb{P}(y_i=1)}{\\mathbb{P}(y_i=0)}\\right) = \\log\\left(\\frac{\\mathbb{P}(y_i=1)}{1 - \\mathbb{P}(y_i=1)}\\right)\n= \\beta_0 + \\sum_{m=1}^M \\beta_m c_{im}\n Then,\n\\mathbb{P}(y_i=1) = \\frac{1}{1+ e^{-\\beta_0 -  \\sum_{m=1}^M \\beta_m c_{im}}}\nWe try to predict the binary indicator variable for species Adelie with this model:\n\npenguins <- penguins %>% mutate(adelie = (species == 'Adelie') * 1) \nlogistic_Adelie <- glm(\n  adelie ~ \n    bill_length_mm + \n    bill_depth_mm  + \n    flipper_length_mm + \n    body_mass_g, family = binomial, data = penguins)\nsummary(logistic_Adelie)\n\n\nCall:\nglm(formula = adelie ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    body_mass_g, family = binomial, data = penguins)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.328   0.000   0.000   0.000   1.652  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)  \n(Intercept)       27.195927  28.156975   0.966   0.3341  \nbill_length_mm    -5.106876   2.730998  -1.870   0.0615 .\nbill_depth_mm      8.953805   5.014702   1.786   0.0742 .\nflipper_length_mm  0.052471   0.119287   0.440   0.6600  \nbody_mass_g        0.006281   0.003952   1.589   0.1120  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 456.5751  on 332  degrees of freedom\nResidual deviance:   9.4492  on 328  degrees of freedom\nAIC: 19.449\n\nNumber of Fisher Scoring iterations: 13\n\n\nThe linear predictor can be recovered as follows (e.g. for penguins #127)\n\nX <- as.matrix(cbind(1, penguins[, 2:5]))\nbeta <- coefficients(logistic_Adelie)\nall.equal(\n  predict(logistic_Adelie, type = \"link\"), \n  as.numeric(X %*% beta),\n  check.attributes = FALSE)\n\n[1] TRUE\n\n\nThe fitted value as follows:\n\nall.equal(\n  predict(logistic_Adelie, type = \"response\"),\n  as.numeric(1 / ( 1 + exp( - (X %*% beta)))),\n check.attributes = FALSE)\n\n[1] TRUE\n\n\nThe ARI is close to 1 in that case\n\naricode::ARI(penguins$adelie, round(predict(logistic_Adelie, type = \"response\")))\n\n[1] 0.9641751\n\n\n\n\n1.1.2 Logistic regression with more than two classes\nAssume now that y_i takes its values in \\{1,2\\ldots,L\\}. The logistic regression model now writes\n \\log\\left(\\frac{\\mathbb{P}(y_i=k)}{\\mathbb{P}(y_i=L)}\\right)\n= \\beta_{k0} +  \\sum_{m=1}^M \\beta_{k m} c_{im} \\quad , \\quad k=1,2,\\ldots,L\n where we set, for instance, \\beta_{L0}=\\beta_{L1}=\\ldots=\\beta_{LM}=0 for identifiabilty reason. Then,\n\n\\mathbb{P}(y_i=k) = \\frac{e^{\\beta_{k0} +  \\sum_{m=1}^M \\beta_{k m} c_{im}}}\n{\\sum_{j=1}^K e^{\\beta_{j0} +  \\sum_{m=1}^M \\beta_{j m} c_{im}}}\n\\quad , \\quad k=1,2,\\ldots,L\n Let us first code all species modalities as dummy variables:\n\npenguins <- penguins %>% \n  mutate(gentoo    = (species == 'Gentoo') * 1) %>% \n  mutate(chinstrap = (species == 'Chinstrap') * 1) \npenguins %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nWe now fit a multinomial model to this 3-class problem:\n\nmultinomial_penguins <- \n  vglm(cbind(gentoo, adelie, chinstrap) ~ \n    bill_length_mm + \n    bill_depth_mm  + \n    flipper_length_mm + \n    body_mass_g, family = multinomial, data = penguins)\nsummary(multinomial_penguins)\n\n\nCall:\nvglm(formula = cbind(gentoo, adelie, chinstrap) ~ bill_length_mm + \n    bill_depth_mm + flipper_length_mm + body_mass_g, family = multinomial, \n    data = penguins)\n\nCoefficients: \n                     Estimate Std. Error z value Pr(>|z|)   \n(Intercept):1        7.294722  60.956154   0.120  0.90474   \n(Intercept):2       26.107462  22.696081   1.150  0.25002   \nbill_length_mm:1    -1.298442   1.061332  -1.223  0.22118   \nbill_length_mm:2    -3.207140   1.205017  -2.661  0.00778 **\nbill_depth_mm:1     -1.532394   1.462216  -1.048  0.29464   \nbill_depth_mm:2      3.341561   1.951686      NA       NA   \nflipper_length_mm:1  0.170417   0.316373   0.539  0.59012   \nflipper_length_mm:2  0.095995   0.125187      NA       NA   \nbody_mass_g:1        0.010319   0.006946   1.486  0.13739   \nbody_mass_g:2        0.009378   0.004705   1.993  0.04624 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3])\n\nResidual deviance: 5.7536 on 656 degrees of freedom\n\nLog-likelihood: -2.8768 on 656 degrees of freedom\n\nNumber of Fisher scoring iterations: 18 \n\nWarning: Hauck-Donner effect detected in the following estimate(s):\n'bill_depth_mm:2', 'flipper_length_mm:2'\n\n\nReference group is level  3  of the response\n\n\n\nposterior_prob <- predict(multinomial_penguins, type = \"response\")\n\nWarning in object@family@linkinv(eta = object@predictors, extra = new.extra):\nfitted probabilities numerically 0 or 1 occurred\n\nmatplot(posterior_prob)\n\n\n\nclustering_map <- apply(posterior_prob, 1, which.max)\n\nwe get a perfect clustering of the penguins with this model (on the training set!)\n\nclustering_map <- apply(posterior_prob, 1, which.max)\naricode::ARI(clustering_map, penguins$species)\n\n[1] 1\n\n\n\n\n\n1.2 Non supervised classification\nIgnoring the known labels (species) of the penguin data, let us identify three clusters with the k-means method and compute the missclassification rate:\n\nkclust <- kmeans(penguins[, 2:5], centers = 3, nstart = 10)\naricode::ARI(kclust$cl,  penguins$species)\n\n[1] 0.3078089\n\n\nLet us know fit a mixture of three multidimensional Gaussian distributions.\nFunction Mclust fits many multivariate Gaussian mixture model, with various parametric form for the covariances. Let us force the model to have spherical variance with equal volume (thus close to the k-means setting):\n\nGMM_EII <- Mclust(penguins[, 2:4], G = 3, modelNames = 'EII')\n\nThe fit is poor\n\nplot(GMM_EII, \"classification\")\n\n\n\n\n\naricode::ARI(penguins$species, map(GMM_EII$z))\n\n[1] 0.6363721\n\n\nWe can let Mclust chose the “best” model, relying on BIC: we found “VVE”, an ellipsoidal model with equal orientation\n\nGMM_best <- Mclust(penguins[, 2:4], G = 3)\nplot(GMM_best, \"classification\")\n\n\n\nsummary(GMM_best, parameters = TRUE)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVE (ellipsoidal, equal orientation) model with 3 components: \n\n log-likelihood   n df       BIC       ICL\n      -2647.114 333 23 -5427.815 -5441.159\n\nClustering table:\n  1   2   3 \n150 119  64 \n\nMixing probabilities:\n        1         2         3 \n0.4468457 0.3573179 0.1958364 \n\nMeans:\n                       [,1]      [,2]      [,3]\nbill_length_mm     38.91639  47.56843  49.05177\nbill_depth_mm      18.32181  14.99638  18.48159\nflipper_length_mm 189.90109 217.23500 196.53408\n\nVariances:\n[,,1]\n                  bill_length_mm bill_depth_mm flipper_length_mm\nbill_length_mm          8.744628      1.529139          7.411096\nbill_depth_mm           1.529139      1.622307          3.249674\nflipper_length_mm       7.411096      3.249674         38.620270\n[,,2]\n                  bill_length_mm bill_depth_mm flipper_length_mm\nbill_length_mm          7.470233     1.5733271          9.389060\nbill_depth_mm           1.573327     0.8783978          3.965065\nflipper_length_mm       9.389060     3.9650651         45.191222\n[,,3]\n                  bill_length_mm bill_depth_mm flipper_length_mm\nbill_length_mm         11.020717      2.043209          9.017857\nbill_depth_mm           2.043209      1.134670          4.030423\nflipper_length_mm       9.017857      4.030423         47.437865\n\n\nWe get an almost perfect clustering with the MAP:\n\naricode::ARI(penguins$species, map(GMM_best$z))\n\n[1] 0.9355119"
  },
  {
    "objectID": "docs/mixture-models/map566-doc-mixture-models-example.html#references",
    "href": "docs/mixture-models/map566-doc-mixture-models-example.html#references",
    "title": "Clustering and classification with model based approaches",
    "section": "References",
    "text": "References\n\n\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218."
  },
  {
    "objectID": "docs/mixture-models/map566-lab-stochastic-blockmodels.html#introduction",
    "href": "docs/mixture-models/map566-lab-stochastic-blockmodels.html#introduction",
    "title": "Graph Clustering: Spectral methods and Stochastic Blockmodels",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis tutorial introduces the graph clustering techniques seen during the lectures, that is,\n\nSpectral methods, mainly spectral clustering and its variants\nModel-based approaches, namely, stochastic blocmodels (binary, weigthed, bipartite, w/o covariates)\n\nThese methods will be illustrated for the analysis of an (ecological) network data set.\n\n1.1 Requirements\nThe packages required for the analysis are sbm and igraph (sbm Team großBM (Barbillon, Chiquet, Donnet, Léger) (2021) is based on the package blockmodels Leger (2021)), plus some others for data manipulation and representation.\n\nlibrary(igraph)    # graph manipulation\nlibrary(sbm)       # stochastic bloc model\nlibrary(tidyverse) # data manipulation\nlibrary(aricode)   # clustering measures comparison\n\n\n\n\n\n\n\nAdvice\n\n\n\nUse the documentation of the aforementioned packages!!, and the vignettes available at https://grosssbm.github.io/sbm/\n\n\nSome extra packages can be obtained for fancier representations,\n\n\nShow the code\nlibrary(corrplot)  # plot of covariance/correlation matrices\nlibrary(ggraph)\nlibrary(RColorBrewer)\ntheme_set(theme_bw())\n\nfancy_network <- function(mygraph, group) {\n\n  V(mygraph)$party  <- as.character(group)\n  V(mygraph)$degree <- degree(mygraph)\n\n  angle <- 90 - 360 * 1:igraph::gorder(mygraph) / igraph::gorder(mygraph)\n  V(mygraph)$hjust <- ifelse(angle < -90, 1, 0)\n  V(mygraph)$angle <- ifelse(angle < -90, angle + 180, angle)\n  \n  p <- ggraph(mygraph, layout = 'linear', circular = TRUE) + \n    geom_edge_arc(alpha = 0.25, show.legend = FALSE) +\n#    geom_node_text(aes(label = party, x = x*1.05, y=y*1.05, angle = angle, hjust = hjust), size = 3) +\n    geom_node_point(aes(colour = party, size = degree), alpha = 0.5) +\n    scale_colour_manual(values = c(brewer.pal(6,\"Paired\"),\"maroon4\",\"springgreen\", \"tan4\",\"plum\",\"brown4\",\"burlywood\",brewer.pal(8, \"Dark2\"), \"navy\")) +\n    theme_graph(base_size = 20)\n  \n  invisible(p)\n}\n\n\n\n\n1.2 Data set: antagonistic tree/fungus interaction network\n\n\n\nFungus on tree\n\n\nWe consider the fungus-tree interaction network studied by Vacher, Piou, and Desprez-Loustau (2008), available with the package sbm:\n\ndata(\"fungusTreeNetwork\")\nstr(fungusTreeNetwork,  max.level = 1)\n#> List of 5\n#>  $ tree_names  : Factor w/ 51 levels \"Abies alba\",\"Abies grandis\",..: 1 2 3 14 42 4 5 6 7 8 ...\n#>  $ fungus_names: Factor w/ 154 levels \"Amphiporthe leiphaemia\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ tree_tree   : num [1:51, 1:51] 0 12 9 3 2 2 0 0 2 7 ...\n#>  $ fungus_tree : int [1:154, 1:51] 0 0 0 0 1 1 1 0 0 0 ...\n#>  $ covar_tree  :List of 3\n\nThis data set provides information about 154 fungi sampled on 51 tree species. It is a list with the following entries:\n\ntree_names : list of the tree species names\nfungus_names: list of the fungus species names\ntree_tree : weighted tree-tree interactions (number of common fungal species two tree species host)\nfungus_tree : binary fungus-tree interactions\ncovar_tree : covariates associated to pairs of trees (namely genetic, taxonomic and geographic distances)\n\nDuring this tutorial we are going to explore successive variants of the Stochastic Blockmodels to analyse binary, weighted, then bipartite network, also by introducing external information via covariates."
  },
  {
    "objectID": "docs/mixture-models/map566-lab-stochastic-blockmodels.html#analysis-of-the-treetree-data",
    "href": "docs/mixture-models/map566-lab-stochastic-blockmodels.html#analysis-of-the-treetree-data",
    "title": "Graph Clustering: Spectral methods and Stochastic Blockmodels",
    "section": "2 Analysis of the tree/tree data",
    "text": "2 Analysis of the tree/tree data\nThe tree-tree interactions result into a simple network.\n\n2.1 Tree-tree binary interaction networks\nWe first consider the binary network where an edge is drawn between two trees when they do share a least one common fungi. Eventually, we will assume that our matrix is the realization of the SBM:\n\n\\begin{aligned}\n(Z_i) \\text{ i.i.d.} \\qquad & Z_i \\sim \\mathcal{M}(1, \\alpha) \\\\\n(Y_{ij}) \\text{ indep.} \\mid (Z_i) \\qquad & (Y_{ij} \\mid Z_i=k, Z_j = \\ell) \\sim \\mathcal{B}(\\pi_{k\\ell})\n\\end{aligned}\n\nQuestions\n\nExtract the binary tree-tree adjacency matrix and plot it (plotMyMatrix). Remove isolated nodes.\nConstruct and build an igraph object to plot the network.\nPerform hierarchical clustering with modularity and edge betweenness.\nImplement a variant of the spectral clustering and test them on this data. Plot the corresponding reordered adjacency matrix.\nAdjust a simple binary SBM with estimateSimpleSBM. Become familiar with object simpleSBM.\nSelect a model via ICL. Explore/check models with similar ICL (using $setModel(), $storedModel)\n\n\n\n2.2 Weighted interaction network with Poisson model\nInstead of considering the binary network tree-tree we may consider the weighted network where the link between two trees is the number of fungi they share.\n\n\\begin{aligned}\n(Z_i) \\text{ i.i.d.} \\qquad & Z_i \\sim \\mathcal{M}(1, \\pi) \\\\\n(Y_{ij}) \\text{ indep.} \\mid (Z_i) \\qquad & (Y_{ij} \\mid Z_i=k, Z_j = \\ell) \\sim \\mathcal{P}(\\exp(\\alpha_{kl})) = \\mathcal{P}(\\lambda_{kl})\n\\end{aligned}\n\n\nAdjust a collection of Poisson SBM, explore the models.\nAdjust spectral methods (you might need to adapt your spectral clustering function to weighted graphs)\nCompare spectral, binary SBM and Poisson SBM with ARI/NID (package aricode) and alluvial plots (plotAlluvial)\n\n\n\n2.3 Including covariate effects\nWe have on each pair of trees 3 covariates, namely the genetic distance, the taxonomic distance and the geographic distance.\nEach covariate has to be introduced as a matrix: X^k_{ij} corresponds to the value of the k-th covariate describing the couple (i,j).\n\nZ_i \\sim^{\\text{iid}} \\mathcal{M}(1, \\alpha) \\\\\nY_{ij} \\mid Z_i=k, Z_j = \\ell \\sim \\mathcal{P}(\\exp(\\pi_{kl} + x_{ij}^\\intercal \\theta)) = \\mathcal{P}(\\gamma_{kl}\\exp(x_{ij}^\\top \\theta))\n\nQuestions\n\nPlot the data matrix of covariates\nUse k-means or hierarchical clustering to cluster the covariates data, and compare with the previous graph clustering obtained\nAdjust a Poisson SBM with covariates (1, 2, all of them).\nCompare the obtained clustering with ARI/NID and alluvial plots\nUse ICL to select the “best” model among all the Poisson models."
  },
  {
    "objectID": "docs/mixture-models/map566-lab-stochastic-blockmodels.html#analysis-of-the-treefungi-data-with-bipartite-sbm",
    "href": "docs/mixture-models/map566-lab-stochastic-blockmodels.html#analysis-of-the-treefungi-data-with-bipartite-sbm",
    "title": "Graph Clustering: Spectral methods and Stochastic Blockmodels",
    "section": "3 Analysis of the tree/fungi data with Bipartite SBM",
    "text": "3 Analysis of the tree/fungi data with Bipartite SBM\nWe now consider the tree-fungi interaction network.\n\nZ^R_i \\sim^{\\text{iid}} \\mathcal{M}(1, \\alpha^R) \\\\\nZ^C_i \\sim^{\\text{iid}} \\mathcal{M}(1, \\alpha^C) \\\\\nY_{ij} \\mid Z^R_i=k, Z^C_j = \\ell \\sim \\mathcal{f}(\\gamma_{k\\ell}, x_{ij}^\\top \\theta)\n\nRedo the whole analysis, using this time a bipartite SBM."
  },
  {
    "objectID": "docs/mixture-models/map566-lab-stochastic-blockmodels.html#references",
    "href": "docs/mixture-models/map566-lab-stochastic-blockmodels.html#references",
    "title": "Graph Clustering: Spectral methods and Stochastic Blockmodels",
    "section": "4 References",
    "text": "4 References\n\n\n\n\nLeger, Jean-Benoist. 2021. “Blockmodels: Latent and Stochastic Block Model Estimation by a ’v-EM’ Algorithm.” https://cran.r-project.org/package=blockmodels.\n\n\nTeam großBM (Barbillon, Chiquet, Donnet, Léger). 2021. “sbm: Stochastic Blockmodels.” https://cran.r-project.org/package=sbm.\n\n\nVacher, Corinne, Dominique Piou, and Marie-Laure Desprez-Loustau. 2008. “Architecture of an Antagonistic Tree/Fungus Network: The Asymmetric Influence of Past Evolutionary History.” PloS One 3 (3): e1740."
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-mixture-models.html#preliminary",
    "href": "docs/mixture-models/map566-lecture-mixture-models.html#preliminary",
    "title": "Mixture Models",
    "section": "Preliminary",
    "text": "Preliminary\nFunctions from R-base and stats (preloaded) are required plus packages from the tidyverse for data representation and manipulation. We will also use the package mixtools, which implements EM for simple mixture models to check our own implementation, and the package aricode for computing various metrics and for comparing clustering:\n\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(aricode)\nlibrary(mixtools)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-mixture-models.html#the-faithful-data",
    "href": "docs/mixture-models/map566-lecture-mixture-models.html#the-faithful-data",
    "title": "Mixture Models",
    "section": "1 The faithful data",
    "text": "1 The faithful data\n\n1.1 The data\nThe faithful data consists of the waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA.\n\ndata(\"faithful\")\nfaithful %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nFor convenience, in the following, the data vector will be denoted by y, with n entries:\n\ny <- faithful$waiting\nn <- length(y)\n\nWe will consider the waiting time in the following. Let us display the empirical distribution of this variable (an histogram).\n\nfaithful %>% ggplot() + \n  geom_histogram(aes(x = waiting), bins = 15) + xlab(\"waiting (mm)\")\n\n\n\n\nWe clearly see 2 modes: the waiting times seem to be distributed either around 50mn or around 80mn.\n\n\n1.2 k-means clustering\nImagine that we want to partition the data (y_i , 1 \\leq i \\leq n) into K clusters. Let \\mu_1, \\mu_2, \\mu_K be the K centers of these K clusters. A way to decide to which cluster belongs an observation y_i consists in minimizing the distance between y_i and the centers (\\mu_k).\nLet Z_i be a label variable such that Z_i=k if observation i belongs to cluster k. Then,\nZ_i = {\\rm arg}\\min_{ k \\in \\{1,2, \\ldots,K\\}} (y_i-\\mu_k)^2\nThe centers (\\mu_k) can be estimated by minimizing the within-cluster sum of squares\n\\begin{aligned}\nU(\\mu_1,\\mu_2,\\ldots,\\mu_L) & =\\sum_{i=1}^n \\min_{k \\in \\{1, \\ldots, K\\}} (y_i - \\mu_k)^2  \\\\\n&= \\sum_{i=1}^n (y_i - \\mu_{Z_i})^2 \\\\\n&= \\sum_{i=1}^n \\sum_{k=1}^K (y_i - \\mu_k)^2 \\mathbf{1}_{\\{{z}_i=k\\}}\n\\end{aligned}\nFor k=1,2,\\ldots, K, the solution \\hat\\mu_k is the empirical mean computed in cluster k. Let n_k = \\sum_{i=1}^n \\mathbf{1}_{\\{{z}_i=k\\}} be the number of observation belonging to cluster k. Then\n \\hat{\\mu}_k =  \\frac{1}{n_k} \\sum_{i=1}^n y_i \\mathbf{1}_{\\{z_i=k\\}} \nLet us compute the centers of the two clusters for our faithful data:\n\nU <- function(mu, y) {\n  sum(pmin((y-mu[1])^2, (y-mu[2])^2))\n}\n\nmu_hat <- nlm(U, c(50,80), y)$estimate\nmu_hat\n\n[1] 54.74997 80.28484\n\n\nWe can then classify the n observations into these 2 clusters\n\ncl_hat <- rep(1, n)\nin_cl2 <- which ((y-mu_hat[1])^2 > (y-mu_hat[2])^2 )\ncl_hat[in_cl2] <- 2\nclustered_data <- data.frame(y, cl_hat = factor(cl_hat))\n\nand plot them on the original data\n\n\nShow the code\nggplot() + \n  geom_point(data = clustered_data, aes(x = y, y = 0, colour = cl_hat), size=3) + \n  geom_point(data = data.frame(y = mu_hat, group = factor(c(1,2))), aes(y, 0, colour = group), size=10, shape=\"x\") + \n  geom_vline(xintercept = mean(mu_hat), linetype = \"dashed\", color = \"gray\")\n\n\n\n\n\nand compute the sizes, the empirical means and standard deviations for each cluster:\n\nclustered_data %>% \n  group_by(cl_hat) %>% \n  summarise(count = n(), means = mean(y), stdev = sd(y))\n\n# A tibble: 2 x 4\n  cl_hat count means stdev\n  <fct>  <int> <dbl> <dbl>\n1 1        100  54.8  5.90\n2 2        172  80.3  5.63\n\n\n\n\n\n\n\n\nInfo\n\n\n\nThe base-R function kmeans generalizes that problem to more than 2 clusters:\n\nkmeans_out <- kmeans(y, centers = 2)\nkmeans_out$size\n\n[1] 100 172\n\nas.vector(kmeans_out$centers)\n\n[1] 54.75000 80.28488\n\nsqrt(kmeans_out$withinss/(kmeans_out$size-1))\n\n[1] 5.895341 5.627335\n\nhead(kmeans_out$cl)\n\n[1] 2 1 2 1 2 1\n\n\n\n\n\n\n1.3 Mixture of probability distributions\nIn a probability framework,\n\nthe labels Z_1, \\ldots, Z_n are a sequence of random variables that take their values in \\{1, 2, \\cdots, K \\} and such that, for k=1,2,\\ldots K,\n\n\\mathbb{P}(Z_i = k) = \\pi_k, \\qquad \\text{s.t } \\sum_{k=1}^K \\pi_k = 1\n\nthe observations in group k, i.e. such Z_i=k, are independent and follow a same probability distribution f_k,\n\n Y_i | Z_i=k \\sim^{\\text{iid}} f_k \nThe probability distribution of Y_i = y_i is therefore a mixture of K distributions:\n\\begin{aligned}\n\\mathbb{P}(Y_i = y_i) &= \\sum_{k=1}^K \\mathbb{P}(Y_i , Z_i = k) \\\\\n& = \\sum_{k=1}^K \\mathbb{P}(Z_i = k) \\, \\mathbb{P}(Y_i | Z_i = k) \\\\\n& = \\sum_{k=1}^K \\pi_k \\, f_k(y_i)\n\\end{aligned}\nIf, for each k, f_k is a normal distribution with mean \\mu_k and variance \\sigma^2_k, the model is a Gaussian mixture model:\n Y_i \\sim^{\\text{iid}} \\sum_{k=1}^K \\pi_k \\, \\mathcal{N}(\\mu_k \\ , \\ \\sigma^2_k)  The vector of parameters of the model regroups the paramateers of each component of the mixture, that is,\n\\theta = (\\boldsymbol{\\pi} = (\\pi_1, \\ldots, \\pi_K), \\boldsymbol\\mu = (\\mu_1,\\ldots, \\mu_K), \\boldsymbol\\sigma^2 = (\\sigma^2_1,\\ldots,\\sigma^2_K) )\nand the likelihood function is\n\\begin{aligned}\n\\ell(\\theta, \\boldsymbol y) &= \\prod_{i=1}^n \\mathbb{P}(y_i ; \\theta) \\\\\n&= \\prod_{i=1}^n \\left( \\sum_{k=1}^K \\mathbb{P}(_i=k ; \\theta)\\mathbb{P}(y_i | Z_i=k ;\\theta) \\right) \\\\\n&= \\prod_{i=1}^n \\left( \\sum_{k=1}^K \\frac{\\pi_k}{\\sqrt{2\\pi \\sigma^2_k}} \\ \\exp \\left\\{-\\frac{1}{2\\sigma_k^2}(y_i - \\mu_k)^2 \\right\\}  \\right)\n\\end{aligned}\nWe can define functions to compute the density and probability distribution for our mixture as a function of the vector of parameters \\theta:\n\ndmixture <- function(x, theta) {\n    mapply(\n      function(pik, muk, sigmak) pik * dnorm(x, muk, sigmak),\n      theta$pi, theta$mu, theta$sigma,\n      SIMPLIFY = TRUE\n    ) %>% rowSums()\n}\n\npmixture <- function(x, theta) {\n    mapply(\n      function(pik, muk, sigmak) pik * pnorm(x, muk, sigmak),\n      theta$pi, theta$mu, theta$sigma,\n      SIMPLIFY = TRUE\n    ) %>% rowSums()\n}\n\ntheta <- list(pi = c(.25,.75), mu = c(52,82), sigma = c(10,10))\nhead(dmixture(y, theta))\n\n[1] 0.02886461 0.01036973 0.02261373 0.01009859 0.02864715 0.01031627\n\nhead(pmixture(y, theta))\n\n[1] 0.5356997 0.1467313 0.4054157 0.2273988 0.7133127 0.1570781\n\n\nThe maximum likelihood (ML) estimate of \\theta cannot be computed in a closed form but several methods can be used for maximizing this likelihood function.\nFor instance, a Newton-type algorithm can be used for minimizing the deviance -2\\log(\\ell(\\theta , y)).\n\n## trick: nlm diverge if two parameters are used for pi, so force the summation to 1\nobjective <- function(theta, y) {\n  theta_l <- list(pi = c(theta[1], 1 - theta[1]), mu = theta[2:3], sigma = theta[4:5])\n  deviance <- -2*sum(log(dmixture(y, theta_l)))\n  deviance\n}\n\ntheta0 <- c(.25,52,82,10,10)\nparam  <- nlm(objective, theta0, y)$estimate\ntheta_hat <- list(pi = c(param[1], 1-param[1]), mu = param[2:3], sigma = param[4:5])\ntheta_hat\n\n$pi\n[1] 0.3608861 0.6391139\n\n$mu\n[1] 54.61486 80.09107\n\n$sigma\n[1] 5.871218 5.867734\n\n\nWe can then plot the empirical distribution of the data together with the probability density function of the mixture:\n\n\nShow the code\nplot_mixture <- data.frame(x = 35:100)\nplot_mixture$pdf <- dmixture(plot_mixture$x, theta_hat)\nfaithful %>% ggplot() + \n  geom_histogram(aes(x = waiting, y=..density..), bins = 15) + xlab(\"waiting (mm)\") + \n  geom_line(data = plot_mixture, aes(x, pdf),colour=\"red\",size=1.5)\n\n\n\n\n\nComparing the empirical and theoretical cumulative distribution functions (cdf) shows how well the mixture model fits the data\n\n\nShow the code\nplot_mixture$cdf <- pmixture(plot_mixture$x, theta_hat)\nfaithful %>% ggplot() + \n  stat_ecdf(aes(waiting), geom = \"step\") + xlab(\"waiting (mm)\") + \n  geom_line(data = plot_mixture, aes(x, cdf),colour=\"red\",size=1.5) +  ylab(\"cdf\")\n\n\n\n\n\nThe estimated mixture distribution F_{\\hat{\\theta}} (obtained with the maximum likelihood estimate \\hat\\theta) seems to perfectly fit the empirical distribution of the faithful data.\nWe can perform a Kolmogorov-Smirnov test for testing H_0: y_i \\sim \\ F_{\\hat{\\theta}} versus H_1: y_i \\sim \\!\\!\\!\\!\\!/ \\ F_{\\hat{\\theta}}:\n\nks.test(y, pmixture, theta_hat)\n\n\n    One-sample Kolmogorov-Smirnov test\n\ndata:  y\nD = 0.033545, p-value = 0.9195\nalternative hypothesis: two-sided\n\n\nWe can compute the posterior distribution of the label variables:\n\\begin{aligned}\n\\mathbb{P}(Z_i=k \\ | \\ y_i \\ ; \\ \\hat{\\theta}) &= \\frac{\\mathbb{P}(Z_i=k \\ ; \\ \\hat{\\theta})\\mathbb{P}(y_i \\ | \\ Z_i=k \\ ; \\ \\hat{\\theta})}{\\mathbb{P}(y_i \\ ; \\ \\hat{\\theta})} \\\\\n&= \\frac{\\mathbb{P}(Z_i=k \\ ; \\ \\hat{\\theta})\\mathbb{P}(y_i \\ | \\ Z_i=k \\ ; \\ \\hat{\\theta})}\n{\\sum_{j=1}^K\\mathbb{P}(Z_i=j \\ ; \\ \\hat{\\theta})\\mathbb{P}(y_i \\ | \\ Z_i=j \\ ; \\ \\hat{\\theta})} \\\\\n&= \\frac{\\frac{\\hat\\pi_k}{\\sqrt{2\\pi \\hat\\sigma_k^2}} \\exp \\left\\{-\\frac{1}{2\\hat\\sigma_k^2}(y_i - \\hat\\mu_k)^2 \\right\\}}\n{\\sum_{j=1}^K\\frac{\\hat\\pi_j}{\\sqrt{2\\pi \\hat\\sigma_j^2}} \\exp \\left\\{-\\frac{1}{2\\hat\\sigma_j^2}(y_i - \\hat\\mu_j)^2 \\right\\}}\n\\end{aligned}\n\ndcomponents <- function(theta, x) {\n    mapply(\n      function(pik, muk, sigmak) pik * dnorm(x, muk, sigmak),\n      theta$pi, theta$mu, theta$sigma,\n      SIMPLIFY = TRUE\n    )\n}\ntau <- dcomponents(theta_hat, y)\ntau %>% as.data.frame() %>% setNames(c(\"comp.1\", \"comp.2\")) %>% \n  add_column(y = y) %>% rmarkdown::paged_table()\n\n\n  \n\n\n\nThe Expectation - Maximization (EM) algorithm (implemented in the mixtools library for instance) could also be used for computing the ML estimate of \\theta. Both algorithms provide the same results.\n\nmixture_EM <- normalmixEM(y)\n\nnumber of iterations= 39 \n\nlist(pi = mixture_EM$lambda, mu=mixture_EM$mu,sigma=mixture_EM$sigma)\n\n$pi\n[1] 0.6391131 0.3608869\n\n$mu\n[1] 80.09109 54.61489\n\n$sigma\n[1] 5.867718 5.871242\n\n\n\nplot(mixture_EM, which=2)\n\n\n\n\n\nmixture_EM$posterior %>% as_tibble() %>% rmarkdown::paged_table()\n\n\n  \n\n\n\n\n\n1.4 Mixture model versus clustering\nLet us sample some data from a Gaussian mixture model. Of course, the labels of the simulated data are known.\n\nn1 <- 120; n2 <- 80\nsome_data <- data.frame(\n  y = c(rnorm(n1, 0, 1), rnorm(n2, 3, 1)),\n  z = rep(1:2, c(n1,n2))\n)\n\nWe can use the k-means method to create two clusters and compute the proportion, center and standard deviation for each cluster,\n\nkmeans_out <- kmeans(some_data$y, centers = 2)\nlist(pi    = kmeans_out$size/sum(kmeans_out$size),\n     mu    = as.vector(kmeans_out$centers),\n     sigma = sqrt(kmeans_out$withinss/kmeans_out$size))\n\n$pi\n[1] 0.42 0.58\n\n$mu\n[1]  3.33965353 -0.09566557\n\n$sigma\n[1] 0.9408479 0.9177584\n\n\nWe can instead consider a Gaussian mixture model, use the EM algorithm with the same data and display the estimated parameters\n\ngmm_out = normalmixEM(some_data$y)\n\nnumber of iterations= 82 \n\nlist(pi    = gmm_out$lambda,\n     mu    = gmm_out$mu,\n     sigma = gmm_out$sigma)\n\n$pi\n[1] 0.5921451 0.4078549\n\n$mu\n[1] -0.03069879  3.34762216\n\n$sigma\n[1] 0.9949958 0.9810413\n\n\nSince the “true” labels are known, we can compute the classification error rate for each method (in % here):\n\ncl_kmeans <- kmeans_out$cluster\ncl_gmm    <- apply(gmm_out$posterior, 1, which.max)\naccuracy  <- c(mean(some_data$z != cl_kmeans)*100, mean(some_data$z != cl_gmm)*100)\naccuracy\n\n[1] 95  6\n\n\n\n\n\n\n\n\n(Adjusted) Rand Index\n\n\n\nAnother metric widely used for evaluating a clustering is the Rand Index – RI (Rand (1971)), more precisely its adjusted version ARI. The Rand index can be seen as a measure of the percentage of correct decisions made by a clustering algorithm. It can be computed using the following formula:\n\nRI = \\frac{TP + TN} {TP + FP + FN + TN}\n\nwhere TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives.\nThe ARI (adjusted Rand index) (Steinley (2004)) is the “corrected-for-chance version” of the Rand index, that is, a correction made by using the expected similarity of all pair-wise comparisons between clusterings specified by a random model (can be seen as a null hypothesis).\nThe ARI can also be used to compare several clusterings: the higher, the better.\n\n\nIn this case, the two clusterings are very close:\n\naricode::ARI(cl_kmeans, some_data$z)\n\n[1] 0.8089061\n\naricode::ARI(cl_gmm   , some_data$z)\n\n[1] 0.7729854\n\naricode::ARI(cl_kmeans, cl_gmm)\n\n[1] 0.9211486\n\n\nOf course, these results depend strongly on the model, i.e. on the parameters of the mixture. This Shiny app allows one to modify the parameters of the second distribution (the first distribution is assumed to be a normal distribution with mean 0 and variance 1) and compare the results provided by the two methods."
  },
  {
    "objectID": "docs/mixture-models/map566-lecture-mixture-models.html#some-em-type-algorithms",
    "href": "docs/mixture-models/map566-lecture-mixture-models.html#some-em-type-algorithms",
    "title": "Mixture Models",
    "section": "2 Some EM-type algorithms",
    "text": "2 Some EM-type algorithms\n\n2.1 Maximisation of the complete likelihood\nAssume first that the label (Z_i) are known. Estimation of the parameters of the model is straightforward: for k =1,2, \\ldots, K,\n\\begin{aligned}\n\\hat{\\pi}_k &= \\frac{\\sum_{i=1}^n \\mathbf{1}_{Z_i=k}}{n} \\\\\n\\hat{\\mu}_k &= \\frac{\\sum_{i=1}^n y_i\\mathbf{1}_{Z_i=k}}{\\sum_{i=1}^n \\mathbf{1}_{Z_i=k}} \\\\\n\\hat{\\sigma}_k^2 &= \\frac{\\sum_{i=1}^n y_i^2\\mathbf{1}_{Z_i=k}}{\\sum_{i=1}^n \\mathbf{1}_{Z_i=k}} - \\hat{\\mu}_k^2\\\\\n\\end{aligned}\nThen, we see that S(z,y) = (\\sum_{i=1}^n \\mathbf{1}_{Z_i=k} \\ , \\ \\sum_{i=1}^n y_i\\mathbf{1}_{Z_i=k}\\ , \\ \\sum_{i=1}^n y_i^2\\mathbf{1}_{Z_i=k} \\ ; \\ 1 \\leq k \\leq K) is the sufficient statistics of the complete model. Indeed, the maximum likelihood estimate of \\theta is a function of S(z,y):\n\n\\hat{\\theta} = ( \\hat{\\boldsymbol\\pi} = (\\hat{\\pi}_1,\\ldots,\\hat{\\pi}_K), \\hat{\\boldsymbol\\mu}=(\\hat{\\mu}_1,\\ldots,\\hat{\\mu}_K), \\hat{\\boldsymbol\\sigma} = (\\hat{\\sigma}_1, \\ldots,\\hat{\\sigma}_K) ) = \\hat\\Theta(S(Z,Y))\n\nwhere \\hat\\Theta is the function defining the maximum likelihood estimator of \\theta.\n\n\n2.2 The EM algorithm\nWhen the labels (Z_i) are unknown, the sufficient statistics S(Z,Y) cannot be computed. Then, the idea of EM is to replace S(Z,Y) by its conditional expectation \\mathbb{E}[S(Z,Y)| Y ;\\theta], wrt the posterior distribution of Z|Y; \\theta.\nThe problem is that this conditional expectation depends on the unknown parameter \\theta. EM is therefore an iterative procedure, where, at iteration t:\n\nthe E-step computes S^{(t)}(Y) = \\mathbb{E}\\left(S(Z,Y)|Y ;\\theta^{(t-1)}\\right)\nthe M-step updates the parameter estimate:\n\n \\theta^{(t)} = \\hat\\Theta(S^{(t)}(Y)) \nLet us see now how to compute \\mathbb{E}\\left(S(Z,Y) | Y ;\\theta^{(t-1)}\\right).\nFirst, for any 1\\leq i \\leq n, and any 1 \\leq k \\leq K, let\n\\tau^{(t)}_{ik} = \\mathbb{E}\\left(\\mathbf{1}_{\\{Z_i=k\\}} \\ | \\ y_i \\ ; \\ \\theta^{(t-1)}\\right) \nBy definition,\n\\begin{aligned}\n\\tau^{(t)}_{ik} &= \\mathbb{E}\\left(\\mathbf{1}_{\\{Z_i=k\\}} \\ | \\ Y_i \\ ; \\ \\theta^{(t-1)}\\right) \\\\\n&= \\mathbb{P}(Z_i=k \\ | \\ Y_i \\ ; \\ \\theta^{(t-1)}) \\\\\n&= \\frac{\\mathbb{P}(Z_i=k\\ ; \\ \\theta^{(t-1)})\\mathbb{P}(Y_i \\ | \\ Z_i=k   \\ ; \\ \\theta^{(t-1)})}{\\mathbb{P}(Y_i \\ ; \\ \\theta^{(t-1)})} \\\\\n&= \\frac{\\mathbb{P}(Z_i=k\\ ; \\ \\theta^{(t-1)}) f_k(Y_i \\ ; \\ \\theta^{(t-1)})}\n{\\sum_{\\ell=1}^K \\mathbb{P}(Z_i=\\ell\\ ; \\ \\theta^{(t-1)}) f_\\ell(y_i  \\ ; \\ \\theta^{(t-1)})} \\\\\n&= \\frac{\\pi^{(t-1)}_k f_k(y_i \\ ; \\ \\theta^{(t-1)})}\n{\\sum_{\\ell=1}^K \\pi_\\ell^{(t-1)} f_\\ell(y_i  \\ ; \\ \\theta^{(t-1)})}\n\\end{aligned}\nwhere \\pi^{(t-1)}_{k}=\\mathbb{P}\\left(Z_i=k\\ ; \\ \\theta^{(t-1)}\\right) is the estimate of \\pi_k obtained at iteration (t-1) and where\n\nf_k(y_i \\ ; \\ \\theta^{(t-1)}) = \\frac{1}{\\sigma^{(t-1)}_{k}\\sqrt{2\\pi}} \\exp\\left\\{ -\\frac{1}{2\\left(\\sigma_k^2\\right)^{(t-1)}}\\left(y_i - \\mu^{(t-1)}_{k}\\right)^2 \\right\\}\n\nis the probability density function of Y_i when Z_i=k, computed at iteration k-1 using \\theta^{(t-1)}. The expected values of the other sufficient statistics can now easily be computed. Indeed, for i=1,2,\\ldots,n and k = 1,2,\\ldots,K,\n\\begin{aligned}\n\\mathbb{E}[Y_i\\mathbf{1}_{Z_i=k} \\ | \\ Y_i = y_i \\ ; \\ \\theta^{(t-1)}] & = y_i\\tau^{(t)}_{ik} \\\\\n\\mathbb{E}[Y_i^2\\mathbf{1}_{Z_i=k} \\ | \\ Y_i = y_i \\ ; \\ \\theta^{(t-1)}] & = y_i^2\\tau^{(t)}_{ik}\n\\end{aligned}\nThen, the t-th iteration of the EM algorithm for a Gaussian mixture consists in\n\ncomputing \\tau^{(t)}_{ik} for i=1,2,\\ldots,n and k = 1,2,\\ldots,L, using \\theta^{(t-1)},\ncomputing \\theta_{k}= \\left(\\pi^{(t)}_k,\\mu^{(t)}_k,\\sigma^{(t)}_k ; 1 \\leq k \\leq K\\right) where\n\n\\begin{aligned}\n\\pi^{(t)}_k &= \\frac1n \\sum_{i=1}^n \\tau^{(t)}_{ik} \\\\\n\\mu^{(t)}_k &= \\frac{\\sum_{i=1}^n y_i\\tau^{(t)}_{ik}}{\\sum_{i=1}^n \\tau^{(t)}_{ik}} \\\\\n\\sigma^{(t)}_k & = \\frac{\\sum_{i=1}^n y_i^2 \\tau^{(t)}_{ik}}{\\sum_{i=1}^n \\tau^{(t)}_{ik}} - \\left(\\mu^{(t)}_k\\right)^2 \\\\\n\\end{aligned}\nFor a given set of initial values \\theta^{(0)} = (\\boldsymbol\\pi^{(0)},\\boldsymbol\\mu^{(0)},\\boldsymbol\\sigma^{(0)}), the following function returns the EM estimate \\theta_K, the sequence of estimates (\\theta^{(t)} \\ , \\ 0\\leq t \\leq T) and the deviance computed with the final estimate -2\\log \\ell(\\boldsymbol y \\ ; \\ \\theta^T). The algorithm stops when the change in the deviance between two iteration is less than a given threshold.\n\nmixture_gaussian1D <- function(x, theta0, max_iter = 100, threshold = 1e-6) {\n  \n  ## initialization\n  n <- length(x)\n  deviance     <- numeric(max_iter)        # we save the results\n  theta        <- vector(\"list\", max_iter) # for monitoring\n  likelihoods  <- dcomponents(theta0, x)\n  deviance[1] <- -2 * sum(log(rowSums(likelihoods)))\n  theta[[1]]  <- theta0\n  \n  for (t in 1:max_iter) {\n\n    # E step\n    tau <- likelihoods / rowSums(likelihoods)\n\n    # M step\n    pi    <- colMeans(tau)\n    mu    <- colSums(tau * x) / colSums(tau)\n    sigma <- sqrt(colSums(tau * x^2) / colSums(tau) - mu^2)\n    theta[[t + 1]] <- list(pi = pi, mu = mu, sigma = sigma)\n    \n    ## Assessing convergence\n    likelihoods   <- dcomponents(theta[[t + 1]], x)\n    deviance[t+1] <- - 2 * sum(log(rowSums(likelihoods)))\n\n    ## prepare next iterations\n    if (abs(deviance[t + 1] - deviance[t]) < threshold)\n      break\n\n  }\n\n  res <- cbind(\n      data.frame(iteration = 2:(t+1) - 1, deviance = deviance[2:(t+1)]),\n      theta[2:(t+1)] %>% purrr::map(unlist) %>% do.call('rbind', .)\n    )\n  \n  list(theta = theta[[t+1]], deviance = deviance[(t+1)], convergence = res)\n}\n\nLet us use this function with our faithful data\n\ntheta0 <- list(pi = c(.5,.5), mu = c(60,70), sigma = c(2,2))\nmyEM_mixture <- mixture_gaussian1D(y, theta0)\nmyEM_mixture$theta\n\n$pi\n[1] 0.3608769 0.6391231\n\n$mu\n[1] 54.61455 80.09088\n\n$sigma\n[1] 5.870959 5.867927\n\nmyEM_mixture$deviance\n\n[1] 2068.004\n\n\nand let us plot the convergence of the algorithm:\n\n\nShow the code\nplot_singleEM <- function(convergence, title = \"Convergence of the EM algorithm\") {\n  p <- ggplot(convergence)\n  gridExtra::grid.arrange(\n    p + geom_line(aes(x = iteration, y = deviance)),\n    p + geom_line(aes(x = iteration, y = pi1)   , color = 'blue') + \n        geom_line(aes(x = iteration, y = pi2)   , color = 'red'),\n    p + geom_line(aes(x = iteration, y = mu1)   , color = 'blue') + \n        geom_line(aes(x = iteration, y = mu2)   , color = 'red'),\n    p + geom_line(aes(x = iteration, y = sigma1), color = 'blue') + \n        geom_line(aes(x = iteration, y = sigma2), color = 'red'),\n    nrow = 2, top = title\n  )\n}\nplot_singleEM(myEM_mixture$convergence)\n\n\n\n\n\n\n\n2.3 Running EM with different initial values\nThe sequence of EM estimates (\\theta_k) depends on the initial guess \\theta_0. Let us plot the convergence of the algorithm obtained with several initial values. To do this, we write a small function to automatize the plot in the same vein as in the above output.\n\n\nShow the code\nplot_convergence <- function(convergence, title = \"Convergence of EM for random initialization\") {\n  if (is.null(convergence$simu)) convergence$simu <- 1\n  p <- ggplot(convergence)\n  plot_list <- lapply(c(\"pi1\", \"pi2\", \"mu1\", \"mu2\", \"sigma1\", \"sigma2\"), function(var) {\n    p + geom_line(aes_string(x = \"iteration\", var, group = \"simu\",  color = \"simu\")) + \n      theme(legend.position = \"none\")\n  })\n  do.call(\"grid.arrange\", c(plot_list, ncol = 3, top = title))\n}\n\n\n\n\nShow the code\nset.seed(12345)\nconvergences <- lapply(1:10, function(i) {\n  pi1 <- runif(1, 0.1, 0.9)\n  theta0 <- list(\n    pi =  c(pi1, 1-pi1),\n    mu = rnorm(2, 70, 15),\n    sigma = rlnorm(2,2,0.7)\n  )\n  res <- mixture_gaussian1D(y, theta0)$convergence\n  res$simu <- i\n  res\n}) %>% do.call(rbind, .) %>% mutate(simu = factor(simu))\nplot_convergence(convergences)\n\n\n\n\n\nWe see that, up to some permutation (the labels are interchangeable), all the runs converge to the same solution with this example. Nevertheless, a very poor initial guess may lead to a very poor convergence of EM, which can get stuck into a local minimum.\n\n\n2.4 A stochastic version of EM\nA stochastic version of EM consists, at iteration k, in replacing the unknown labels (Z_i) by a sequence (Z_i^{(k)}), where Z_i^{(k)} is sampled from the conditional distribution of Z_i:\n\\begin{aligned}\n\\mathbb{P}(Z_i^{(k)}=k) &= \\mathbb{P}(Z_i=k \\ | \\ y_i \\ ; \\ \\theta^{(t-1)}) \\\\\n&=  \\frac{\\pi^{(t-1)}_{k}f_k(y_i \\ ; \\ \\theta^{(t-1)})}\n{\\sum_{j=1}^K \\pi_{j,k-1}f_j(y_i  \\ ; \\ \\theta^{(t-1)})}\n\\end{aligned}\nWe can then use these sampled labels for computing the sufficient statistics S(z^{(k)},y) and updating the estimation of \\theta as \\theta_k = \\hat\\Theta(S(z^{(k)},y)).\n\nmixture_gaussian1D_SEM <- function(x, theta0, max_iter = 100, threshold = 1e-6) {\n  \n  ## initialization\n  n <- length(x)\n  deviance     <- numeric(max_iter)        # we save the results\n  theta        <- vector(\"list\", max_iter) # for monitoring\n  likelihoods  <- dcomponents(theta0, x)\n  deviance[1] <- -2 * sum(log(rowSums(likelihoods)))\n  theta[[1]]  <- theta0\n  \n  for (t in 1:max_iter) {\n\n    # SE step\n    tau1 <- 1 * (runif(n) < likelihoods[, 1] / rowSums(likelihoods))\n    tau2 <- 1 - tau1\n    tau <- cbind(tau1, tau2); colnames(tau) <- NULL\n\n    # M step\n    pi    <- colMeans(tau)\n    mu    <- colSums(tau * x) / colSums(tau)\n    sigma <- sqrt(colSums(tau * x^2) / colSums(tau) - mu^2)\n    theta[[t + 1]] <- list(pi = pi, mu = mu, sigma = sigma)\n    \n    ## Assessing convergence\n    likelihoods   <- dcomponents(theta[[t + 1]], x)\n    deviance[t+1] <- - 2 * sum(log(rowSums(likelihoods)))\n\n    ## prepare next iterations\n    if (abs(deviance[t + 1] - deviance[t]) < threshold)\n      break\n\n  }\n\n  res <- cbind(\n      data.frame(iteration = 2:(t+1) - 1, deviance = deviance[2:(t+1)]),\n      theta[2:(t+1)] %>% purrr::map(unlist) %>% do.call('rbind', .)\n    )\n  \n  list(theta = theta[[t+1]], deviance = deviance[(t+1)], convergence = res)\n}\n\n\ntheta0 <- list(pi=c(.2,.8), mu=c(75,75), sigma=c(10,4))\nmySEM_mixture <- mixture_gaussian1D_SEM(y, theta0)\nmySEM_mixture$theta\n\n$pi\n[1] 0.3529412 0.6470588\n\n$mu\n[1] 54.33333 79.93182\n\n$sigma\n[1] 5.615579 6.009075\n\nmySEM_mixture$deviance\n\n[1] 2068.361\n\n\n\nplot_singleEM(mySEM_mixture$convergence, title=\"Convergence of S-EM\")\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nRand, William M. 1971. “Objective Criteria for the Evaluation of Clustering Methods.” Journal of the American Statistical Association 66 (336): 846–50.\n\n\nSteinley, Douglas. 2004. “Properties of the Hubert-Arable Adjusted Rand Index.” Psychological Methods 9 (3): 386."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Welcome",
    "section": "Course description",
    "text": "Course description\nThe objective of this course is to show students how statistics is used in practice to answer a specific question, by introducing a series of important model-based approaches.\nThe students will learn to select and use appropriate statistical methodologies and acquire solid and practical skills by working-out examples on real-world data sets from various areas including medicine, genomics, ecology, and others.\nAll analyses will be conducted mainly with the R software, possibly with interfacing to Python. No strong knwoledge neither of R nor Python programming is required (only basic scripting).\n\n\n\n\n\n\nImportant remark\n\n\n\nMuch of the material used in this course is due to Marc Lavielle, who was the first to set up the Statistics in Actions course. We only have made some adjustments to it."
  },
  {
    "objectID": "index.html#schedule-tentative",
    "href": "index.html#schedule-tentative",
    "title": "Welcome",
    "section": "Schedule (tentative)",
    "text": "Schedule (tentative)\nTeachers : Julien Chiquet (lecture + 1 PC), Geneviève Robin (2 PC)\nCourse Evaluation: 2 individual homework assignements + a final exam/project\nCourse Language: French with all material in English\n\nStatistical tests (x1.5)\n\nTwo-populations comparison\nPower analysis\nMultiple Testing\n\nRegression models (x1.5)\n\nLinear and Non Linear Regression models\nNonlinear regression models\nInference Diagnostic, Model comparison\n\nMixed effects models (x2)\n\nLinear mixed effects models\nNonlinear mixed effects models\n\nMixture models and model-based clustering (x2)\n\nGaussian mixture models for data clustering\nStochastic Block Models for graph clustering\n(Variational) EM algorithm\n\nModel-based Dimension Reduction (x2)\n\nMultivariate Gaussian model\nProbabilistic Gaussian PCA\nGeneralized mixed effect models"
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Setup instructions",
    "section": "",
    "text": "R and RStudio are separate downloads and installations\n\nR is the underlying statistical computing environment\nRStudio is a graphical integrated development environment (IDE)\n\n\nInstalling R\nGo to the CRAN webpage, select your OS and follow the instructions.\n\n\nInstalling RStudio Desktop\nGo to the download page. Select, download and install the file corresponding to your OS.\n\n\nInstalling R packages\nLaunch Rstudio and execute the following commands in the console (at least these R packages will be needed during MAP566)\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"knitr\")\ninstall.packages(\"lme4\")\ninstall.packages(\"lattice\")\ninstall.packages(\"nlme\")\ninstall.packages(\"ggfortify\")\ninstall.packages(\"gridextra\")\ninstall.packages(\"saemix\")\n\n\nOn Windows\n\nYou may need Rtools (dedicated page) and git (dedicated page)\n\nOn MacOS\n\nYou may need XCode: visit the dedicated page, download the Mandatory tools and install them on your computer\n\nOn Linux\n\nIf installation of a package fails in Rstudio, just READ THE MESSAGES: you may be asked to install some missing system libraries with, e.g.,\n\nsudo apt-get install lib-missing"
  }
]